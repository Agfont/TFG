{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA (APÉNDICE)\n",
    "\n",
    "En este notebook, se entrenan modelos de deep learning (LSTM) utilizando series temporales de las variables que componen OASIS, para predecir si una estancia en UCI va a ser > 3 días o no. Luego se evalua el rendimiento de los modelos con ACC, AUC-ROC, AUC-PR, confusion matrix...También se realiza Cross Validation y student-t-test para determinar si las diferencias entre los resultados son estadísticamente significativas.\n",
    "\n",
    "Obs. Ejecute primero '06CreateTimeSeries.ipynb' para obtener 'result_OneBigDataset.csv', 'result_OneBigDataset_y_true.csv', 'result_OneBigDataset_test.csv' y 'result_OneBigDataset_y_true_test.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "random.seed(42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_OneBigDataset = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/lengthOfStay/result_OneBigDataset.csv')\n",
    "y_true = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/lengthOfStay/result_OneBigDataset_y_true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>HRATE</th>\n",
       "      <th>MAP</th>\n",
       "      <th>RESP_RATE</th>\n",
       "      <th>TEMP_C</th>\n",
       "      <th>gcs_e</th>\n",
       "      <th>gcs_m</th>\n",
       "      <th>gcs_total_carevue</th>\n",
       "      <th>gcs_v</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PRELOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>81.0</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.944444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>83.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.944444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>88.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.944444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>86.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.944444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>89.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>89.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>93.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>94.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>84.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>100.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>84.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>88.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>85.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>82.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.531507</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>84.333298</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>72.666702</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>75.666702</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>52.666698</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>54.0</td>\n",
       "      <td>69.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7</td>\n",
       "      <td>53.0</td>\n",
       "      <td>88.666702</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>57.0</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11</td>\n",
       "      <td>61.0</td>\n",
       "      <td>86.333298</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>12</td>\n",
       "      <td>59.0</td>\n",
       "      <td>92.666702</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13</td>\n",
       "      <td>68.0</td>\n",
       "      <td>88.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>79.0</td>\n",
       "      <td>83.666702</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>62.0</td>\n",
       "      <td>76.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.333298</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17</td>\n",
       "      <td>65.0</td>\n",
       "      <td>82.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.666698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>18</td>\n",
       "      <td>72.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>19</td>\n",
       "      <td>78.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20</td>\n",
       "      <td>77.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>21</td>\n",
       "      <td>74.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22</td>\n",
       "      <td>72.0</td>\n",
       "      <td>104.667000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23</td>\n",
       "      <td>66.0</td>\n",
       "      <td>95.666702</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.747945</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86.813699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86.813699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hours  HRATE         MAP  RESP_RATE     TEMP_C  gcs_e  gcs_m  \\\n",
       "0       0   97.0   74.000000       23.0  36.600000    4.0    6.0   \n",
       "1       1   96.0   74.000000       22.0  36.600000    3.0    6.0   \n",
       "2       2   90.0   74.000000       20.0  36.600000    3.0    6.0   \n",
       "3       3   90.0   76.000000       16.0  36.600000    3.0    6.0   \n",
       "4       4   81.0   76.000000       16.0  36.600000    3.0    6.0   \n",
       "5       5   80.0   76.000000       15.0  36.944444    3.0    6.0   \n",
       "6       6   83.0   81.000000       18.0  36.944444    3.0    6.0   \n",
       "7       7   88.0   81.000000       18.0  36.944444    3.0    6.0   \n",
       "8       8   86.0   81.000000       16.0  36.944444    3.0    6.0   \n",
       "9       9   89.0   78.000000       18.0  37.055556    3.0    6.0   \n",
       "10     10  100.0   78.000000       24.0  37.055556    3.0    6.0   \n",
       "11     11   89.0   78.000000       20.0  37.055556    3.0    6.0   \n",
       "12     12  100.0   74.000000       20.0  37.055556    3.0    6.0   \n",
       "13     13   93.0   74.000000       23.0  37.055556    4.0    6.0   \n",
       "14     14   94.0   74.000000       14.0  37.055556    4.0    6.0   \n",
       "15     15   97.0   90.000000       17.0  37.055556    4.0    6.0   \n",
       "16     16   89.0   90.000000       21.0  37.055556    4.0    6.0   \n",
       "17     17   84.0   90.000000       20.0  37.055556    4.0    6.0   \n",
       "18     18   85.0   85.000000       17.0  37.055556    4.0    6.0   \n",
       "19     19  100.0   73.000000       18.0  37.055556    4.0    6.0   \n",
       "20     20   84.0   73.000000       23.0  37.055556    4.0    6.0   \n",
       "21     21   88.0   81.000000       15.0  37.055556    4.0    6.0   \n",
       "22     22   85.0   82.000000       17.0  37.055556    4.0    6.0   \n",
       "23     23   82.0   80.000000       18.0  37.055556    4.0    6.0   \n",
       "24      0   66.0   84.333298       25.0  36.600000    4.0    6.0   \n",
       "25      1   59.0   68.000000       17.0  36.600000    4.0    6.0   \n",
       "26      2   58.0   72.666702       16.0  36.388901    4.0    6.0   \n",
       "27      3   62.0   75.666702       18.0  36.388901    4.0    6.0   \n",
       "28      4   62.0   52.666698       19.0  36.388901    4.0    6.0   \n",
       "29      5   54.0   58.000000       17.0  36.388901    4.0    6.0   \n",
       "30      6   54.0   69.333298       18.0  36.666698    4.0    6.0   \n",
       "31      7   53.0   88.666702       16.0  36.666698    4.0    6.0   \n",
       "32      8   61.0   80.000000       20.0  36.666698    4.0    6.0   \n",
       "33      9   61.0   80.000000       20.0  36.666698    4.0    6.0   \n",
       "34     10   57.0   69.000000       22.0  36.555599    4.0    6.0   \n",
       "35     11   61.0   86.333298       19.0  36.555599    4.0    6.0   \n",
       "36     12   59.0   92.666702       17.0  36.555599    4.0    6.0   \n",
       "37     13   68.0   88.333298       18.0  36.555599    4.0    6.0   \n",
       "38     14   79.0   83.666702       24.0  36.666698    4.0    6.0   \n",
       "39     15   62.0   76.333298       18.0  36.666698    4.0    6.0   \n",
       "40     16   63.0   67.333298       19.0  36.666698    4.0    6.0   \n",
       "41     17   65.0   82.333298       18.0  36.666698    4.0    6.0   \n",
       "42     18   72.0   82.000000       18.0  36.444401    4.0    6.0   \n",
       "43     19   78.0   82.000000       20.0  36.444401    4.0    6.0   \n",
       "44     20   77.0   82.000000       20.0  36.444401    4.0    6.0   \n",
       "45     21   74.0   82.000000       23.0  36.444401    4.0    6.0   \n",
       "46     22   72.0  104.667000       23.0  36.777802    4.0    6.0   \n",
       "47     23   66.0   95.666702       21.0  36.777802    4.0    6.0   \n",
       "48      0   86.0   77.000000       19.0  36.600000    4.0    6.0   \n",
       "49      1   86.0   77.000000       19.0  36.600000    4.0    6.0   \n",
       "\n",
       "    gcs_total_carevue  gcs_v        AGE  PRELOS  \n",
       "0                15.0    5.0  43.531507     2.8  \n",
       "1                15.0    1.0  43.531507     2.8  \n",
       "2                15.0    1.0  43.531507     2.8  \n",
       "3                15.0    1.0  43.531507     2.8  \n",
       "4                15.0    1.0  43.531507     2.8  \n",
       "5                15.0    1.0  43.531507     2.8  \n",
       "6                15.0    1.0  43.531507     2.8  \n",
       "7                15.0    1.0  43.531507     2.8  \n",
       "8                15.0    1.0  43.531507     2.8  \n",
       "9                15.0    5.0  43.531507     2.8  \n",
       "10               15.0    5.0  43.531507     2.8  \n",
       "11               15.0    5.0  43.531507     2.8  \n",
       "12               15.0    5.0  43.531507     2.8  \n",
       "13               15.0    5.0  43.531507     2.8  \n",
       "14               15.0    5.0  43.531507     2.8  \n",
       "15               15.0    5.0  43.531507     2.8  \n",
       "16               15.0    5.0  43.531507     2.8  \n",
       "17               15.0    5.0  43.531507     2.8  \n",
       "18               15.0    5.0  43.531507     2.8  \n",
       "19               15.0    5.0  43.531507     2.8  \n",
       "20               15.0    5.0  43.531507     2.8  \n",
       "21               15.0    5.0  43.531507     2.8  \n",
       "22               15.0    5.0  43.531507     2.8  \n",
       "23               15.0    5.0  43.531507     2.8  \n",
       "24               15.0    5.0  80.747945    33.5  \n",
       "25               15.0    5.0  80.747945    33.5  \n",
       "26               15.0    5.0  80.747945    33.5  \n",
       "27               15.0    5.0  80.747945    33.5  \n",
       "28               15.0    5.0  80.747945    33.5  \n",
       "29               15.0    5.0  80.747945    33.5  \n",
       "30               15.0    5.0  80.747945    33.5  \n",
       "31               15.0    5.0  80.747945    33.5  \n",
       "32               15.0    5.0  80.747945    33.5  \n",
       "33               15.0    5.0  80.747945    33.5  \n",
       "34               15.0    5.0  80.747945    33.5  \n",
       "35               15.0    5.0  80.747945    33.5  \n",
       "36               15.0    5.0  80.747945    33.5  \n",
       "37               15.0    5.0  80.747945    33.5  \n",
       "38               14.0    4.0  80.747945    33.5  \n",
       "39               14.0    4.0  80.747945    33.5  \n",
       "40               14.0    4.0  80.747945    33.5  \n",
       "41               14.0    4.0  80.747945    33.5  \n",
       "42               14.0    4.0  80.747945    33.5  \n",
       "43               14.0    4.0  80.747945    33.5  \n",
       "44               14.0    4.0  80.747945    33.5  \n",
       "45               14.0    4.0  80.747945    33.5  \n",
       "46               15.0    5.0  80.747945    33.5  \n",
       "47               15.0    5.0  80.747945    33.5  \n",
       "48               15.0    5.0  86.813699     0.0  \n",
       "49               15.0    5.0  86.813699     0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_OneBigDataset.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hours                0\n",
       "HRATE                0\n",
       "MAP                  0\n",
       "RESP_RATE            0\n",
       "TEMP_C               0\n",
       "gcs_e                0\n",
       "gcs_m                0\n",
       "gcs_total_carevue    0\n",
       "gcs_v                0\n",
       "AGE                  0\n",
       "PRELOS               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_OneBigDataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_OneBigDataset = np.array(result_OneBigDataset.drop(['Hours'],axis=1))\n",
    "scaler = StandardScaler() #normalizar\n",
    "result_OneBigDataset_tranformed = scaler.fit_transform(result_OneBigDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sin normalizar:\n",
    "#train_input = np.array(result_OneBigDataset.drop(['Hours'],axis=1)).reshape(y_true.shape[0], 24, 8)\n",
    "#Normalizado:\n",
    "train_input = result_OneBigDataset_tranformed.reshape(y_true.shape[0], 24, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.04373538e-01, -3.00562352e-02,  7.29956932e-01, ...,\n",
       "          6.48531688e-01, -1.19838408e+00, -2.92974697e-01],\n",
       "        [ 5.52791841e-01, -3.00562352e-02,  5.52048634e-01, ...,\n",
       "         -1.66958158e+00, -1.19838408e+00, -2.92974697e-01],\n",
       "        [ 2.43301658e-01, -3.00562352e-02,  1.96232039e-01, ...,\n",
       "         -1.66958158e+00, -1.19838408e+00, -2.92974697e-01],\n",
       "        ...,\n",
       "        [ 1.40138263e-01,  1.99531822e-02, -6.93309448e-01, ...,\n",
       "          6.48531688e-01, -1.19838408e+00, -2.92974697e-01],\n",
       "        [-1.46068282e-02,  2.70973847e-02, -3.37492853e-01, ...,\n",
       "          6.48531688e-01, -1.19838408e+00, -2.92974697e-01],\n",
       "        [-1.69351920e-01,  1.28089797e-02, -1.59584556e-01, ...,\n",
       "          6.48531688e-01, -1.19838408e+00, -2.92974697e-01]],\n",
       "\n",
       "       [[-9.94659075e-01,  4.37669361e-02,  1.08577353e+00, ...,\n",
       "          6.48531688e-01,  9.50236535e-01,  7.83182393e-02],\n",
       "        [-1.35573096e+00, -7.29214502e-02, -3.37492853e-01, ...,\n",
       "          6.48531688e-01,  9.50236535e-01,  7.83182393e-02],\n",
       "        [-1.40731265e+00, -3.95815842e-02, -5.15401151e-01, ...,\n",
       "          6.48531688e-01,  9.50236535e-01,  7.83182393e-02],\n",
       "        ...,\n",
       "        [-5.82005497e-01,  2.70973847e-02,  7.29956932e-01, ...,\n",
       "          6.90033709e-02,  9.50236535e-01,  7.83182393e-02],\n",
       "        [-6.85168892e-01,  1.89035021e-01,  7.29956932e-01, ...,\n",
       "          6.48531688e-01,  9.50236535e-01,  7.83182393e-02],\n",
       "        [-9.94659075e-01,  1.24735073e-01,  3.74140337e-01, ...,\n",
       "          6.48531688e-01,  9.50236535e-01,  7.83182393e-02]],\n",
       "\n",
       "       [[ 3.69748690e-02, -8.62362776e-03,  1.83237420e-02, ...,\n",
       "          6.48531688e-01,  1.30043133e+00, -3.26838548e-01],\n",
       "        [ 3.69748690e-02, -8.62362776e-03,  1.83237420e-02, ...,\n",
       "          6.48531688e-01,  1.30043133e+00, -3.26838548e-01],\n",
       "        [-3.24097011e-01, -5.14888427e-02,  1.83237420e-02, ...,\n",
       "          6.48531688e-01,  1.30043133e+00, -3.26838548e-01],\n",
       "        ...,\n",
       "        [-4.27260406e-01, -5.14888427e-02, -3.37492853e-01, ...,\n",
       "          6.90033709e-02,  1.30043133e+00, -3.26838548e-01],\n",
       "        [-5.30423800e-01, -1.37219273e-01, -1.04912604e+00, ...,\n",
       "          6.90033709e-02,  1.30043133e+00, -3.26838548e-01],\n",
       "        [-4.27260406e-01, -1.30075070e-01, -3.37492853e-01, ...,\n",
       "          6.90033709e-02,  1.30043133e+00, -3.26838548e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.69748690e-02, -8.62362776e-03,  1.83237420e-02, ...,\n",
       "          6.90033709e-02, -1.39891062e-01, -3.26838548e-01],\n",
       "        [ 1.12019051e+00, -7.29214502e-02,  9.07865229e-01, ...,\n",
       "          6.90033709e-02, -1.39891062e-01, -3.26838548e-01],\n",
       "        [ 1.01702712e+00, -8.00656526e-02,  7.29956932e-01, ...,\n",
       "          6.90033709e-02, -1.39891062e-01, -3.26838548e-01],\n",
       "        ...,\n",
       "        [ 1.12019051e+00, -6.57772477e-02,  1.08577353e+00, ...,\n",
       "          6.48531688e-01, -1.39891062e-01, -3.26838548e-01],\n",
       "        [ 1.58442579e+00, -5.86330452e-02,  1.97531501e+00, ...,\n",
       "          6.48531688e-01, -1.39891062e-01, -3.26838548e-01],\n",
       "        [ 1.12019051e+00, -8.72098551e-02,  3.04276480e+00, ...,\n",
       "          6.48531688e-01, -1.39891062e-01, -3.26838548e-01]],\n",
       "\n",
       "       [[ 8.62282024e-01, -1.01498260e-01, -1.04912604e+00, ...,\n",
       "          6.48531688e-01,  1.80728660e+00, -8.76023437e-03],\n",
       "        [ 8.62282024e-01, -8.00656526e-02,  3.74140337e-01, ...,\n",
       "          6.48531688e-01,  1.80728660e+00, -8.76023437e-03],\n",
       "        [ 8.62282024e-01, -9.43540576e-02,  1.83237420e-02, ...,\n",
       "          6.48531688e-01,  1.80728660e+00, -8.76023437e-03],\n",
       "        ...,\n",
       "        [ 8.10700327e-01, -1.47942527e-03, -8.71217746e-01, ...,\n",
       "          6.90033709e-02,  1.80728660e+00, -8.76023437e-03],\n",
       "        [ 3.69748690e-02, -1.15786665e-01, -1.58285094e+00, ...,\n",
       "          6.90033709e-02,  1.80728660e+00, -8.76023437e-03],\n",
       "        [-1.17770223e-01, -1.15786665e-01, -1.22703434e+00, ...,\n",
       "          6.90033709e-02,  1.80728660e+00, -8.76023437e-03]],\n",
       "\n",
       "       [[ 1.63600748e+00, -1.87228690e-01,  7.29956932e-01, ...,\n",
       "          6.48531688e-01,  8.20692934e-01, -1.60167738e-02],\n",
       "        [ 1.32651730e+00, -1.99135921e-01,  1.79740672e+00, ...,\n",
       "          6.48531688e-01,  8.20692934e-01, -1.60167738e-02],\n",
       "        [ 1.17177221e+00, -1.72940285e-01,  1.61949842e+00, ...,\n",
       "          6.90033709e-02,  8.20692934e-01, -1.60167738e-02],\n",
       "        ...,\n",
       "        [ 1.40138263e-01, -6.10141916e-02,  1.44159012e+00, ...,\n",
       "          6.90033709e-02,  8.20692934e-01, -1.60167738e-02],\n",
       "        [-1.17770223e-01, -1.84847516e-01,  1.83237420e-02, ...,\n",
       "          6.90033709e-02,  8.20692934e-01, -1.60167738e-02],\n",
       "        [-1.69351920e-01, -8.62362776e-03, -3.37492853e-01, ...,\n",
       "          6.90033709e-02,  8.20692934e-01, -1.60167738e-02]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29846, 24, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(y_true)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29846, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_OneBigDataset_test = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/lengthOfStay/result_OneBigDataset_test.csv')\n",
    "y_true_test = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data//lengthOfStay/result_OneBigDataset_y_true_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>HRATE</th>\n",
       "      <th>MAP</th>\n",
       "      <th>RESP_RATE</th>\n",
       "      <th>TEMP_C</th>\n",
       "      <th>gcs_e</th>\n",
       "      <th>gcs_m</th>\n",
       "      <th>gcs_total_carevue</th>\n",
       "      <th>gcs_v</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PRELOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>80.333298</td>\n",
       "      <td>10.0</td>\n",
       "      <td>36.111099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>69.333298</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36.111099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>63.0</td>\n",
       "      <td>71.333298</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>63.0</td>\n",
       "      <td>70.666702</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>61.0</td>\n",
       "      <td>75.666702</td>\n",
       "      <td>9.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.944401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>70.666702</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.944401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>58.0</td>\n",
       "      <td>75.666702</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.944401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>60.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35.944401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>56.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>57.0</td>\n",
       "      <td>68.666702</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>57.0</td>\n",
       "      <td>65.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>57.0</td>\n",
       "      <td>71.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.444401</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>57.0</td>\n",
       "      <td>72.666702</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>60.0</td>\n",
       "      <td>72.666702</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>56.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>55.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.166698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>58.0</td>\n",
       "      <td>74.333298</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.166698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.333298</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.166698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.333298</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.166698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.704110</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>62.666698</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>65.666702</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.222198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.222198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>67.333298</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>84.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7</td>\n",
       "      <td>95.0</td>\n",
       "      <td>65.666702</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>94.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>108.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>96.0</td>\n",
       "      <td>67.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11</td>\n",
       "      <td>93.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>12</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57.666698</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13</td>\n",
       "      <td>87.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>88.0</td>\n",
       "      <td>69.666702</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.666702</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>99.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17</td>\n",
       "      <td>105.0</td>\n",
       "      <td>63.666698</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>18</td>\n",
       "      <td>81.0</td>\n",
       "      <td>69.333298</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>19</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20</td>\n",
       "      <td>89.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>21</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22</td>\n",
       "      <td>78.0</td>\n",
       "      <td>65.666702</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68.333298</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69.610959</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>65.0</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35.099998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69.610959</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hours  HRATE        MAP  RESP_RATE     TEMP_C  gcs_e  gcs_m  \\\n",
       "0       0   54.0  89.000000       14.0  36.600000    4.0    6.0   \n",
       "1       1   54.0  77.000000       15.0  36.600000    4.0    6.0   \n",
       "2       2   55.0  80.333298       10.0  36.111099    4.0    6.0   \n",
       "3       3   57.0  69.333298        8.0  36.111099    4.0    6.0   \n",
       "4       4   63.0  71.333298       14.0  35.777802    4.0    6.0   \n",
       "5       5   63.0  70.666702       13.0  35.777802    4.0    6.0   \n",
       "6       6   61.0  75.666702        9.0  35.777802    4.0    6.0   \n",
       "7       7   58.0  66.000000        8.0  35.777802    4.0    6.0   \n",
       "8       8   56.0  82.000000        8.0  35.944401    4.0    6.0   \n",
       "9       9   61.0  70.666702       22.0  35.944401    4.0    6.0   \n",
       "10     10   58.0  75.666702       15.0  35.944401    4.0    6.0   \n",
       "11     11   60.0  78.000000       13.0  35.944401    4.0    6.0   \n",
       "12     12   56.0  63.000000       12.0  36.444401    4.0    6.0   \n",
       "13     13   57.0  68.666702        9.0  36.444401    4.0    6.0   \n",
       "14     14   57.0  65.333298       18.0  36.444401    4.0    6.0   \n",
       "15     15   57.0  71.333298       18.0  36.444401    4.0    6.0   \n",
       "16     16   57.0  72.666702       18.0  36.500000    4.0    6.0   \n",
       "17     17   60.0  72.666702       13.0  36.500000    4.0    6.0   \n",
       "18     18   56.0  74.000000        9.0  36.500000    4.0    6.0   \n",
       "19     19   55.0  74.000000       23.0  36.500000    4.0    6.0   \n",
       "20     20   60.0  77.000000       16.0  36.166698    4.0    6.0   \n",
       "21     21   58.0  74.333298       15.0  36.166698    4.0    6.0   \n",
       "22     22   60.0  65.333298       14.0  36.166698    4.0    6.0   \n",
       "23     23   60.0  65.333298       14.0  36.166698    4.0    6.0   \n",
       "24      0   81.0  67.000000       23.0  36.600000    4.0    6.0   \n",
       "25      1   76.0  62.666698       21.0  36.333302    4.0    6.0   \n",
       "26      2   79.0  65.666702       20.0  36.333302    4.0    6.0   \n",
       "27      3   72.0  65.000000       20.0  36.222198    4.0    6.0   \n",
       "28      4   74.0  71.000000       21.0  36.222198    4.0    6.0   \n",
       "29      5   78.0  67.333298       20.0  36.388901    4.0    6.0   \n",
       "30      6   84.0  64.000000       21.0  36.388901    4.0    6.0   \n",
       "31      7   95.0  65.666702       20.0  36.388901    4.0    6.0   \n",
       "32      8   94.0  62.000000       19.0  37.055599    4.0    6.0   \n",
       "33      9  108.0  75.000000       21.0  37.055599    4.0    6.0   \n",
       "34     10   96.0  67.333298       18.0  37.055599    4.0    6.0   \n",
       "35     11   93.0  62.000000       19.0  37.055599    4.0    6.0   \n",
       "36     12   90.0  57.666698       18.0  37.055599    4.0    6.0   \n",
       "37     13   87.0  70.000000       20.0  37.055599    4.0    6.0   \n",
       "38     14   88.0  69.666702       14.0  37.000000    4.0    6.0   \n",
       "39     15  102.0  69.666702       22.0  37.000000    4.0    6.0   \n",
       "40     16   99.0  70.000000       22.0  37.000000    4.0    6.0   \n",
       "41     17  105.0  63.666698       16.0  37.000000    3.0    6.0   \n",
       "42     18   81.0  69.333298       16.0  36.888901    3.0    5.0   \n",
       "43     19   85.0  86.000000       15.0  36.888901    3.0    5.0   \n",
       "44     20   89.0  70.000000       18.0  36.888901    3.0    4.0   \n",
       "45     21   79.0  68.000000       16.0  36.888901    3.0    4.0   \n",
       "46     22   78.0  65.666702       18.0  36.888901    3.0    5.0   \n",
       "47     23   74.0  68.333298       14.0  36.888901    3.0    5.0   \n",
       "48      0   64.0  70.000000       14.0  36.600000    4.0    6.0   \n",
       "49      1   65.0  76.000000       14.0  35.099998    4.0    6.0   \n",
       "\n",
       "    gcs_total_carevue  gcs_v        AGE  PRELOS  \n",
       "0                15.0    5.0  49.704110    84.9  \n",
       "1                15.0    5.0  49.704110    84.9  \n",
       "2                15.0    5.0  49.704110    84.9  \n",
       "3                15.0    5.0  49.704110    84.9  \n",
       "4                15.0    5.0  49.704110    84.9  \n",
       "5                15.0    5.0  49.704110    84.9  \n",
       "6                15.0    5.0  49.704110    84.9  \n",
       "7                15.0    5.0  49.704110    84.9  \n",
       "8                15.0    5.0  49.704110    84.9  \n",
       "9                15.0    5.0  49.704110    84.9  \n",
       "10               15.0    5.0  49.704110    84.9  \n",
       "11               15.0    5.0  49.704110    84.9  \n",
       "12               15.0    5.0  49.704110    84.9  \n",
       "13               15.0    5.0  49.704110    84.9  \n",
       "14               15.0    5.0  49.704110    84.9  \n",
       "15               15.0    5.0  49.704110    84.9  \n",
       "16               15.0    5.0  49.704110    84.9  \n",
       "17               15.0    5.0  49.704110    84.9  \n",
       "18               15.0    5.0  49.704110    84.9  \n",
       "19               15.0    5.0  49.704110    84.9  \n",
       "20               15.0    5.0  49.704110    84.9  \n",
       "21               15.0    5.0  49.704110    84.9  \n",
       "22               15.0    5.0  49.704110    84.9  \n",
       "23               15.0    5.0  49.704110    84.9  \n",
       "24               15.0    5.0  36.213699     0.0  \n",
       "25               15.0    5.0  36.213699     0.0  \n",
       "26               15.0    5.0  36.213699     0.0  \n",
       "27               15.0    5.0  36.213699     0.0  \n",
       "28               15.0    5.0  36.213699     0.0  \n",
       "29               15.0    5.0  36.213699     0.0  \n",
       "30               15.0    5.0  36.213699     0.0  \n",
       "31               15.0    5.0  36.213699     0.0  \n",
       "32               14.0    4.0  36.213699     0.0  \n",
       "33               14.0    4.0  36.213699     0.0  \n",
       "34               14.0    4.0  36.213699     0.0  \n",
       "35               14.0    4.0  36.213699     0.0  \n",
       "36               14.0    4.0  36.213699     0.0  \n",
       "37               14.0    4.0  36.213699     0.0  \n",
       "38               14.0    4.0  36.213699     0.0  \n",
       "39               14.0    4.0  36.213699     0.0  \n",
       "40               14.0    4.0  36.213699     0.0  \n",
       "41               13.0    4.0  36.213699     0.0  \n",
       "42               12.0    4.0  36.213699     0.0  \n",
       "43               12.0    4.0  36.213699     0.0  \n",
       "44               11.0    4.0  36.213699     0.0  \n",
       "45               11.0    4.0  36.213699     0.0  \n",
       "46               12.0    4.0  36.213699     0.0  \n",
       "47               12.0    4.0  36.213699     0.0  \n",
       "48               15.0    5.0  69.610959     3.1  \n",
       "49               15.0    5.0  69.610959     3.1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_OneBigDataset_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_OneBigDataset_test = np.array(result_OneBigDataset_test.drop(['Hours'],axis=1))\n",
    "scaler = StandardScaler()\n",
    "result_OneBigDataset_test_tranformed = scaler.fit_transform(result_OneBigDataset_test)\n",
    "test_input = result_OneBigDataset_test_tranformed.reshape(y_true_test.shape[0], 24, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00393468,  0.49202727, -0.89175725, ...,  0.6607166 ,\n",
       "         -0.81395485,  0.73924543],\n",
       "        [-0.00393468, -0.0494832 , -0.71248693, ...,  0.6607166 ,\n",
       "         -0.81395485,  0.73924543],\n",
       "        [-0.00389908,  0.10093476, -1.6088385 , ...,  0.6607166 ,\n",
       "         -0.81395485,  0.73924543],\n",
       "        ...,\n",
       "        [-0.00379226, -0.16982047, -0.71248693, ...,  0.6607166 ,\n",
       "         -0.81395485,  0.73924543],\n",
       "        [-0.00372105, -0.57595333, -0.89175725, ...,  0.6607166 ,\n",
       "         -0.81395485,  0.73924543],\n",
       "        [-0.00372105, -0.57595333, -0.89175725, ...,  0.6607166 ,\n",
       "         -0.81395485,  0.73924543]],\n",
       "\n",
       "       [[-0.00297335, -0.50074193,  0.72167558, ...,  0.6607166 ,\n",
       "         -1.57832581, -0.33165456],\n",
       "        [-0.00315137, -0.69628594,  0.36313495, ...,  0.6607166 ,\n",
       "         -1.57832581, -0.33165456],\n",
       "        [-0.00304456, -0.56090815,  0.18386464, ...,  0.6607166 ,\n",
       "         -1.57832581, -0.33165456],\n",
       "        ...,\n",
       "        [-0.00304456, -0.45561606, -0.53321662, ...,  0.08607715,\n",
       "         -1.57832581, -0.33165456],\n",
       "        [-0.00308016, -0.56090815, -0.17467599, ...,  0.08607715,\n",
       "         -1.57832581, -0.33165456],\n",
       "        [-0.00322258, -0.44057571, -0.89175725, ...,  0.08607715,\n",
       "         -1.57832581, -0.33165456]],\n",
       "\n",
       "       [[-0.00357863, -0.36536431, -0.89175725, ...,  0.6607166 ,\n",
       "          0.31397353, -0.29255221],\n",
       "        [-0.00354303, -0.09460908, -0.89175725, ...,  0.6607166 ,\n",
       "          0.31397353, -0.29255221],\n",
       "        [-0.003365  , -0.36536431, -0.89175725, ...,  0.6607166 ,\n",
       "          0.31397353, -0.29255221],\n",
       "        ...,\n",
       "        [-0.0026529 , -0.13973495,  0.36313495, ...,  0.6607166 ,\n",
       "          0.31397353, -0.29255221],\n",
       "        [-0.00283093, -0.22998669,  0.9009459 , ...,  0.6607166 ,\n",
       "          0.31397353, -0.29255221],\n",
       "        [-0.00275972, -0.68124542,  1.43875684, ...,  0.6607166 ,\n",
       "          0.31397353, -0.29255221]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00279532, -0.0494832 ,  0.00459433, ...,  0.6607166 ,\n",
       "         -1.62598257, -0.33165456],\n",
       "        [-0.00304456, -0.32023844, -0.3539463 , ...,  0.6607166 ,\n",
       "         -1.62598257, -0.33165456],\n",
       "        [-0.00318698, -0.50074193, -0.3539463 , ...,  0.6607166 ,\n",
       "         -1.62598257, -0.33165456],\n",
       "        ...,\n",
       "        [-0.00300895, -0.69628594,  0.00459433, ...,  0.6607166 ,\n",
       "         -1.62598257, -0.33165456],\n",
       "        [-0.00297335, -0.84670839,  0.36313495, ...,  0.6607166 ,\n",
       "         -1.62598257, -0.33165456],\n",
       "        [-0.00297335, -0.84670839,  0.36313495, ...,  0.6607166 ,\n",
       "         -1.62598257, -0.33165456]],\n",
       "\n",
       "       [[-0.00279532, -0.0494832 ,  0.00459433, ...,  0.6607166 ,\n",
       "          0.09354162, -0.33165456],\n",
       "        [-0.00279532, -0.0494832 ,  0.00459433, ...,  0.6607166 ,\n",
       "          0.09354162, -0.33165456],\n",
       "        [-0.00279532, -0.0494832 ,  0.00459433, ...,  0.6607166 ,\n",
       "          0.09354162, -0.33165456],\n",
       "        ...,\n",
       "        [-0.00357863, -0.22998669,  1.79729747, ...,  0.6607166 ,\n",
       "          0.09354162, -0.33165456],\n",
       "        [-0.00297335, -0.45561606,  1.97656778, ...,  0.6607166 ,\n",
       "          0.09354162, -0.33165456],\n",
       "        [-0.00308016, -0.18486082,  1.61802715, ...,  0.6607166 ,\n",
       "          0.09354162, -0.33165456]],\n",
       "\n",
       "       [[-0.00279532,  0.17614616,  0.00459433, ...,  0.6607166 ,\n",
       "          0.82484778, -0.33165456],\n",
       "        [-0.00354303,  0.49202727, -0.89175725, ...,  0.6607166 ,\n",
       "          0.82484778, -0.33165456],\n",
       "        [-0.00340061,  0.35664965, -0.53321662, ...,  0.6607166 ,\n",
       "          0.82484778, -0.33165456],\n",
       "        ...,\n",
       "        [-0.00340061,  0.17614616, -0.17467599, ...,  0.6607166 ,\n",
       "          0.82484778, -0.33165456],\n",
       "        [-0.00364984, -0.63611955, -0.17467599, ...,  0.6607166 ,\n",
       "          0.82484778, -0.33165456],\n",
       "        [-0.00357863, -0.36536431,  0.18386464, ...,  0.6607166 ,\n",
       "          0.82484778, -0.33165456]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5282, 24, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_test = np.array(y_true_test)\n",
    "y_true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5282, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5282,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_true_test.ravel()\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para Cross Validation, usar el dataset completo # Arthur\n",
    "y = (np.concatenate([y_true, y_true_test], axis=0))\n",
    "X = (np.concatenate([result_OneBigDataset_tranformed, result_OneBigDataset_test_tranformed], axis=0)).reshape(y.shape[0], 24, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35128, 24, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35128, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_output_dir = '/data/codi/OASIS/timesSeriesv2_cohorte/data/lengthOfStay/CV/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM without class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_input, y_true, epochs=8, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model.predict(test_input, verbose=1) #return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (test_output.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "print('labels(0 short stay, 1 long stay) predicted: ', y_pred)\n",
    "print('true labels: ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ac=np.round(metrics.accuracy_score(y_test, y_pred)*100,4)\n",
    "print(\"Accuracy test:\",test_ac)\n",
    "auroc = metrics.roc_auc_score(y_test, test_output)\n",
    "print(\"AUC-ROC: \", auroc)\n",
    "(precisions, recalls, thresholds) = metrics.precision_recall_curve(y_test, test_output)\n",
    "auprc = metrics.auc(recalls, precisions)\n",
    "print(\"AUC-PR: \", auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format((test_ac))\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(test_ac)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classWeight = class_weight.compute_class_weight('balanced',np.unique(np.ravel(y_true)),np.ravel(y_true))\n",
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classWeight = {i : classWeight[i] for i in range(2)}  #convert to dictionary in order to fit to keras model\n",
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_input, y_true, epochs=8, verbose=1,class_weight=classWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test Dataset, same process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model.predict(test_input, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (test_output.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('labels(0 short stay, 1 long stay) predicted: ', y_pred)\n",
    "print('true labels: ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ac=np.round(metrics.accuracy_score(y_test, y_pred)*100,4)\n",
    "print(\"Accuracy test:\",test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manera 2 de calcular Accuracy:\n",
    "#https://keras.io/api/metrics/accuracy_metrics/\n",
    "#import tensorflow as tf\n",
    "#m = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "#m.update_state(y_true_test,test_output)\n",
    "#score = m.result().numpy()\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = metrics.roc_auc_score(y_test, test_output)\n",
    "print(\"AUC-ROC: \", auroc)\n",
    "(precisions, recalls, thresholds) = metrics.precision_recall_curve(y_test, test_output)\n",
    "auprc = metrics.auc(recalls, precisions)\n",
    "print(\"AUC-PR: \", auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format((test_ac))\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(test_ac)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arthur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def kfoldclustered(classifier, X, y, weighted=False):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f = 0;\n",
    "    aucprs = []\n",
    "    aucrocs = []\n",
    "    accuracies = []\n",
    "    for train, test in cv.split(X, y): #train and test are indexes\n",
    "        f += 1\n",
    "        print('KFold ',f,' ---')\n",
    "        train_input = X[train]\n",
    "        test_input = X[test]\n",
    "        y_true = y[train]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        print(\"---- Clustering Train ---- \")\n",
    "        # Apply TSCK\n",
    "        n_clusters = 3\n",
    "        kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", n_init=1, max_iter=10, random_state=42)\n",
    "        # Fit to the training data\n",
    "        kmeans.fit(train_input)\n",
    "        # Generate out clusters\n",
    "        train_cluster = kmeans.predict(train_input)\n",
    "        # Add predicted cluster and y regression label to our training DataFrame\n",
    "        train_df = list(zip(train_cluster, y_true, train_input))\n",
    "        ls = sorted(train_df, key=lambda t: t[0])\n",
    "        # Unzip sorted data\n",
    "        cluster, y_, data = zip(*ls)\n",
    "        data = np.array(data)\n",
    "        y_ = np.array(y_)\n",
    "        # Getting indexes of clusters division\n",
    "        c = 0\n",
    "        ind = []\n",
    "        for i in range(len(ls)):\n",
    "            if ls[i][0] > c:\n",
    "                c = ls[i][0]\n",
    "                ind.append(i)\n",
    "        # Removing clusters with less than 10 samples\n",
    "        d1 = ind[0]\n",
    "        d2 = ind[1] - ind[0]\n",
    "        d3 = y_true.shape[0] - ind[1]\n",
    "        cluster_centers_ = kmeans.cluster_centers_\n",
    "        if d1 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 1\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[:ind[0]] = c\n",
    "            ind = ind[1:]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 0, 0)\n",
    "        elif d2 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[ind[0]:ind[1]] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 1, 0)\n",
    "        elif d3 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 1\n",
    "            train_cluster[ind[1]:] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 2, 0)\n",
    "            \n",
    "        print(\"-------- LSTM fitting for each cluster --------\")\n",
    "        i=0\n",
    "        if weighted:\n",
    "            classWeight = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                classes=np.unique(np.ravel(y_true)),\n",
    "                                                y=np.ravel(y_true))\n",
    "            classWeight = {i : classWeight[i] for i in range(2)}\n",
    "        for index in range(len(ind)+1):\n",
    "            if index == 0:\n",
    "                cluster_X = data[:ind[index],:,:]\n",
    "                cluster_Y = y_[:ind[index]]\n",
    "            elif index == len(ind):\n",
    "                cluster_X = data[ind[index-1]:,:,:]\n",
    "                cluster_Y = y_[ind[index-1]:]\n",
    "            else:\n",
    "                cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "                cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "            print(cluster_X.shape)\n",
    "            \n",
    "            # LSTM\n",
    "            model = Sequential()\n",
    "            model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "            # LSTM\n",
    "            if weighted == False:\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_tsck{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "            # LSTM_W\n",
    "            else:\n",
    "                print(\"PASSOU\")\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_tsck{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, class_weight=classWeight, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "                \n",
    "        testkfold(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted)\n",
    "    return aucprs, aucrocs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def testkfold(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted):\n",
    "    # Create initial test data to store assigned clusters\n",
    "    test_df = list(zip(train_cluster, y_test, test_input))\n",
    "    # Test sample es asignado al cluster correspondiente mediante Distancia euclidiana y se aplica el modelo correspondiente\n",
    "    print(\"Assigning each test sample to the closest cluster centroid...\")\n",
    "    new_cluster = [0 for i in range(y_test.shape[0])]\n",
    "    for row in range(len(test_df)):\n",
    "        min_distance = float('inf')\n",
    "        closest_cluster = None\n",
    "        for k in range(cluster_centers_.shape[0]):\n",
    "            # Check if the assigned cluster has more than 100 samples\n",
    "            # if train_clusters_df[k].shape[0] > 100: # Probar sin limite\n",
    "            distance = np.linalg.norm(cluster_centers_[k]-test_df[row][2])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_cluster = k\n",
    "        # Assign cluster to test sample\n",
    "        new_cluster[row] = closest_cluster\n",
    "    # Sort test data\n",
    "    test_df = list(zip(new_cluster, y_test, test_input))\n",
    "    ls_test = sorted(test_df, key=lambda t: t[0])\n",
    "    # Unzip sorted data\n",
    "    cluster_t, y_t, data_t = zip(*ls_test)\n",
    "    data_t = np.array(data_t)\n",
    "    y_t = np.array(y_t)\n",
    "    # Getting indexes\n",
    "    c = 0\n",
    "    ind_t = []\n",
    "    for i in range(len(ls_test)):\n",
    "        if ls_test[i][0] > c:\n",
    "            c = ls_test[i][0]\n",
    "            ind_t.append(i)\n",
    "    print(\"-------- Train metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data[:ind[index],:,:]\n",
    "            cluster_Y = y_[:ind[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data[ind[index-1]:,:,:]\n",
    "            cluster_Y = y_[ind[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "            cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_tsck{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_tsck{}.h5'.format(f,i))\n",
    "        \n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            train_X_probs = np.array(classes_prob)\n",
    "            y_train = np.array(cluster_Y)\n",
    "        else: \n",
    "            train_X_probs = np.concatenate((train_X_probs, classes_prob))\n",
    "            y_train = np.concatenate((y_train, cluster_Y))\n",
    "        i += 1\n",
    "\n",
    "    # Test metrics\n",
    "    print(\"-------- Test metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind_t)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data_t[:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[:ind_t[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data_t[ind_t[index-1]:,:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data_t[ind_t[index-1]:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:ind_t[index]]\n",
    "            \n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_tsck{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_tsck{}.h5'.format(f,i))\n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            test_X_probs = np.array(classes_prob)\n",
    "            test_y = np.array(cluster_Y)\n",
    "        else: \n",
    "            test_X_probs = np.concatenate((test_X_probs, classes_prob))\n",
    "            test_y = np.concatenate((test_y, cluster_Y))\n",
    "        print('----- cluster ',i,' -----')\n",
    "        print(cluster_X.shape)\n",
    "        # Test metrics\n",
    "        y_pred1 = (classes_prob.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "        test_ac1=np.round(metrics.accuracy_score(cluster_Y, y_pred1)*100,4)\n",
    "        print(\"Accuracy test cluster \",i,\":\", test_ac1)\n",
    "        i += 1\n",
    "    print(\"------ Metrics ------\")\n",
    "    # Train metrics\n",
    "    y_pred_train = (train_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred_train)\n",
    "    print('true labels: ', y_train)\n",
    "    # Test metrics\n",
    "    y_pred = (test_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred)\n",
    "    print('true labels: ', test_y)\n",
    "    #\n",
    "    train_ac=np.round(metrics.accuracy_score(y_train, y_pred_train)*100,4)\n",
    "    print(\"Accuracy train:\", train_ac)\n",
    "    test_ac=np.round(metrics.accuracy_score(test_y, y_pred)*100,4)\n",
    "    print(\"Accuracy test:\", test_ac)\n",
    "    auroc = metrics.roc_auc_score(test_y, test_X_probs)\n",
    "    print(\"AUC-ROC: \", auroc)\n",
    "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(test_y, test_X_probs)\n",
    "    auprc = metrics.auc(recalls, precisions)\n",
    "    print(\"AUC-PR: \", auprc)\n",
    "    aucprs.append(auprc)\n",
    "    aucrocs.append(auroc)\n",
    "    accuracies.append(test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(12979, 24, 10)\n",
      "Epoch 1/10\n",
      "345/345 [==============================] - 10s 20ms/step - loss: 0.6391 - accuracy: 0.6446 - val_loss: 0.6148 - val_accuracy: 0.6795\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.6065 - accuracy: 0.6894 - val_loss: 0.6090 - val_accuracy: 0.6800\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.6031 - accuracy: 0.6996 - val_loss: 0.6054 - val_accuracy: 0.6980\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.6015 - accuracy: 0.7034 - val_loss: 0.6047 - val_accuracy: 0.6975\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.5987 - accuracy: 0.7040 - val_loss: 0.6041 - val_accuracy: 0.6990\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.5962 - accuracy: 0.7048 - val_loss: 0.6039 - val_accuracy: 0.6995\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.5938 - accuracy: 0.7059 - val_loss: 0.6039 - val_accuracy: 0.6995\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.5959 - accuracy: 0.7044 - val_loss: 0.6039 - val_accuracy: 0.6990\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 7s 19ms/step - loss: 0.5922 - accuracy: 0.7069 - val_loss: 0.6058 - val_accuracy: 0.7006\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 7s 20ms/step - loss: 0.5933 - accuracy: 0.7052 - val_loss: 0.6046 - val_accuracy: 0.6985\n",
      "(9129, 24, 10)\n",
      "Epoch 1/10\n",
      "243/243 [==============================] - 8s 21ms/step - loss: 0.6042 - accuracy: 0.6980 - val_loss: 0.5705 - val_accuracy: 0.7022\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5666 - accuracy: 0.7235 - val_loss: 0.5418 - val_accuracy: 0.7599\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5534 - accuracy: 0.7440 - val_loss: 0.5322 - val_accuracy: 0.7664\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5481 - accuracy: 0.7483 - val_loss: 0.5280 - val_accuracy: 0.7708\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5476 - accuracy: 0.7489 - val_loss: 0.5271 - val_accuracy: 0.7686\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5439 - accuracy: 0.7510 - val_loss: 0.5258 - val_accuracy: 0.7686\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5437 - accuracy: 0.7532 - val_loss: 0.5261 - val_accuracy: 0.7701\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5406 - accuracy: 0.7543 - val_loss: 0.5251 - val_accuracy: 0.7679\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5439 - accuracy: 0.7510 - val_loss: 0.5260 - val_accuracy: 0.7686\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 5s 20ms/step - loss: 0.5427 - accuracy: 0.7505 - val_loss: 0.5248 - val_accuracy: 0.7664\n",
      "(5994, 24, 10)\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 6s 22ms/step - loss: 0.6305 - accuracy: 0.6602 - val_loss: 0.6112 - val_accuracy: 0.6778\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6188 - accuracy: 0.6708 - val_loss: 0.6021 - val_accuracy: 0.6856\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6061 - accuracy: 0.6928 - val_loss: 0.5949 - val_accuracy: 0.7078\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5996 - accuracy: 0.7040 - val_loss: 0.5919 - val_accuracy: 0.7211\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5930 - accuracy: 0.7130 - val_loss: 0.5891 - val_accuracy: 0.7256\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5928 - accuracy: 0.7140 - val_loss: 0.5873 - val_accuracy: 0.7256\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5901 - accuracy: 0.7118 - val_loss: 0.5872 - val_accuracy: 0.7244\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5861 - accuracy: 0.7165 - val_loss: 0.5858 - val_accuracy: 0.7233\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5848 - accuracy: 0.7157 - val_loss: 0.5862 - val_accuracy: 0.7244\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.5845 - accuracy: 0.7177 - val_loss: 0.5868 - val_accuracy: 0.7222\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "406/406 [==============================] - 3s 6ms/step\n",
      "286/286 [==============================] - 2s 6ms/step\n",
      "188/188 [==============================] - 1s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "107/107 [==============================] - 1s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(3406, 24, 10)\n",
      "Accuracy test cluster  0 : 70.0822\n",
      "66/66 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(2082, 24, 10)\n",
      "Accuracy test cluster  1 : 76.7531\n",
      "49/49 [==============================] - 1s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(1538, 24, 10)\n",
      "Accuracy test cluster  2 : 71.0013\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 72.5856\n",
      "Accuracy test: 72.2602\n",
      "AUC-ROC:  0.7432762046323007\n",
      "AUC-PR:  0.6613680322003546\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(19120, 24, 10)\n",
      "Epoch 1/10\n",
      "508/508 [==============================] - 13s 21ms/step - loss: 0.6049 - accuracy: 0.6903 - val_loss: 0.5785 - val_accuracy: 0.7158\n",
      "Epoch 2/10\n",
      "508/508 [==============================] - 10s 20ms/step - loss: 0.5780 - accuracy: 0.7219 - val_loss: 0.5752 - val_accuracy: 0.7218\n",
      "Epoch 3/10\n",
      "508/508 [==============================] - 10s 20ms/step - loss: 0.5755 - accuracy: 0.7251 - val_loss: 0.5743 - val_accuracy: 0.7242\n",
      "Epoch 4/10\n",
      "508/508 [==============================] - 10s 20ms/step - loss: 0.5724 - accuracy: 0.7265 - val_loss: 0.5746 - val_accuracy: 0.7252\n",
      "Epoch 5/10\n",
      "508/508 [==============================] - 10s 20ms/step - loss: 0.5712 - accuracy: 0.7272 - val_loss: 0.5747 - val_accuracy: 0.7266\n",
      "Epoch 6/10\n",
      "506/508 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7260Restoring model weights from the end of the best epoch: 3.\n",
      "508/508 [==============================] - 10s 20ms/step - loss: 0.5713 - accuracy: 0.7264 - val_loss: 0.5768 - val_accuracy: 0.7238\n",
      "Epoch 6: early stopping\n",
      "(2490, 24, 10)\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 27ms/step - loss: 0.6712 - accuracy: 0.5922 - val_loss: 0.6025 - val_accuracy: 0.7139\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.6014 - accuracy: 0.7131 - val_loss: 0.5884 - val_accuracy: 0.7139\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 0.5947 - accuracy: 0.7146 - val_loss: 0.5868 - val_accuracy: 0.7139\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5884 - accuracy: 0.7155 - val_loss: 0.5853 - val_accuracy: 0.7139\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5847 - accuracy: 0.7141 - val_loss: 0.5839 - val_accuracy: 0.7139\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5870 - accuracy: 0.7146 - val_loss: 0.5827 - val_accuracy: 0.7139\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5808 - accuracy: 0.7160 - val_loss: 0.5814 - val_accuracy: 0.7139\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5809 - accuracy: 0.7150 - val_loss: 0.5806 - val_accuracy: 0.7166\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5766 - accuracy: 0.7160 - val_loss: 0.5790 - val_accuracy: 0.7166\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5816 - accuracy: 0.7136 - val_loss: 0.5781 - val_accuracy: 0.7166\n",
      "(6492, 24, 10)\n",
      "Epoch 1/10\n",
      "173/173 [==============================] - 6s 23ms/step - loss: 0.6764 - accuracy: 0.5758 - val_loss: 0.6407 - val_accuracy: 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.6328 - accuracy: 0.6539 - val_loss: 0.6007 - val_accuracy: 0.7156\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.6085 - accuracy: 0.6919 - val_loss: 0.5872 - val_accuracy: 0.7146\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.6021 - accuracy: 0.6997 - val_loss: 0.5813 - val_accuracy: 0.7177\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.5990 - accuracy: 0.7042 - val_loss: 0.5813 - val_accuracy: 0.7166\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.5940 - accuracy: 0.7033 - val_loss: 0.5801 - val_accuracy: 0.7166\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.5956 - accuracy: 0.7073 - val_loss: 0.5778 - val_accuracy: 0.7228\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.5959 - accuracy: 0.7048 - val_loss: 0.5780 - val_accuracy: 0.7207\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 3s 19ms/step - loss: 0.5950 - accuracy: 0.7090 - val_loss: 0.5770 - val_accuracy: 0.7187\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 3s 20ms/step - loss: 0.5930 - accuracy: 0.7108 - val_loss: 0.5757 - val_accuracy: 0.7207\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "598/598 [==============================] - 4s 6ms/step\n",
      "78/78 [==============================] - 1s 6ms/step\n",
      "203/203 [==============================] - 1s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "150/150 [==============================] - 1s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(4784, 24, 10)\n",
      "Accuracy test cluster  0 : 73.1814\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(540, 24, 10)\n",
      "Accuracy test cluster  1 : 70.1852\n",
      "54/54 [==============================] - 1s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(1702, 24, 10)\n",
      "Accuracy test cluster  2 : 69.8002\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 72.3329\n",
      "Accuracy test: 72.1321\n",
      "AUC-ROC:  0.7435762780123726\n",
      "AUC-PR:  0.6589476165051861\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(5320, 24, 10)\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 6s 23ms/step - loss: 0.7072 - accuracy: 0.5130 - val_loss: 0.6483 - val_accuracy: 0.6040\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.6406 - accuracy: 0.6479 - val_loss: 0.6066 - val_accuracy: 0.7218\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6121 - accuracy: 0.6860 - val_loss: 0.5805 - val_accuracy: 0.7381\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.5999 - accuracy: 0.6988 - val_loss: 0.5681 - val_accuracy: 0.7356\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.5912 - accuracy: 0.7072 - val_loss: 0.5630 - val_accuracy: 0.7356\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5882 - accuracy: 0.7152 - val_loss: 0.5612 - val_accuracy: 0.7406\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 4s 26ms/step - loss: 0.5912 - accuracy: 0.7107 - val_loss: 0.5604 - val_accuracy: 0.7406\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5872 - accuracy: 0.7116 - val_loss: 0.5604 - val_accuracy: 0.7343\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.5791 - accuracy: 0.7176 - val_loss: 0.5595 - val_accuracy: 0.7331\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 4s 29ms/step - loss: 0.5825 - accuracy: 0.7167 - val_loss: 0.5587 - val_accuracy: 0.7356\n",
      "(18874, 24, 10)\n",
      "Epoch 1/10\n",
      "502/502 [==============================] - 17s 28ms/step - loss: 0.5879 - accuracy: 0.7164 - val_loss: 0.5779 - val_accuracy: 0.7172\n",
      "Epoch 2/10\n",
      "502/502 [==============================] - 11s 22ms/step - loss: 0.5722 - accuracy: 0.7274 - val_loss: 0.5741 - val_accuracy: 0.7242\n",
      "Epoch 3/10\n",
      "502/502 [==============================] - 10s 19ms/step - loss: 0.5678 - accuracy: 0.7293 - val_loss: 0.5737 - val_accuracy: 0.7253\n",
      "Epoch 4/10\n",
      "502/502 [==============================] - 10s 20ms/step - loss: 0.5663 - accuracy: 0.7297 - val_loss: 0.5734 - val_accuracy: 0.7285\n",
      "Epoch 5/10\n",
      "502/502 [==============================] - 11s 22ms/step - loss: 0.5656 - accuracy: 0.7307 - val_loss: 0.5731 - val_accuracy: 0.7292\n",
      "Epoch 6/10\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.5656 - accuracy: 0.7290 - val_loss: 0.5734 - val_accuracy: 0.7299\n",
      "Epoch 7/10\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.5629 - accuracy: 0.7316 - val_loss: 0.5745 - val_accuracy: 0.7281\n",
      "Epoch 8/10\n",
      "500/502 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7314Restoring model weights from the end of the best epoch: 5.\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.5629 - accuracy: 0.7312 - val_loss: 0.5748 - val_accuracy: 0.7278\n",
      "Epoch 8: early stopping\n",
      "(3908, 24, 10)\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 5s 20ms/step - loss: 0.6458 - accuracy: 0.6483 - val_loss: 0.6326 - val_accuracy: 0.6457\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6318 - accuracy: 0.6600 - val_loss: 0.6229 - val_accuracy: 0.6644\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6244 - accuracy: 0.6775 - val_loss: 0.6169 - val_accuracy: 0.6746\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6149 - accuracy: 0.6820 - val_loss: 0.6138 - val_accuracy: 0.6814\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6105 - accuracy: 0.6886 - val_loss: 0.6120 - val_accuracy: 0.6865\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6136 - accuracy: 0.6862 - val_loss: 0.6109 - val_accuracy: 0.6882\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6021 - accuracy: 0.6950 - val_loss: 0.6100 - val_accuracy: 0.6848\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6051 - accuracy: 0.6983 - val_loss: 0.6119 - val_accuracy: 0.6882\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.6022 - accuracy: 0.6941 - val_loss: 0.6099 - val_accuracy: 0.6865\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.6040 - accuracy: 0.6947 - val_loss: 0.6104 - val_accuracy: 0.6882\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "167/167 [==============================] - 1s 4ms/step\n",
      "590/590 [==============================] - 4s 6ms/step\n",
      "123/123 [==============================] - 1s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "57/57 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(1806, 24, 10)\n",
      "Accuracy test cluster  0 : 70.9856\n",
      "153/153 [==============================] - 2s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(4881, 24, 10)\n",
      "Accuracy test cluster  1 : 71.8705\n",
      "11/11 [==============================] - 0s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(339, 24, 10)\n",
      "Accuracy test cluster  2 : 71.6814\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 72.6212\n",
      "Accuracy test: 71.6339\n",
      "AUC-ROC:  0.7349363226432507\n",
      "AUC-PR:  0.6474585594591604\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(11982, 24, 10)\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 14s 34ms/step - loss: 0.5930 - accuracy: 0.7017 - val_loss: 0.5499 - val_accuracy: 0.7586\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 10s 30ms/step - loss: 0.5550 - accuracy: 0.7450 - val_loss: 0.5388 - val_accuracy: 0.7614\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 5s 16ms/step - loss: 0.5501 - accuracy: 0.7496 - val_loss: 0.5369 - val_accuracy: 0.7625\n",
      "Epoch 4/10\n",
      "319/319 [==============================] - 6s 18ms/step - loss: 0.5459 - accuracy: 0.7508 - val_loss: 0.5352 - val_accuracy: 0.7659\n",
      "Epoch 5/10\n",
      "319/319 [==============================] - 8s 25ms/step - loss: 0.5451 - accuracy: 0.7497 - val_loss: 0.5348 - val_accuracy: 0.7636\n",
      "Epoch 6/10\n",
      "319/319 [==============================] - 7s 22ms/step - loss: 0.5457 - accuracy: 0.7536 - val_loss: 0.5345 - val_accuracy: 0.7625\n",
      "Epoch 7/10\n",
      "319/319 [==============================] - 7s 21ms/step - loss: 0.5447 - accuracy: 0.7536 - val_loss: 0.5343 - val_accuracy: 0.7631\n",
      "Epoch 8/10\n",
      "319/319 [==============================] - 6s 19ms/step - loss: 0.5448 - accuracy: 0.7534 - val_loss: 0.5337 - val_accuracy: 0.7636\n",
      "Epoch 9/10\n",
      "319/319 [==============================] - 5s 16ms/step - loss: 0.5424 - accuracy: 0.7518 - val_loss: 0.5336 - val_accuracy: 0.7631\n",
      "Epoch 10/10\n",
      "319/319 [==============================] - 5s 16ms/step - loss: 0.5418 - accuracy: 0.7514 - val_loss: 0.5335 - val_accuracy: 0.7625\n",
      "(16121, 24, 10)\n",
      "Epoch 1/10\n",
      "429/429 [==============================] - 10s 17ms/step - loss: 0.6475 - accuracy: 0.6354 - val_loss: 0.6121 - val_accuracy: 0.6854\n",
      "Epoch 2/10\n",
      "429/429 [==============================] - 7s 17ms/step - loss: 0.6150 - accuracy: 0.6907 - val_loss: 0.6079 - val_accuracy: 0.6945\n",
      "Epoch 3/10\n",
      "429/429 [==============================] - 7s 16ms/step - loss: 0.6104 - accuracy: 0.6961 - val_loss: 0.6069 - val_accuracy: 0.6924\n",
      "Epoch 4/10\n",
      "429/429 [==============================] - 7s 17ms/step - loss: 0.6083 - accuracy: 0.6957 - val_loss: 0.6062 - val_accuracy: 0.6928\n",
      "Epoch 5/10\n",
      "429/429 [==============================] - 8s 19ms/step - loss: 0.6059 - accuracy: 0.6969 - val_loss: 0.6060 - val_accuracy: 0.6941\n",
      "Epoch 6/10\n",
      "429/429 [==============================] - 7s 17ms/step - loss: 0.6039 - accuracy: 0.6976 - val_loss: 0.6055 - val_accuracy: 0.6933\n",
      "Epoch 7/10\n",
      "429/429 [==============================] - 7s 17ms/step - loss: 0.6063 - accuracy: 0.6973 - val_loss: 0.6056 - val_accuracy: 0.6941\n",
      "Epoch 8/10\n",
      "429/429 [==============================] - 7s 17ms/step - loss: 0.6048 - accuracy: 0.6969 - val_loss: 0.6052 - val_accuracy: 0.6928\n",
      "Epoch 9/10\n",
      "429/429 [==============================] - 8s 18ms/step - loss: 0.6045 - accuracy: 0.6989 - val_loss: 0.6055 - val_accuracy: 0.6920\n",
      "Epoch 10/10\n",
      "429/429 [==============================] - 8s 18ms/step - loss: 0.6023 - accuracy: 0.6981 - val_loss: 0.6048 - val_accuracy: 0.6916\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "375/375 [==============================] - 2s 5ms/step\n",
      "504/504 [==============================] - 3s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "98/98 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(3111, 24, 10)\n",
      "Accuracy test cluster  0 : 76.0206\n",
      "123/123 [==============================] - 1s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(3914, 24, 10)\n",
      "Accuracy test cluster  1 : 71.5125\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 1. 0. ... 0. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 72.3517\n",
      "Accuracy test: 73.5089\n",
      "AUC-ROC:  0.7545235331534101\n",
      "AUC-PR:  0.6666725268965457\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(10438, 24, 10)\n",
      "Epoch 1/10\n",
      "278/278 [==============================] - 12s 30ms/step - loss: 0.6062 - accuracy: 0.6810 - val_loss: 0.5496 - val_accuracy: 0.7503\n",
      "Epoch 2/10\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 0.5563 - accuracy: 0.7378 - val_loss: 0.5299 - val_accuracy: 0.7676\n",
      "Epoch 3/10\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 0.5474 - accuracy: 0.7475 - val_loss: 0.5260 - val_accuracy: 0.7637\n",
      "Epoch 4/10\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 0.5458 - accuracy: 0.7491 - val_loss: 0.5237 - val_accuracy: 0.7682\n",
      "Epoch 5/10\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 0.5429 - accuracy: 0.7530 - val_loss: 0.5228 - val_accuracy: 0.7695\n",
      "Epoch 6/10\n",
      "278/278 [==============================] - 7s 25ms/step - loss: 0.5418 - accuracy: 0.7510 - val_loss: 0.5223 - val_accuracy: 0.7701\n",
      "Epoch 7/10\n",
      "278/278 [==============================] - 6s 21ms/step - loss: 0.5428 - accuracy: 0.7538 - val_loss: 0.5223 - val_accuracy: 0.7663\n",
      "Epoch 8/10\n",
      "278/278 [==============================] - 7s 25ms/step - loss: 0.5397 - accuracy: 0.7524 - val_loss: 0.5219 - val_accuracy: 0.7688\n",
      "Epoch 9/10\n",
      "278/278 [==============================] - 9s 31ms/step - loss: 0.5412 - accuracy: 0.7538 - val_loss: 0.5227 - val_accuracy: 0.7676\n",
      "Epoch 10/10\n",
      "278/278 [==============================] - 11s 38ms/step - loss: 0.5393 - accuracy: 0.7532 - val_loss: 0.5225 - val_accuracy: 0.7682\n",
      "(3637, 24, 10)\n",
      "Epoch 1/10\n",
      "97/97 [==============================] - 14s 35ms/step - loss: 0.6211 - accuracy: 0.6833 - val_loss: 0.5818 - val_accuracy: 0.7234\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 0.6089 - accuracy: 0.6865 - val_loss: 0.5755 - val_accuracy: 0.7234\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 3s 28ms/step - loss: 0.6056 - accuracy: 0.6920 - val_loss: 0.5721 - val_accuracy: 0.7216\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 0.5957 - accuracy: 0.6962 - val_loss: 0.5671 - val_accuracy: 0.7216\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 0.5925 - accuracy: 0.7046 - val_loss: 0.5631 - val_accuracy: 0.7308\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.5852 - accuracy: 0.7095 - val_loss: 0.5580 - val_accuracy: 0.7326\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 0.5847 - accuracy: 0.7172 - val_loss: 0.5587 - val_accuracy: 0.7436\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.5778 - accuracy: 0.7282 - val_loss: 0.5565 - val_accuracy: 0.7436\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.5768 - accuracy: 0.7260 - val_loss: 0.5532 - val_accuracy: 0.7436\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5793 - accuracy: 0.7202 - val_loss: 0.5506 - val_accuracy: 0.7454\n",
      "(14028, 24, 10)\n",
      "Epoch 1/10\n",
      "373/373 [==============================] - 12s 22ms/step - loss: 0.6509 - accuracy: 0.6232 - val_loss: 0.6278 - val_accuracy: 0.6651\n",
      "Epoch 2/10\n",
      "373/373 [==============================] - 7s 19ms/step - loss: 0.6124 - accuracy: 0.6925 - val_loss: 0.6231 - val_accuracy: 0.6770\n",
      "Epoch 3/10\n",
      "373/373 [==============================] - 7s 19ms/step - loss: 0.6063 - accuracy: 0.6966 - val_loss: 0.6224 - val_accuracy: 0.6779\n",
      "Epoch 4/10\n",
      "373/373 [==============================] - 7s 19ms/step - loss: 0.6048 - accuracy: 0.6997 - val_loss: 0.6223 - val_accuracy: 0.6784\n",
      "Epoch 5/10\n",
      "373/373 [==============================] - 6s 17ms/step - loss: 0.6026 - accuracy: 0.6996 - val_loss: 0.6215 - val_accuracy: 0.6803\n",
      "Epoch 6/10\n",
      "373/373 [==============================] - 7s 18ms/step - loss: 0.6010 - accuracy: 0.6989 - val_loss: 0.6218 - val_accuracy: 0.6812\n",
      "Epoch 7/10\n",
      "373/373 [==============================] - 6s 17ms/step - loss: 0.6007 - accuracy: 0.7012 - val_loss: 0.6211 - val_accuracy: 0.6846\n",
      "Epoch 8/10\n",
      "373/373 [==============================] - 7s 20ms/step - loss: 0.5996 - accuracy: 0.7004 - val_loss: 0.6224 - val_accuracy: 0.6831\n",
      "Epoch 9/10\n",
      "373/373 [==============================] - 7s 19ms/step - loss: 0.6000 - accuracy: 0.7007 - val_loss: 0.6206 - val_accuracy: 0.6836\n",
      "Epoch 10/10\n",
      "373/373 [==============================] - 6s 16ms/step - loss: 0.5991 - accuracy: 0.7034 - val_loss: 0.6202 - val_accuracy: 0.6836\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "327/327 [==============================] - 2s 5ms/step\n",
      "114/114 [==============================] - 1s 5ms/step\n",
      "439/439 [==============================] - 2s 5ms/step\n",
      "-------- Test metrics ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(2523, 24, 10)\n",
      "Accuracy test cluster  0 : 75.6639\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "----- cluster  1  -----\n",
      "(1052, 24, 10)\n",
      "Accuracy test cluster  1 : 70.4373\n",
      "108/108 [==============================] - 1s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(3450, 24, 10)\n",
      "Accuracy test cluster  2 : 70.6377\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 72.5901\n",
      "Accuracy test: 72.4128\n",
      "AUC-ROC:  0.7337221920781335\n",
      "AUC-PR:  0.6354399796243597\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM\n",
    "aucprs, aucrocs, accuracies = kfoldclustered(\"LSTM\", X, y, weighted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aucpr scores: [0.6613680322003546, 0.6589476165051861, 0.6474585594591604, 0.6666725268965457, 0.6354399796243597]\n",
      "0.6540 mean aucpr with a standard deviation of 0.0112\n",
      "aucroc scores: [0.7432762046323007, 0.7435762780123726, 0.7349363226432507, 0.7545235331534101, 0.7337221920781335]\n",
      "0.7420 mean aucroc with a standard deviation of 0.0075\n",
      "accuracy scores: [72.2602, 72.1321, 71.6339, 73.5089, 72.4128]\n",
      "72.3896 mean accuracy with a standard deviation of 0.6177\n"
     ]
    }
   ],
   "source": [
    "print ('aucpr scores:', aucprs)\n",
    "print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs), np.std(aucprs)))\n",
    "\n",
    "print ('aucroc scores:', aucrocs)\n",
    "print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs), np.std(aucrocs)))\n",
    "\n",
    "print ('accuracy scores:', accuracies)\n",
    "print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(12979, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "345/345 [==============================] - 8s 17ms/step - loss: 0.6574 - accuracy: 0.6468 - val_loss: 0.6413 - val_accuracy: 0.6893\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 5s 15ms/step - loss: 0.6406 - accuracy: 0.6616 - val_loss: 0.6214 - val_accuracy: 0.6949\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 5s 15ms/step - loss: 0.6364 - accuracy: 0.6753 - val_loss: 0.6277 - val_accuracy: 0.6826\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 5s 15ms/step - loss: 0.6349 - accuracy: 0.6763 - val_loss: 0.6242 - val_accuracy: 0.6908\n",
      "Epoch 5/10\n",
      "343/345 [============================>.] - ETA: 0s - loss: 0.6309 - accuracy: 0.6808Restoring model weights from the end of the best epoch: 2.\n",
      "345/345 [==============================] - 5s 15ms/step - loss: 0.6307 - accuracy: 0.6811 - val_loss: 0.6379 - val_accuracy: 0.6810\n",
      "Epoch 5: early stopping\n",
      "(9129, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "243/243 [==============================] - 7s 17ms/step - loss: 0.6404 - accuracy: 0.6518 - val_loss: 0.5936 - val_accuracy: 0.7489\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5997 - accuracy: 0.7259 - val_loss: 0.5629 - val_accuracy: 0.7584\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5846 - accuracy: 0.7379 - val_loss: 0.5455 - val_accuracy: 0.7547\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5785 - accuracy: 0.7420 - val_loss: 0.5456 - val_accuracy: 0.7533\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5784 - accuracy: 0.7440 - val_loss: 0.5432 - val_accuracy: 0.7533\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5727 - accuracy: 0.7475 - val_loss: 0.5460 - val_accuracy: 0.7540\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5721 - accuracy: 0.7426 - val_loss: 0.5387 - val_accuracy: 0.7533\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5727 - accuracy: 0.7434 - val_loss: 0.5406 - val_accuracy: 0.7526\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5719 - accuracy: 0.7449 - val_loss: 0.5398 - val_accuracy: 0.7562\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 4s 16ms/step - loss: 0.5699 - accuracy: 0.7440 - val_loss: 0.5368 - val_accuracy: 0.7584\n",
      "(5994, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 6s 19ms/step - loss: 0.9044 - accuracy: 0.4806 - val_loss: 0.6240 - val_accuracy: 0.6756\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 2s 16ms/step - loss: 0.6329 - accuracy: 0.6622 - val_loss: 0.6234 - val_accuracy: 0.6778\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.6183 - accuracy: 0.6620 - val_loss: 0.6203 - val_accuracy: 0.6778\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.6155 - accuracy: 0.6665 - val_loss: 0.6155 - val_accuracy: 0.6778\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.6065 - accuracy: 0.6657 - val_loss: 0.6134 - val_accuracy: 0.6789\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.6028 - accuracy: 0.6716 - val_loss: 0.6107 - val_accuracy: 0.6844\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.5927 - accuracy: 0.6873 - val_loss: 0.6050 - val_accuracy: 0.6989\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.5881 - accuracy: 0.6941 - val_loss: 0.6054 - val_accuracy: 0.7100\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.5854 - accuracy: 0.7020 - val_loss: 0.5989 - val_accuracy: 0.7144\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.5856 - accuracy: 0.7053 - val_loss: 0.6023 - val_accuracy: 0.7133\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "406/406 [==============================] - 2s 4ms/step\n",
      "286/286 [==============================] - 2s 4ms/step\n",
      "188/188 [==============================] - 1s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "107/107 [==============================] - 1s 5ms/step\n",
      "66/66 [==============================] - 1s 5ms/step\n",
      "49/49 [==============================] - 1s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 72.1372\n",
      "Accuracy test: 71.8901\n",
      "AUC-ROC:  0.738998318143643\n",
      "AUC-PR:  0.6476107357022802\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(19120, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "508/508 [==============================] - 11s 16ms/step - loss: 0.6353 - accuracy: 0.6654 - val_loss: 0.5972 - val_accuracy: 0.7211\n",
      "Epoch 2/10\n",
      "508/508 [==============================] - 8s 15ms/step - loss: 0.6146 - accuracy: 0.7076 - val_loss: 0.6021 - val_accuracy: 0.7148\n",
      "Epoch 3/10\n",
      "508/508 [==============================] - 8s 15ms/step - loss: 0.6091 - accuracy: 0.7100 - val_loss: 0.6109 - val_accuracy: 0.7033\n",
      "Epoch 4/10\n",
      "505/508 [============================>.] - ETA: 0s - loss: 0.6089 - accuracy: 0.7096Restoring model weights from the end of the best epoch: 1.\n",
      "508/508 [==============================] - 8s 15ms/step - loss: 0.6083 - accuracy: 0.7102 - val_loss: 0.6025 - val_accuracy: 0.7123\n",
      "Epoch 4: early stopping\n",
      "(2490, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 4s 23ms/step - loss: 0.7771 - accuracy: 0.5695 - val_loss: 0.5946 - val_accuracy: 0.7112\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.5998 - accuracy: 0.7136 - val_loss: 0.5949 - val_accuracy: 0.7139\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.5857 - accuracy: 0.7155 - val_loss: 0.6012 - val_accuracy: 0.7139\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5845 - accuracy: 0.7146Restoring model weights from the end of the best epoch: 1.\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.5845 - accuracy: 0.7146 - val_loss: 0.6028 - val_accuracy: 0.7139\n",
      "Epoch 4: early stopping\n",
      "(6492, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "173/173 [==============================] - 6s 18ms/step - loss: 0.6758 - accuracy: 0.5627 - val_loss: 0.6626 - val_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 3s 16ms/step - loss: 0.6249 - accuracy: 0.6379 - val_loss: 0.6214 - val_accuracy: 0.6869\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.6101 - accuracy: 0.6803 - val_loss: 0.6214 - val_accuracy: 0.6879\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.6067 - accuracy: 0.6937 - val_loss: 0.6151 - val_accuracy: 0.6992\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.6013 - accuracy: 0.6970 - val_loss: 0.6088 - val_accuracy: 0.7002\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.6025 - accuracy: 0.6935 - val_loss: 0.6011 - val_accuracy: 0.7002\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 3s 16ms/step - loss: 0.5992 - accuracy: 0.6957 - val_loss: 0.6140 - val_accuracy: 0.7012\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.5995 - accuracy: 0.6968 - val_loss: 0.6025 - val_accuracy: 0.7023\n",
      "Epoch 9/10\n",
      "171/173 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.6933Restoring model weights from the end of the best epoch: 6.\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.5996 - accuracy: 0.6939 - val_loss: 0.6060 - val_accuracy: 0.7023\n",
      "Epoch 9: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "598/598 [==============================] - 3s 4ms/step\n",
      "78/78 [==============================] - 1s 5ms/step\n",
      "203/203 [==============================] - 1s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "54/54 [==============================] - 1s 4ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 71.9201\n",
      "Accuracy test: 71.5912\n",
      "AUC-ROC:  0.7397990248010862\n",
      "AUC-PR:  0.6515220395344019\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(5320, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 19ms/step - loss: 0.6947 - accuracy: 0.5708 - val_loss: 0.6601 - val_accuracy: 0.5451\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 2s 16ms/step - loss: 0.6242 - accuracy: 0.6303 - val_loss: 0.6106 - val_accuracy: 0.6842\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 0.5986 - accuracy: 0.6889 - val_loss: 0.5933 - val_accuracy: 0.7030\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 2s 16ms/step - loss: 0.5920 - accuracy: 0.6968 - val_loss: 0.5842 - val_accuracy: 0.7093\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 2s 16ms/step - loss: 0.5819 - accuracy: 0.7059 - val_loss: 0.5975 - val_accuracy: 0.7030\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 0.5895 - accuracy: 0.7034 - val_loss: 0.5804 - val_accuracy: 0.7105\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 0.5865 - accuracy: 0.7065 - val_loss: 0.5826 - val_accuracy: 0.7105\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 0.5808 - accuracy: 0.7077 - val_loss: 0.5824 - val_accuracy: 0.7118\n",
      "Epoch 9/10\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5880 - accuracy: 0.7023Restoring model weights from the end of the best epoch: 6.\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 0.5877 - accuracy: 0.7026 - val_loss: 0.5807 - val_accuracy: 0.7130\n",
      "Epoch 9: early stopping\n",
      "(18874, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "502/502 [==============================] - 10s 16ms/step - loss: 0.6203 - accuracy: 0.7002 - val_loss: 0.5927 - val_accuracy: 0.7207\n",
      "Epoch 2/10\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.6097 - accuracy: 0.7124 - val_loss: 0.6006 - val_accuracy: 0.7136\n",
      "Epoch 3/10\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.6048 - accuracy: 0.7187 - val_loss: 0.5910 - val_accuracy: 0.7172\n",
      "Epoch 4/10\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.6042 - accuracy: 0.7235 - val_loss: 0.5933 - val_accuracy: 0.7157\n",
      "Epoch 5/10\n",
      "502/502 [==============================] - 7s 15ms/step - loss: 0.6029 - accuracy: 0.7207 - val_loss: 0.5889 - val_accuracy: 0.7193\n",
      "Epoch 6/10\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.5997 - accuracy: 0.7232 - val_loss: 0.5954 - val_accuracy: 0.7147\n",
      "Epoch 7/10\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.6004 - accuracy: 0.7240 - val_loss: 0.5965 - val_accuracy: 0.7150\n",
      "Epoch 8/10\n",
      "502/502 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.7239Restoring model weights from the end of the best epoch: 5.\n",
      "502/502 [==============================] - 8s 15ms/step - loss: 0.6011 - accuracy: 0.7239 - val_loss: 0.5935 - val_accuracy: 0.7179\n",
      "Epoch 8: early stopping\n",
      "(3908, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 5s 24ms/step - loss: 0.6898 - accuracy: 0.6155 - val_loss: 0.6574 - val_accuracy: 0.6457\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6347 - accuracy: 0.6471 - val_loss: 0.6598 - val_accuracy: 0.6457\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6255 - accuracy: 0.6486 - val_loss: 0.6569 - val_accuracy: 0.6457\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6195 - accuracy: 0.6498 - val_loss: 0.6465 - val_accuracy: 0.6457\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6154 - accuracy: 0.6507 - val_loss: 0.6411 - val_accuracy: 0.6474\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6097 - accuracy: 0.6603 - val_loss: 0.6392 - val_accuracy: 0.6593\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6108 - accuracy: 0.6652 - val_loss: 0.6377 - val_accuracy: 0.6661\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6063 - accuracy: 0.6715 - val_loss: 0.6420 - val_accuracy: 0.6661\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6031 - accuracy: 0.6691 - val_loss: 0.6382 - val_accuracy: 0.6695\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.6053 - accuracy: 0.6781Restoring model weights from the end of the best epoch: 7.\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.6053 - accuracy: 0.6781 - val_loss: 0.6400 - val_accuracy: 0.6729\n",
      "Epoch 10: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "167/167 [==============================] - 1s 4ms/step\n",
      "590/590 [==============================] - 3s 5ms/step\n",
      "123/123 [==============================] - 1s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "57/57 [==============================] - 1s 4ms/step\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 71.7244\n",
      "Accuracy test: 71.1785\n",
      "AUC-ROC:  0.7343867905322874\n",
      "AUC-PR:  0.6433301815159652\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(11982, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 8s 17ms/step - loss: 0.6299 - accuracy: 0.6848 - val_loss: 0.5741 - val_accuracy: 0.7581\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 5s 15ms/step - loss: 0.5892 - accuracy: 0.7342 - val_loss: 0.5580 - val_accuracy: 0.7531\n",
      "Epoch 3/10\n",
      "319/319 [==============================] - 5s 15ms/step - loss: 0.5795 - accuracy: 0.7328 - val_loss: 0.5530 - val_accuracy: 0.7519\n",
      "Epoch 4/10\n",
      "319/319 [==============================] - 5s 15ms/step - loss: 0.5759 - accuracy: 0.7376 - val_loss: 0.5520 - val_accuracy: 0.7536\n",
      "Epoch 5/10\n",
      "319/319 [==============================] - 5s 15ms/step - loss: 0.5751 - accuracy: 0.7391 - val_loss: 0.5552 - val_accuracy: 0.7481\n",
      "Epoch 6/10\n",
      "319/319 [==============================] - 5s 16ms/step - loss: 0.5727 - accuracy: 0.7431 - val_loss: 0.5524 - val_accuracy: 0.7514\n",
      "Epoch 7/10\n",
      "316/319 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7430Restoring model weights from the end of the best epoch: 4.\n",
      "319/319 [==============================] - 5s 15ms/step - loss: 0.5712 - accuracy: 0.7436 - val_loss: 0.5543 - val_accuracy: 0.7503\n",
      "Epoch 7: early stopping\n",
      "(16121, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "429/429 [==============================] - 9s 16ms/step - loss: 0.6638 - accuracy: 0.5951 - val_loss: 0.6315 - val_accuracy: 0.6900\n",
      "Epoch 2/10\n",
      "429/429 [==============================] - 7s 15ms/step - loss: 0.6342 - accuracy: 0.6612 - val_loss: 0.6218 - val_accuracy: 0.6928\n",
      "Epoch 3/10\n",
      "429/429 [==============================] - 6s 15ms/step - loss: 0.6315 - accuracy: 0.6676 - val_loss: 0.6208 - val_accuracy: 0.6900\n",
      "Epoch 4/10\n",
      "429/429 [==============================] - 7s 15ms/step - loss: 0.6277 - accuracy: 0.6759 - val_loss: 0.6425 - val_accuracy: 0.6751\n",
      "Epoch 5/10\n",
      "429/429 [==============================] - 6s 15ms/step - loss: 0.6266 - accuracy: 0.6737 - val_loss: 0.6323 - val_accuracy: 0.6829\n",
      "Epoch 6/10\n",
      "428/429 [============================>.] - ETA: 0s - loss: 0.6241 - accuracy: 0.6792Restoring model weights from the end of the best epoch: 3.\n",
      "429/429 [==============================] - 7s 15ms/step - loss: 0.6240 - accuracy: 0.6792 - val_loss: 0.6281 - val_accuracy: 0.6800\n",
      "Epoch 6: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Train metrics ---------\n",
      "375/375 [==============================] - 2s 4ms/step\n",
      "504/504 [==============================] - 3s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "98/98 [==============================] - 1s 4ms/step\n",
      "123/123 [==============================] - 1s 4ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 1. 0. ... 0. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 71.7895\n",
      "Accuracy test: 73.1103\n",
      "AUC-ROC:  0.7510376296694833\n",
      "AUC-PR:  0.6555467656100392\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(10438, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "278/278 [==============================] - 7s 17ms/step - loss: 0.6268 - accuracy: 0.7072 - val_loss: 0.5683 - val_accuracy: 0.7625\n",
      "Epoch 2/10\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5855 - accuracy: 0.7393 - val_loss: 0.5553 - val_accuracy: 0.7561\n",
      "Epoch 3/10\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5782 - accuracy: 0.7400 - val_loss: 0.5489 - val_accuracy: 0.7573\n",
      "Epoch 4/10\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5709 - accuracy: 0.7491 - val_loss: 0.5594 - val_accuracy: 0.7573\n",
      "Epoch 5/10\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5719 - accuracy: 0.7466 - val_loss: 0.5395 - val_accuracy: 0.7573\n",
      "Epoch 6/10\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5699 - accuracy: 0.7481 - val_loss: 0.5443 - val_accuracy: 0.7580\n",
      "Epoch 7/10\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5698 - accuracy: 0.7475 - val_loss: 0.5400 - val_accuracy: 0.7548\n",
      "Epoch 8/10\n",
      "275/278 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7489Restoring model weights from the end of the best epoch: 5.\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 0.5689 - accuracy: 0.7494 - val_loss: 0.5459 - val_accuracy: 0.7561\n",
      "Epoch 8: early stopping\n",
      "(3637, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 20ms/step - loss: 0.6182 - accuracy: 0.6859 - val_loss: 0.5875 - val_accuracy: 0.7234\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.6040 - accuracy: 0.6865 - val_loss: 0.5819 - val_accuracy: 0.7234\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5932 - accuracy: 0.6875 - val_loss: 0.5793 - val_accuracy: 0.7234\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5870 - accuracy: 0.6881 - val_loss: 0.5715 - val_accuracy: 0.7234\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 0.5818 - accuracy: 0.6907 - val_loss: 0.5710 - val_accuracy: 0.7253\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5765 - accuracy: 0.6946 - val_loss: 0.5701 - val_accuracy: 0.7234\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5736 - accuracy: 0.6972 - val_loss: 0.5643 - val_accuracy: 0.7234\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5687 - accuracy: 0.7130 - val_loss: 0.5660 - val_accuracy: 0.7289\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5680 - accuracy: 0.7130 - val_loss: 0.5651 - val_accuracy: 0.7381\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.5657 - accuracy: 0.7117 - val_loss: 0.5560 - val_accuracy: 0.7363\n",
      "(14028, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "373/373 [==============================] - 9s 16ms/step - loss: 0.6625 - accuracy: 0.6439 - val_loss: 0.6465 - val_accuracy: 0.6746\n",
      "Epoch 2/10\n",
      "373/373 [==============================] - 6s 15ms/step - loss: 0.6360 - accuracy: 0.6671 - val_loss: 0.6495 - val_accuracy: 0.6651\n",
      "Epoch 3/10\n",
      "373/373 [==============================] - 6s 15ms/step - loss: 0.6323 - accuracy: 0.6729 - val_loss: 0.6463 - val_accuracy: 0.6684\n",
      "Epoch 4/10\n",
      "373/373 [==============================] - 6s 15ms/step - loss: 0.6298 - accuracy: 0.6747 - val_loss: 0.6423 - val_accuracy: 0.6736\n",
      "Epoch 5/10\n",
      "373/373 [==============================] - 6s 15ms/step - loss: 0.6292 - accuracy: 0.6806 - val_loss: 0.6488 - val_accuracy: 0.6618\n",
      "Epoch 6/10\n",
      "373/373 [==============================] - 6s 15ms/step - loss: 0.6273 - accuracy: 0.6801 - val_loss: 0.6548 - val_accuracy: 0.6527\n",
      "Epoch 7/10\n",
      "371/373 [============================>.] - ETA: 0s - loss: 0.6268 - accuracy: 0.6821Restoring model weights from the end of the best epoch: 4.\n",
      "373/373 [==============================] - 6s 15ms/step - loss: 0.6266 - accuracy: 0.6822 - val_loss: 0.6429 - val_accuracy: 0.6689\n",
      "Epoch 7: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "327/327 [==============================] - 2s 4ms/step\n",
      "114/114 [==============================] - 1s 5ms/step\n",
      "439/439 [==============================] - 2s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "79/79 [==============================] - 1s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "108/108 [==============================] - 1s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 71.9034\n",
      "Accuracy test: 71.5302\n",
      "AUC-ROC:  0.7305359618978717\n",
      "AUC-PR:  0.631819612345009\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM_w\n",
    "aucprs_w, aucrocs_w, accuracies_w = kfoldclustered(\"LSTM\", X, y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aucpr scores: [0.6476107357022802, 0.6515220395344019, 0.6433301815159652, 0.6555467656100392, 0.631819612345009]\n",
      "0.6460 mean aucpr with a standard deviation of 0.0082\n",
      "aucroc scores: [0.738998318143643, 0.7397990248010862, 0.7343867905322874, 0.7510376296694833, 0.7305359618978717]\n",
      "0.7390 mean aucroc with a standard deviation of 0.0069\n",
      "accuracy scores: [71.8901, 71.5912, 71.1785, 73.1103, 71.5302]\n",
      "71.8601 mean accuracy with a standard deviation of 0.6648\n"
     ]
    }
   ],
   "source": [
    "print ('aucpr scores:', aucprs_w)\n",
    "print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs_w), np.std(aucprs_w)))\n",
    "\n",
    "print ('aucroc scores:', aucrocs_w)\n",
    "print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs_w), np.std(aucrocs_w)))\n",
    "\n",
    "print ('accuracy scores:', accuracies_w)\n",
    "print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies_w), np.std(accuracies_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def kfoldclusteredkmeans(classifier, X, y, weighted=False):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f = 0;\n",
    "    aucprs = []\n",
    "    aucrocs = []\n",
    "    accuracies = []\n",
    "    for train, test in cv.split(X, y): #train and test are indexes\n",
    "        f += 1\n",
    "        print('KFold ',f,' ---')\n",
    "        train_input = X[train]\n",
    "        test_input = X[test]\n",
    "        y_true = y[train]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        print(\"---- Clustering Train ---- \")\n",
    "        # Apply KMeans\n",
    "        n_clusters = 3\n",
    "        kmeans = KMeans(\n",
    "            init=\"random\",\n",
    "            n_clusters=n_clusters,\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=42\n",
    "        )\n",
    "        # Fit to the training data\n",
    "        kmeans.fit(train_input.reshape(y_true.shape[0], 24*10))\n",
    "         # Generate out clusters\n",
    "        train_cluster = kmeans.predict(train_input.reshape(y_true.shape[0], 24*10))\n",
    "        # Add predicted cluster and y regression label to our training DataFrame\n",
    "        train_df = list(zip(train_cluster, y_true, train_input))\n",
    "        ls = sorted(train_df, key=lambda t: t[0])\n",
    "        # Unzip sorted data\n",
    "        cluster, y_, data = zip(*ls)\n",
    "        data = np.array(data)\n",
    "        y_ = np.array(y_)\n",
    "        # Getting indexes of clusters division\n",
    "        c = 0\n",
    "        ind = []\n",
    "        for i in range(len(ls)):\n",
    "            if ls[i][0] > c:\n",
    "                c = ls[i][0]\n",
    "                ind.append(i)\n",
    "        # Removing clusters with less than 10 samples\n",
    "        d1 = ind[0]\n",
    "        d2 = ind[1] - ind[0]\n",
    "        d3 = y_true.shape[0] - ind[1]\n",
    "        cluster_centers_ = kmeans.cluster_centers_\n",
    "        if d1 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 1\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[:ind[0]] = c\n",
    "            ind = ind[1:]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 0, 0)\n",
    "        elif d2 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[ind[0]:ind[1]] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 1, 0)\n",
    "        elif d3 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 1\n",
    "            train_cluster[ind[1]:] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 2, 0)\n",
    "            \n",
    "        print(\"-------- LSTM fitting for each cluster --------\")\n",
    "        i=0\n",
    "        if weighted:\n",
    "            classWeight = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                classes=np.unique(np.ravel(y_true)),\n",
    "                                                y=np.ravel(y_true))\n",
    "            classWeight = {i : classWeight[i] for i in range(2)}\n",
    "        for index in range(len(ind)+1):\n",
    "            if index == 0:\n",
    "                cluster_X = data[:ind[index],:,:]\n",
    "                cluster_Y = y_[:ind[index]]\n",
    "            elif index == len(ind):\n",
    "                cluster_X = data[ind[index-1]:,:,:]\n",
    "                cluster_Y = y_[ind[index-1]:]\n",
    "            else:\n",
    "                cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "                cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "            print(cluster_X.shape)\n",
    "            \n",
    "            # LSTM\n",
    "            model = Sequential()\n",
    "            model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "            \n",
    "            # LSTM\n",
    "            if weighted == False:\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_kmeans{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "            # LSTM_W\n",
    "            else:\n",
    "                print(\"PASSOU\")\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_kmeans{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, class_weight=classWeight, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "                \n",
    "        testkfoldkmeans(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted)\n",
    "    return aucprs, aucrocs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def testkfoldkmeans(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted):\n",
    "    # Create initial test data to store assigned clusters\n",
    "    test_df = list(zip(train_cluster, y_test, test_input))\n",
    "    # Test sample es asignado al cluster correspondiente mediante Distancia euclidiana y se aplica el modelo correspondiente\n",
    "    print(\"Assigning each test sample to the closest cluster centroid...\")\n",
    "    new_cluster = [0 for i in range(y_test.shape[0])]\n",
    "    for row in range(len(test_df)):\n",
    "        min_distance = float('inf')\n",
    "        closest_cluster = None\n",
    "        for k in range(cluster_centers_.shape[0]):\n",
    "            # Check if the assigned cluster has more than 100 samples\n",
    "            # if train_clusters_df[k].shape[0] > 100: # Probar sin limite\n",
    "            distance = np.linalg.norm(cluster_centers_[k]-test_df[row][2].reshape(240))\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_cluster = k\n",
    "        # Assign cluster to test sample\n",
    "        new_cluster[row] = closest_cluster\n",
    "    # Sort test data\n",
    "    test_df = list(zip(new_cluster, y_test, test_input))\n",
    "    ls_test = sorted(test_df, key=lambda t: t[0])\n",
    "    # Unzip sorted data\n",
    "    cluster_t, y_t, data_t = zip(*ls_test)\n",
    "    data_t = np.array(data_t)\n",
    "    y_t = np.array(y_t)\n",
    "    # Getting indexes\n",
    "    c = 0\n",
    "    ind_t = []\n",
    "    for i in range(len(ls_test)):\n",
    "        if ls_test[i][0] > c:\n",
    "            c = ls_test[i][0]\n",
    "            ind_t.append(i)\n",
    "    print(\"-------- Train metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data[:ind[index],:,:]\n",
    "            cluster_Y = y_[:ind[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data[ind[index-1]:,:,:]\n",
    "            cluster_Y = y_[ind[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "            cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_kmeans{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_kmeans{}.h5'.format(f,i))\n",
    "        \n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            train_X_probs = np.array(classes_prob)\n",
    "            y_train = np.array(cluster_Y)\n",
    "        else: \n",
    "            train_X_probs = np.concatenate((train_X_probs, classes_prob))\n",
    "            y_train = np.concatenate((y_train, cluster_Y))\n",
    "        i += 1\n",
    "\n",
    "    # Test metrics\n",
    "    print(\"-------- Test metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind_t)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data_t[:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[:ind_t[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data_t[ind_t[index-1]:,:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data_t[ind_t[index-1]:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:ind_t[index]]\n",
    "            \n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_kmeans{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_kmeans{}.h5'.format(f,i))\n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            test_X_probs = np.array(classes_prob)\n",
    "            test_y = np.array(cluster_Y)\n",
    "        else: \n",
    "            test_X_probs = np.concatenate((test_X_probs, classes_prob))\n",
    "            test_y = np.concatenate((test_y, cluster_Y))\n",
    "        print('----- cluster ',i,' -----')\n",
    "        print(cluster_X.shape)\n",
    "        # Test metrics\n",
    "        y_pred1 = (classes_prob.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "        test_ac1=np.round(metrics.accuracy_score(cluster_Y, y_pred1)*100,4)\n",
    "        print(\"Accuracy test cluster \",i,\":\", test_ac1)\n",
    "        \n",
    "        i += 1\n",
    "    print(\"------ Metrics ------\")\n",
    "    # Train metrics\n",
    "    y_pred_train = (train_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred_train)\n",
    "    print('true labels: ', y_train)\n",
    "    # Test metrics\n",
    "    y_pred = (test_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred)\n",
    "    print('true labels: ', test_y)\n",
    "    #\n",
    "    train_ac=np.round(metrics.accuracy_score(y_train, y_pred_train)*100,4)\n",
    "    print(\"Accuracy train:\", train_ac)\n",
    "    test_ac=np.round(metrics.accuracy_score(test_y, y_pred)*100,4)\n",
    "    print(\"Accuracy test:\", test_ac)\n",
    "    auroc = metrics.roc_auc_score(test_y, test_X_probs)\n",
    "    print(\"AUC-ROC: \", auroc)\n",
    "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(test_y, test_X_probs)\n",
    "    auprc = metrics.auc(recalls, precisions)\n",
    "    print(\"AUC-PR: \", auprc)\n",
    "    aucprs.append(auprc)\n",
    "    aucrocs.append(auroc)\n",
    "    accuracies.append(test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(6779, 24, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:29:40.467440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-02 15:29:40.468202: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-02 15:29:40.470156: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sony-vaio): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "181/181 [==============================] - 9s 34ms/step - loss: 0.6775 - accuracy: 0.5781 - val_loss: 0.6239 - val_accuracy: 0.6549\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 5s 28ms/step - loss: 0.6294 - accuracy: 0.6656 - val_loss: 0.6109 - val_accuracy: 0.6971\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 6s 32ms/step - loss: 0.6143 - accuracy: 0.6867 - val_loss: 0.5986 - val_accuracy: 0.7109\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 7s 38ms/step - loss: 0.6091 - accuracy: 0.6987 - val_loss: 0.5937 - val_accuracy: 0.7129\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 4s 24ms/step - loss: 0.6021 - accuracy: 0.7008 - val_loss: 0.5902 - val_accuracy: 0.7168\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 5s 26ms/step - loss: 0.5993 - accuracy: 0.7044 - val_loss: 0.5886 - val_accuracy: 0.7158\n",
      "Epoch 7/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.5955 - accuracy: 0.7088 - val_loss: 0.5877 - val_accuracy: 0.7178\n",
      "Epoch 8/10\n",
      "181/181 [==============================] - 4s 19ms/step - loss: 0.5990 - accuracy: 0.7081 - val_loss: 0.5877 - val_accuracy: 0.7178\n",
      "Epoch 9/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.5952 - accuracy: 0.7058 - val_loss: 0.5861 - val_accuracy: 0.7148\n",
      "Epoch 10/10\n",
      "181/181 [==============================] - 6s 36ms/step - loss: 0.5943 - accuracy: 0.7051 - val_loss: 0.5853 - val_accuracy: 0.7148\n",
      "(20716, 24, 10)\n",
      "Epoch 1/10\n",
      "551/551 [==============================] - 17s 24ms/step - loss: 0.5898 - accuracy: 0.7098 - val_loss: 0.5779 - val_accuracy: 0.7165\n",
      "Epoch 2/10\n",
      "551/551 [==============================] - 11s 19ms/step - loss: 0.5748 - accuracy: 0.7247 - val_loss: 0.5771 - val_accuracy: 0.7259\n",
      "Epoch 3/10\n",
      "551/551 [==============================] - 10s 19ms/step - loss: 0.5719 - accuracy: 0.7273 - val_loss: 0.5738 - val_accuracy: 0.7288\n",
      "Epoch 4/10\n",
      "551/551 [==============================] - 10s 19ms/step - loss: 0.5705 - accuracy: 0.7277 - val_loss: 0.5743 - val_accuracy: 0.7297\n",
      "Epoch 5/10\n",
      "551/551 [==============================] - 11s 19ms/step - loss: 0.5688 - accuracy: 0.7282 - val_loss: 0.5735 - val_accuracy: 0.7297\n",
      "Epoch 6/10\n",
      "551/551 [==============================] - 10s 19ms/step - loss: 0.5688 - accuracy: 0.7294 - val_loss: 0.5738 - val_accuracy: 0.7291\n",
      "Epoch 7/10\n",
      "551/551 [==============================] - 15s 27ms/step - loss: 0.5688 - accuracy: 0.7297 - val_loss: 0.5741 - val_accuracy: 0.7291\n",
      "Epoch 8/10\n",
      "549/551 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7313Restoring model weights from the end of the best epoch: 5.\n",
      "551/551 [==============================] - 10s 19ms/step - loss: 0.5671 - accuracy: 0.7312 - val_loss: 0.5743 - val_accuracy: 0.7294\n",
      "Epoch 8: early stopping\n",
      "(607, 24, 10)\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 4s 52ms/step - loss: 0.6856 - accuracy: 0.5728 - val_loss: 0.6551 - val_accuracy: 0.5978\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6835 - accuracy: 0.5631 - val_loss: 0.6493 - val_accuracy: 0.6413\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.6610 - accuracy: 0.6078 - val_loss: 0.6422 - val_accuracy: 0.6413\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6664 - accuracy: 0.5981 - val_loss: 0.6394 - val_accuracy: 0.7065\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6625 - accuracy: 0.6039 - val_loss: 0.6378 - val_accuracy: 0.7391\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.6750 - accuracy: 0.5592 - val_loss: 0.6301 - val_accuracy: 0.7283\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6550 - accuracy: 0.5961 - val_loss: 0.6253 - val_accuracy: 0.7283\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.6452 - accuracy: 0.6252 - val_loss: 0.6204 - val_accuracy: 0.6957\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6532 - accuracy: 0.6058 - val_loss: 0.6167 - val_accuracy: 0.7391\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6485 - accuracy: 0.6291 - val_loss: 0.6123 - val_accuracy: 0.7065\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "212/212 [==============================] - 2s 5ms/step\n",
      "648/648 [==============================] - 4s 6ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "55/55 [==============================] - 1s 7ms/step\n",
      "----- cluster  0  -----\n",
      "(1751, 24, 10)\n",
      "Accuracy test cluster  0 : 69.7316\n",
      "161/161 [==============================] - 1s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(5121, 24, 10)\n",
      "Accuracy test cluster  1 : 73.1693\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(154, 24, 10)\n",
      "Accuracy test cluster  2 : 67.5325\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 0.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 72.5144\n",
      "Accuracy test: 72.189\n",
      "AUC-ROC:  0.7408432479803762\n",
      "AUC-PR:  0.6565342290034353\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(6813, 24, 10)\n",
      "Epoch 1/10\n",
      "181/181 [==============================] - 7s 22ms/step - loss: 0.6384 - accuracy: 0.6460 - val_loss: 0.6230 - val_accuracy: 0.6341\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 5s 27ms/step - loss: 0.6240 - accuracy: 0.6766 - val_loss: 0.6084 - val_accuracy: 0.6986\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.6131 - accuracy: 0.6942 - val_loss: 0.6009 - val_accuracy: 0.7006\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 0.6035 - accuracy: 0.7006 - val_loss: 0.5974 - val_accuracy: 0.7006\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 4s 24ms/step - loss: 0.5994 - accuracy: 0.7040 - val_loss: 0.5972 - val_accuracy: 0.6977\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 4s 24ms/step - loss: 0.5967 - accuracy: 0.7071 - val_loss: 0.5934 - val_accuracy: 0.7084\n",
      "Epoch 7/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.5953 - accuracy: 0.7045 - val_loss: 0.5931 - val_accuracy: 0.7035\n",
      "Epoch 8/10\n",
      "181/181 [==============================] - 5s 25ms/step - loss: 0.5931 - accuracy: 0.7075 - val_loss: 0.5929 - val_accuracy: 0.7055\n",
      "Epoch 9/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.5937 - accuracy: 0.7056 - val_loss: 0.5928 - val_accuracy: 0.7025\n",
      "Epoch 10/10\n",
      "181/181 [==============================] - 5s 30ms/step - loss: 0.5918 - accuracy: 0.7094 - val_loss: 0.5923 - val_accuracy: 0.7035\n",
      "(20713, 24, 10)\n",
      "Epoch 1/10\n",
      "551/551 [==============================] - 19s 26ms/step - loss: 0.5980 - accuracy: 0.6997 - val_loss: 0.5788 - val_accuracy: 0.7103\n",
      "Epoch 2/10\n",
      "551/551 [==============================] - 14s 25ms/step - loss: 0.5759 - accuracy: 0.7220 - val_loss: 0.5709 - val_accuracy: 0.7309\n",
      "Epoch 3/10\n",
      "551/551 [==============================] - 15s 27ms/step - loss: 0.5719 - accuracy: 0.7282 - val_loss: 0.5701 - val_accuracy: 0.7303\n",
      "Epoch 4/10\n",
      "551/551 [==============================] - 14s 25ms/step - loss: 0.5691 - accuracy: 0.7283 - val_loss: 0.5701 - val_accuracy: 0.7300\n",
      "Epoch 5/10\n",
      "551/551 [==============================] - 11s 20ms/step - loss: 0.5669 - accuracy: 0.7306 - val_loss: 0.5699 - val_accuracy: 0.7293\n",
      "Epoch 6/10\n",
      "551/551 [==============================] - 12s 21ms/step - loss: 0.5661 - accuracy: 0.7307 - val_loss: 0.5698 - val_accuracy: 0.7309\n",
      "Epoch 7/10\n",
      "551/551 [==============================] - 12s 22ms/step - loss: 0.5667 - accuracy: 0.7313 - val_loss: 0.5701 - val_accuracy: 0.7300\n",
      "Epoch 8/10\n",
      "551/551 [==============================] - 12s 21ms/step - loss: 0.5654 - accuracy: 0.7329 - val_loss: 0.5704 - val_accuracy: 0.7303\n",
      "Epoch 9/10\n",
      "549/551 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7313Restoring model weights from the end of the best epoch: 6.\n",
      "551/551 [==============================] - 14s 25ms/step - loss: 0.5651 - accuracy: 0.7311 - val_loss: 0.5710 - val_accuracy: 0.7309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: early stopping\n",
      "(576, 24, 10)\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 4s 51ms/step - loss: 0.7219 - accuracy: 0.4499 - val_loss: 0.7053 - val_accuracy: 0.4828\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6958 - accuracy: 0.5256 - val_loss: 0.6888 - val_accuracy: 0.5057\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6903 - accuracy: 0.5235 - val_loss: 0.6757 - val_accuracy: 0.5287\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6851 - accuracy: 0.5521 - val_loss: 0.6680 - val_accuracy: 0.5862\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.6839 - accuracy: 0.5562 - val_loss: 0.6593 - val_accuracy: 0.5862\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6782 - accuracy: 0.5562 - val_loss: 0.6528 - val_accuracy: 0.5862\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6669 - accuracy: 0.5890 - val_loss: 0.6467 - val_accuracy: 0.6207\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6647 - accuracy: 0.5869 - val_loss: 0.6386 - val_accuracy: 0.6207\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6586 - accuracy: 0.5951 - val_loss: 0.6330 - val_accuracy: 0.6552\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.6537 - accuracy: 0.5869 - val_loss: 0.6276 - val_accuracy: 0.6782\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "213/213 [==============================] - 2s 7ms/step\n",
      "648/648 [==============================] - 4s 6ms/step\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "54/54 [==============================] - 1s 7ms/step\n",
      "----- cluster  0  -----\n",
      "(1724, 24, 10)\n",
      "Accuracy test cluster  0 : 71.4617\n",
      "161/161 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(5146, 24, 10)\n",
      "Accuracy test cluster  1 : 72.9693\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(156, 24, 10)\n",
      "Accuracy test cluster  2 : 64.7436\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 0.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 0. 1. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 72.4397\n",
      "Accuracy test: 72.4167\n",
      "AUC-ROC:  0.7451711551371734\n",
      "AUC-PR:  0.6607602253939872\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(12816, 24, 10)\n",
      "Epoch 1/10\n",
      "341/341 [==============================] - 11s 24ms/step - loss: 0.7017 - accuracy: 0.5730 - val_loss: 0.6267 - val_accuracy: 0.6641\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 9s 27ms/step - loss: 0.6174 - accuracy: 0.6783 - val_loss: 0.6190 - val_accuracy: 0.6698\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 8s 24ms/step - loss: 0.6045 - accuracy: 0.6915 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 10s 31ms/step - loss: 0.6008 - accuracy: 0.6993 - val_loss: 0.6126 - val_accuracy: 0.6864\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 9s 27ms/step - loss: 0.6006 - accuracy: 0.7024 - val_loss: 0.6117 - val_accuracy: 0.6869\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 0.5977 - accuracy: 0.7021 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 10s 29ms/step - loss: 0.5959 - accuracy: 0.7037 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 0.5946 - accuracy: 0.7063 - val_loss: 0.6116 - val_accuracy: 0.6880\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 0.5956 - accuracy: 0.7045 - val_loss: 0.6117 - val_accuracy: 0.6890\n",
      "Epoch 10/10\n",
      "340/341 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.7063Restoring model weights from the end of the best epoch: 7.\n",
      "341/341 [==============================] - 7s 20ms/step - loss: 0.5945 - accuracy: 0.7066 - val_loss: 0.6115 - val_accuracy: 0.6911\n",
      "Epoch 10: early stopping\n",
      "(9331, 24, 10)\n",
      "Epoch 1/10\n",
      "248/248 [==============================] - 9s 23ms/step - loss: 0.6036 - accuracy: 0.6829 - val_loss: 0.5533 - val_accuracy: 0.7329\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.5535 - accuracy: 0.7434 - val_loss: 0.5391 - val_accuracy: 0.7450\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 6s 23ms/step - loss: 0.5451 - accuracy: 0.7520 - val_loss: 0.5312 - val_accuracy: 0.7579\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 6s 23ms/step - loss: 0.5425 - accuracy: 0.7563 - val_loss: 0.5302 - val_accuracy: 0.7614\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.5382 - accuracy: 0.7587 - val_loss: 0.5281 - val_accuracy: 0.7650\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 6s 26ms/step - loss: 0.5349 - accuracy: 0.7616 - val_loss: 0.5255 - val_accuracy: 0.7650\n",
      "Epoch 7/10\n",
      "248/248 [==============================] - 6s 25ms/step - loss: 0.5354 - accuracy: 0.7592 - val_loss: 0.5266 - val_accuracy: 0.7671\n",
      "Epoch 8/10\n",
      "248/248 [==============================] - 6s 24ms/step - loss: 0.5375 - accuracy: 0.7597 - val_loss: 0.5256 - val_accuracy: 0.7679\n",
      "Epoch 9/10\n",
      "247/248 [============================>.] - ETA: 0s - loss: 0.5310 - accuracy: 0.7620Restoring model weights from the end of the best epoch: 6.\n",
      "248/248 [==============================] - 7s 28ms/step - loss: 0.5306 - accuracy: 0.7623 - val_loss: 0.5268 - val_accuracy: 0.7686\n",
      "Epoch 9: early stopping\n",
      "(5955, 24, 10)\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 8s 27ms/step - loss: 0.6414 - accuracy: 0.6497 - val_loss: 0.6146 - val_accuracy: 0.6689\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 4s 23ms/step - loss: 0.6242 - accuracy: 0.6744 - val_loss: 0.6032 - val_accuracy: 0.6834\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 3s 20ms/step - loss: 0.6120 - accuracy: 0.6977 - val_loss: 0.5956 - val_accuracy: 0.7181\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 4s 23ms/step - loss: 0.6027 - accuracy: 0.7024 - val_loss: 0.5902 - val_accuracy: 0.7181\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 4s 23ms/step - loss: 0.6001 - accuracy: 0.7052 - val_loss: 0.5877 - val_accuracy: 0.7204\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 3s 22ms/step - loss: 0.5944 - accuracy: 0.7086 - val_loss: 0.5860 - val_accuracy: 0.7204\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 4s 22ms/step - loss: 0.5945 - accuracy: 0.7082 - val_loss: 0.5857 - val_accuracy: 0.7237\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 3s 20ms/step - loss: 0.5935 - accuracy: 0.7097 - val_loss: 0.5840 - val_accuracy: 0.7204\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 3s 20ms/step - loss: 0.5939 - accuracy: 0.7091 - val_loss: 0.5838 - val_accuracy: 0.7181\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 3s 20ms/step - loss: 0.5886 - accuracy: 0.7123 - val_loss: 0.5830 - val_accuracy: 0.7192\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "401/401 [==============================] - 3s 6ms/step\n",
      "292/292 [==============================] - 2s 6ms/step\n",
      "187/187 [==============================] - 1s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "100/100 [==============================] - 1s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(3193, 24, 10)\n",
      "Accuracy test cluster  0 : 69.809\n",
      "74/74 [==============================] - 1s 7ms/step\n",
      "----- cluster  1  -----\n",
      "(2347, 24, 10)\n",
      "Accuracy test cluster  1 : 74.8189\n",
      "47/47 [==============================] - 1s 7ms/step\n",
      "----- cluster  2  -----\n",
      "(1486, 24, 10)\n",
      "Accuracy test cluster  2 : 72.0727\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 72.5713\n",
      "Accuracy test: 71.9613\n",
      "AUC-ROC:  0.7373378165630684\n",
      "AUC-PR:  0.6550276592490609\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- LSTM fitting for each cluster --------\n",
      "(20667, 24, 10)\n",
      "Epoch 1/10\n",
      "549/549 [==============================] - 19s 28ms/step - loss: 0.6097 - accuracy: 0.6811 - val_loss: 0.5791 - val_accuracy: 0.7130\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 11s 20ms/step - loss: 0.5793 - accuracy: 0.7194 - val_loss: 0.5734 - val_accuracy: 0.7269\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 12s 22ms/step - loss: 0.5763 - accuracy: 0.7237 - val_loss: 0.5728 - val_accuracy: 0.7304\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 13s 24ms/step - loss: 0.5738 - accuracy: 0.7255 - val_loss: 0.5716 - val_accuracy: 0.7320\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 15s 27ms/step - loss: 0.5725 - accuracy: 0.7262 - val_loss: 0.5714 - val_accuracy: 0.7327\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 11s 21ms/step - loss: 0.5705 - accuracy: 0.7266 - val_loss: 0.5712 - val_accuracy: 0.7317\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 12s 21ms/step - loss: 0.5696 - accuracy: 0.7270 - val_loss: 0.5709 - val_accuracy: 0.7333\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 12s 21ms/step - loss: 0.5700 - accuracy: 0.7269 - val_loss: 0.5710 - val_accuracy: 0.7330\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 12s 21ms/step - loss: 0.5682 - accuracy: 0.7262 - val_loss: 0.5709 - val_accuracy: 0.7317\n",
      "Epoch 10/10\n",
      "547/549 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7279Restoring model weights from the end of the best epoch: 7.\n",
      "549/549 [==============================] - 11s 20ms/step - loss: 0.5671 - accuracy: 0.7279 - val_loss: 0.5712 - val_accuracy: 0.7320\n",
      "Epoch 10: early stopping\n",
      "(6805, 24, 10)\n",
      "Epoch 1/10\n",
      "181/181 [==============================] - 9s 28ms/step - loss: 0.6517 - accuracy: 0.6425 - val_loss: 0.6408 - val_accuracy: 0.6268\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.6285 - accuracy: 0.6653 - val_loss: 0.6261 - val_accuracy: 0.6631\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 5s 28ms/step - loss: 0.6170 - accuracy: 0.6826 - val_loss: 0.6168 - val_accuracy: 0.6885\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 5s 27ms/step - loss: 0.6139 - accuracy: 0.6959 - val_loss: 0.6118 - val_accuracy: 0.6885\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 5s 26ms/step - loss: 0.6030 - accuracy: 0.6997 - val_loss: 0.6082 - val_accuracy: 0.6915\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.6009 - accuracy: 0.6995 - val_loss: 0.6059 - val_accuracy: 0.6925\n",
      "Epoch 7/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.5992 - accuracy: 0.7035 - val_loss: 0.6046 - val_accuracy: 0.6944\n",
      "Epoch 8/10\n",
      "181/181 [==============================] - 5s 26ms/step - loss: 0.5983 - accuracy: 0.7031 - val_loss: 0.6055 - val_accuracy: 0.6885\n",
      "Epoch 9/10\n",
      "181/181 [==============================] - 5s 29ms/step - loss: 0.5986 - accuracy: 0.7044 - val_loss: 0.6028 - val_accuracy: 0.6974\n",
      "Epoch 10/10\n",
      "181/181 [==============================] - 5s 26ms/step - loss: 0.5969 - accuracy: 0.7050 - val_loss: 0.6013 - val_accuracy: 0.6983\n",
      "(631, 24, 10)\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 4s 53ms/step - loss: 0.7606 - accuracy: 0.4590 - val_loss: 0.7438 - val_accuracy: 0.4105\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7099 - accuracy: 0.4944 - val_loss: 0.6987 - val_accuracy: 0.5474\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6952 - accuracy: 0.5019 - val_loss: 0.6740 - val_accuracy: 0.5684\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6758 - accuracy: 0.5616 - val_loss: 0.6606 - val_accuracy: 0.5579\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.6712 - accuracy: 0.5914 - val_loss: 0.6524 - val_accuracy: 0.5579\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6676 - accuracy: 0.5616 - val_loss: 0.6454 - val_accuracy: 0.5579\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6729 - accuracy: 0.5560 - val_loss: 0.6413 - val_accuracy: 0.5684\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6523 - accuracy: 0.6045 - val_loss: 0.6367 - val_accuracy: 0.5789\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6511 - accuracy: 0.5877 - val_loss: 0.6322 - val_accuracy: 0.5684\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.6502 - accuracy: 0.5765 - val_loss: 0.6274 - val_accuracy: 0.5895\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "646/646 [==============================] - 6s 7ms/step\n",
      "213/213 [==============================] - 1s 5ms/step\n",
      "20/20 [==============================] - 1s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "162/162 [==============================] - 2s 9ms/step\n",
      "----- cluster  0  -----\n",
      "(5176, 24, 10)\n",
      "Accuracy test cluster  0 : 73.9567\n",
      "53/53 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(1689, 24, 10)\n",
      "Accuracy test cluster  1 : 72.2321\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      "----- cluster  2  -----\n",
      "(160, 24, 10)\n",
      "Accuracy test cluster  2 : 52.5\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 72.1453\n",
      "Accuracy test: 73.0534\n",
      "AUC-ROC:  0.7511341881820595\n",
      "AUC-PR:  0.6685799085433248\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(6859, 24, 10)\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 9s 26ms/step - loss: 0.6681 - accuracy: 0.5889 - val_loss: 0.6339 - val_accuracy: 0.6433\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.6327 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6463\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.6219 - accuracy: 0.6727 - val_loss: 0.6107 - val_accuracy: 0.6803\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.6116 - accuracy: 0.6959 - val_loss: 0.6035 - val_accuracy: 0.6929\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.6044 - accuracy: 0.7041 - val_loss: 0.5996 - val_accuracy: 0.6997\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.6013 - accuracy: 0.7051 - val_loss: 0.5970 - val_accuracy: 0.6987\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.5985 - accuracy: 0.7075 - val_loss: 0.5958 - val_accuracy: 0.6978\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.5968 - accuracy: 0.7098 - val_loss: 0.5936 - val_accuracy: 0.7046\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.5931 - accuracy: 0.7117 - val_loss: 0.5927 - val_accuracy: 0.7017\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.5929 - accuracy: 0.7110 - val_loss: 0.5911 - val_accuracy: 0.7017\n",
      "(656, 24, 10)\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 4s 51ms/step - loss: 0.6938 - accuracy: 0.5224 - val_loss: 0.6762 - val_accuracy: 0.5253\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.6835 - accuracy: 0.5601 - val_loss: 0.6681 - val_accuracy: 0.5657\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6921 - accuracy: 0.5332 - val_loss: 0.6606 - val_accuracy: 0.5960\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.6741 - accuracy: 0.5817 - val_loss: 0.6542 - val_accuracy: 0.6869\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6786 - accuracy: 0.5601 - val_loss: 0.6478 - val_accuracy: 0.7273\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.6617 - accuracy: 0.6086 - val_loss: 0.6429 - val_accuracy: 0.7071\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.6560 - accuracy: 0.5996 - val_loss: 0.6364 - val_accuracy: 0.7172\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 33ms/step - loss: 0.6641 - accuracy: 0.5943 - val_loss: 0.6289 - val_accuracy: 0.6970\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.6513 - accuracy: 0.6320 - val_loss: 0.6231 - val_accuracy: 0.6869\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6490 - accuracy: 0.6158 - val_loss: 0.6186 - val_accuracy: 0.6970\n",
      "(20588, 24, 10)\n",
      "Epoch 1/10\n",
      "547/547 [==============================] - 19s 27ms/step - loss: 0.5963 - accuracy: 0.7037 - val_loss: 0.5780 - val_accuracy: 0.7148\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.5744 - accuracy: 0.7240 - val_loss: 0.5736 - val_accuracy: 0.7271\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 0.5711 - accuracy: 0.7259 - val_loss: 0.5725 - val_accuracy: 0.7277\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 0.5701 - accuracy: 0.7282 - val_loss: 0.5723 - val_accuracy: 0.7284\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 0.5689 - accuracy: 0.7284 - val_loss: 0.5723 - val_accuracy: 0.7281\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 0.5658 - accuracy: 0.7283 - val_loss: 0.5723 - val_accuracy: 0.7294\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 12s 21ms/step - loss: 0.5677 - accuracy: 0.7296 - val_loss: 0.5728 - val_accuracy: 0.7297\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 11s 20ms/step - loss: 0.5649 - accuracy: 0.7298 - val_loss: 0.5727 - val_accuracy: 0.7284\n",
      "Epoch 9/10\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.7312Restoring model weights from the end of the best epoch: 6.\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 0.5667 - accuracy: 0.7311 - val_loss: 0.5728 - val_accuracy: 0.7281\n",
      "Epoch 9: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "215/215 [==============================] - 1s 5ms/step\n",
      "21/21 [==============================] - 0s 5ms/step\n",
      "644/644 [==============================] - 4s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "52/52 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(1637, 24, 10)\n",
      "Accuracy test cluster  0 : 69.5174\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(143, 24, 10)\n",
      "Accuracy test cluster  1 : 60.8392\n",
      "164/164 [==============================] - 1s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(5245, 24, 10)\n",
      "Accuracy test cluster  2 : 73.346\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 0. ... 1. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 72.5118\n",
      "Accuracy test: 72.1993\n",
      "AUC-ROC:  0.7340974429200149\n",
      "AUC-PR:  0.633228760856434\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM\n",
    "aucprs, aucrocs, accuracies = kfoldclusteredkmeans(\"LSTM\", X, y, weighted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aucpr scores: [0.6565342290034353, 0.6607602253939872, 0.6550276592490609, 0.6685799085433248, 0.633228760856434]\n",
      "0.6548 mean aucpr with a standard deviation of 0.0118\n",
      "aucroc scores: [0.7408432479803762, 0.7451711551371734, 0.7373378165630684, 0.7511341881820595, 0.7340974429200149]\n",
      "0.7417 mean aucroc with a standard deviation of 0.0060\n",
      "accuracy scores: [72.189, 72.4167, 71.9613, 73.0534, 72.1993]\n",
      "72.3639 mean accuracy with a standard deviation of 0.3736\n"
     ]
    }
   ],
   "source": [
    "print ('aucpr scores:', aucprs)\n",
    "print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs), np.std(aucprs)))\n",
    "\n",
    "print ('aucroc scores:', aucrocs)\n",
    "print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs), np.std(aucrocs)))\n",
    "\n",
    "print ('accuracy scores:', accuracies)\n",
    "print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manera 2\n",
    "#https://amirhessam88.github.io/roc-vs-pr/\n",
    "\n",
    "def customCrossValidationMetrics(classifier, X, y, weighted = False  ):\n",
    "    scaler = StandardScaler()\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #ejemplo classifier=RandomForestClassifier(n_estimators=10, random_state = 42)\n",
    "    aucprs = []\n",
    "    aucrocs = []\n",
    "    accuracies = []\n",
    "    \n",
    "    i=1\n",
    "    for train, test in cv.split(X, y): #train and test are indexes\n",
    "        print('KFold ',i)\n",
    "        #reshape because scaler need <= 2d\n",
    "        X_train_tranformed = scaler.fit_transform(X[train].reshape(train.shape[0]*24, 10))\n",
    "        X_test_tranformed = scaler.transform(X[test].reshape(test.shape[0]*24, 10))\n",
    "        \n",
    "        #re-reshape\n",
    "        \n",
    "        X_train_tranformed = X_train_tranformed.reshape(train.shape[0], 24, 10)\n",
    "        X_test_tranformed = X_test_tranformed.reshape(test.shape[0], 24, 10)\n",
    "        \n",
    "        if(weighted):\n",
    "            classWeight = class_weight.compute_class_weight('balanced',np.unique(np.ravel(y[train])),np.ravel(y[train]))\n",
    "            classWeight = {i : classWeight[i] for i in range(2)}  #convert to dictionary in order to fit to keras model\n",
    "            print(\"training: \")\n",
    "            history = classifier.fit(X_train_tranformed, np.ravel(y[train]), epochs=8, verbose=1,class_weight=classWeight)\n",
    "\n",
    "        else:\n",
    "            print(\"training: \")\n",
    "            history = classifier.fit(X_train_tranformed, np.ravel(y[train]),epochs=8, verbose=1)\n",
    "\n",
    "        print(\"testing: \")\n",
    "        probas_ = np.ravel(classifier.predict(X_test_tranformed, verbose=1))\n",
    "        # Compute PR curve and area the curve\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y[test], probas_)\n",
    "        pr_auc =  np.round(metrics.auc(recall, precision), 6)\n",
    "        aucprs.append(pr_auc)\n",
    "        auroc =  np.round(metrics.roc_auc_score(y[test],probas_),6)\n",
    "        aucrocs.append(auroc)\n",
    "\n",
    "        y_pred_binary = classifier.predict(X_test_tranformed)\n",
    "        y_pred_binary = (y_pred_binary.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "        acc = np.round(metrics.accuracy_score(y[test],y_pred_binary)*100,4)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        i = i+1\n",
    "\n",
    "    print ('aucpr scores:', aucprs)\n",
    "    print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs), np.std(aucprs)))\n",
    "\n",
    "    print ('aucroc scores:', aucrocs)\n",
    "    print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs), np.std(aucrocs)))\n",
    "\n",
    "    print ('accuracy scores:', accuracies)\n",
    "    print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies), np.std(accuracies)))\n",
    "    \n",
    "    return aucprs, aucrocs, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1\n",
      "training: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:52:00.810529: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 26977920 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "879/879 [==============================] - 18s 21ms/step - loss: 0.5759 - accuracy: 0.7243\n",
      "Epoch 2/8\n",
      "879/879 [==============================] - 19s 21ms/step - loss: 0.5751 - accuracy: 0.7238\n",
      "Epoch 3/8\n",
      "879/879 [==============================] - 16s 19ms/step - loss: 0.5747 - accuracy: 0.7250\n",
      "Epoch 4/8\n",
      "879/879 [==============================] - 21s 24ms/step - loss: 0.5731 - accuracy: 0.7241\n",
      "Epoch 5/8\n",
      "879/879 [==============================] - 15s 17ms/step - loss: 0.5739 - accuracy: 0.7241\n",
      "Epoch 6/8\n",
      "879/879 [==============================] - 19s 21ms/step - loss: 0.5729 - accuracy: 0.7246\n",
      "Epoch 7/8\n",
      "879/879 [==============================] - 22s 25ms/step - loss: 0.5728 - accuracy: 0.7254\n",
      "Epoch 8/8\n",
      "879/879 [==============================] - 19s 22ms/step - loss: 0.5733 - accuracy: 0.7246\n",
      "testing: \n",
      "220/220 [==============================] - 1s 5ms/step\n",
      "KFold  2\n",
      "training: \n",
      "Epoch 1/8\n",
      "879/879 [==============================] - 15s 17ms/step - loss: 0.5727 - accuracy: 0.7255\n",
      "Epoch 2/8\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.5723 - accuracy: 0.7254\n",
      "Epoch 3/8\n",
      "879/879 [==============================] - 20s 22ms/step - loss: 0.5714 - accuracy: 0.7246\n",
      "Epoch 4/8\n",
      "879/879 [==============================] - 19s 22ms/step - loss: 0.5716 - accuracy: 0.7252\n",
      "Epoch 5/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5705 - accuracy: 0.7272\n",
      "Epoch 6/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5705 - accuracy: 0.7259\n",
      "Epoch 7/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5691 - accuracy: 0.7266\n",
      "Epoch 8/8\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.5705 - accuracy: 0.7270\n",
      "testing: \n",
      "220/220 [==============================] - 1s 6ms/step\n",
      "KFold  3\n",
      "training: \n",
      "Epoch 1/8\n",
      "  4/879 [..............................] - ETA: 17s - loss: 0.5913 - accuracy: 0.7344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:56:58.271995: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 26977920 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5683 - accuracy: 0.7262\n",
      "Epoch 2/8\n",
      "879/879 [==============================] - 17s 20ms/step - loss: 0.5679 - accuracy: 0.7270\n",
      "Epoch 3/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5678 - accuracy: 0.7257\n",
      "Epoch 4/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5679 - accuracy: 0.7249\n",
      "Epoch 5/8\n",
      "879/879 [==============================] - 17s 20ms/step - loss: 0.5677 - accuracy: 0.7261\n",
      "Epoch 6/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5673 - accuracy: 0.7257\n",
      "Epoch 7/8\n",
      "879/879 [==============================] - 17s 20ms/step - loss: 0.5666 - accuracy: 0.7272\n",
      "Epoch 8/8\n",
      "879/879 [==============================] - 19s 22ms/step - loss: 0.5668 - accuracy: 0.7264\n",
      "testing: \n",
      "220/220 [==============================] - 2s 10ms/step\n",
      "KFold  4\n",
      "training: \n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:59:25.278477: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 26978880 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - 20s 23ms/step - loss: 0.5711 - accuracy: 0.7243\n",
      "Epoch 2/8\n",
      "879/879 [==============================] - 18s 21ms/step - loss: 0.5695 - accuracy: 0.7247\n",
      "Epoch 3/8\n",
      "879/879 [==============================] - 16s 18ms/step - loss: 0.5702 - accuracy: 0.7239\n",
      "Epoch 4/8\n",
      "879/879 [==============================] - 16s 18ms/step - loss: 0.5708 - accuracy: 0.7237\n",
      "Epoch 5/8\n",
      "879/879 [==============================] - 17s 20ms/step - loss: 0.5699 - accuracy: 0.7247\n",
      "Epoch 6/8\n",
      "879/879 [==============================] - 17s 19ms/step - loss: 0.5691 - accuracy: 0.7250\n",
      "Epoch 7/8\n",
      "879/879 [==============================] - 16s 19ms/step - loss: 0.5685 - accuracy: 0.7244\n",
      "Epoch 8/8\n",
      "879/879 [==============================] - 17s 20ms/step - loss: 0.5699 - accuracy: 0.7244\n",
      "testing: \n",
      "220/220 [==============================] - 1s 6ms/step\n",
      "KFold  5\n",
      "training: \n",
      "Epoch 1/8\n",
      "879/879 [==============================] - 17s 20ms/step - loss: 0.5662 - accuracy: 0.7262\n",
      "Epoch 2/8\n",
      "879/879 [==============================] - 18s 20ms/step - loss: 0.5660 - accuracy: 0.7258\n",
      "Epoch 3/8\n",
      "879/879 [==============================] - 16s 18ms/step - loss: 0.5665 - accuracy: 0.7263\n",
      "Epoch 4/8\n",
      "879/879 [==============================] - 16s 18ms/step - loss: 0.5653 - accuracy: 0.7257\n",
      "Epoch 5/8\n",
      "879/879 [==============================] - 14s 16ms/step - loss: 0.5665 - accuracy: 0.7245\n",
      "Epoch 6/8\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 0.5657 - accuracy: 0.7265\n",
      "Epoch 7/8\n",
      "879/879 [==============================] - 14s 15ms/step - loss: 0.5658 - accuracy: 0.7264\n",
      "Epoch 8/8\n",
      "879/879 [==============================] - 15s 17ms/step - loss: 0.5653 - accuracy: 0.7275\n",
      "testing: \n",
      "220/220 [==============================] - 1s 6ms/step\n",
      "aucpr scores: [0.664502, 0.673576, 0.672413, 0.686458, 0.661199]\n",
      "0.6716 mean aucpr with a standard deviation of 0.0088\n",
      "aucroc scores: [0.746278, 0.751817, 0.743593, 0.764057, 0.747589]\n",
      "0.7507 mean aucroc with a standard deviation of 0.0072\n",
      "accuracy scores: [72.431, 72.189, 72.2032, 73.6655, 72.6406]\n",
      "72.6259 mean accuracy with a standard deviation of 0.5456\n"
     ]
    }
   ],
   "source": [
    "aucprs_LSTM, aucrocs_LSTM, accuracies_LSTM = customCrossValidationMetrics(classifier, X, y, weighted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "classifier_weighted = Sequential()\n",
    "classifier_weighted.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "classifier_weighted.add(Dropout(0.2))\n",
    "classifier_weighted.add(Dense(1, activation='sigmoid'))\n",
    "classifier_weighted.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucprs_LSTM_weighted, aucrocs_LSTM_weighted, accuracies_LSTM_weighted = customCrossValidationMetrics(classifier_weighted, X, y, weighted = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Statistical Significance Tests for Comparing Machine Learning Algorithms\n",
    "\n",
    "https://stackoverflow.com/questions/54498235/compare-whether-the-difference-between-performance-accuracy-of-2-ml-models-is-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM vs LSTM_weighted: ', stats.ttest_rel(aucrocs_LSTM,aucrocs_LSTM_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM vs LSTM_weighted: ', stats.ttest_rel(aucprs_LSTM,aucprs_LSTM_weighted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "las diferencias de aucroc y aucpr entre LSTM y LSTM_weighted no son estadísticamente signifiativas. (p_value > 0.05, no podemos rechazar la hipotesi nula de que los modelos tienen comportamiento similar)  \n",
    "Obs. para LOS>3 días, el cohorte de benchmark está más balanceado. (38% positivo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
