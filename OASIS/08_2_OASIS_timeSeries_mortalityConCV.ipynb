{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA (APÉNDICE)\n",
    "\n",
    "En este notebook, se entrenan modelos de deep learning (LSTM) utilizando series temporales de las variables que componen OASIS, para predecir la mortalidad hospitalaria. Luego se evalua el rendimiento de los modelos con ACC, AUC-ROC, AUC-PR, confusion matrix...También se realiza Cross Validation y student-t-test para determinar si las diferencias entre los resultados son estadísticamente significativas.\n",
    "\n",
    "Obs. Ejecute primero '06CreateTimeSeries.ipynb' para obtener 'result_OneBigDataset.csv', 'result_OneBigDataset_y_true.csv', 'result_OneBigDataset_test.csv' y 'result_OneBigDataset_y_true_test.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "random.seed(42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_OneBigDataset = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/mortality/result_OneBigDataset.csv')\n",
    "y_true = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/mortality/result_OneBigDataset_y_true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>HRATE</th>\n",
       "      <th>MAP</th>\n",
       "      <th>RESP_RATE</th>\n",
       "      <th>TEMP_C</th>\n",
       "      <th>gcs_e</th>\n",
       "      <th>gcs_m</th>\n",
       "      <th>gcs_total_carevue</th>\n",
       "      <th>gcs_v</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PRELOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>68.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.111111</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>74.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.111111</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>77.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>84.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>84.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>121.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>119.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>119.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>119.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.444444</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>86.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>88.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>36.166667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>87.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.166667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>161.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.166667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>91.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>91.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>92.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.334247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>83.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>89.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>107.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7</td>\n",
       "      <td>103.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>102.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>99.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11</td>\n",
       "      <td>99.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>12</td>\n",
       "      <td>104.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13</td>\n",
       "      <td>101.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.222222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>101.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>37.222222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>104.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.222222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>105.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.222222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17</td>\n",
       "      <td>104.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.277778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>18</td>\n",
       "      <td>106.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.277778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>19</td>\n",
       "      <td>106.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.277778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20</td>\n",
       "      <td>116.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.277778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>21</td>\n",
       "      <td>120.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22</td>\n",
       "      <td>107.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23</td>\n",
       "      <td>110.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.717808</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.183562</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.183562</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hours  HRATE    MAP  RESP_RATE     TEMP_C  gcs_e  gcs_m  \\\n",
       "0       0   86.0   77.0       19.0  36.600000    4.0    6.0   \n",
       "1       1   86.0   77.0       19.0  36.600000    4.0    6.0   \n",
       "2       2   86.0   77.0       19.0  36.600000    4.0    6.0   \n",
       "3       3   80.0   76.0       29.0  36.600000    4.0    6.0   \n",
       "4       4   80.0   76.0       29.0  36.600000    4.0    6.0   \n",
       "5       5   68.0   64.0       26.0  36.600000    4.0    6.0   \n",
       "6       6  100.0   61.0       25.0  36.111111    4.0    6.0   \n",
       "7       7   74.0   69.0       29.0  36.111111    4.0    6.0   \n",
       "8       8   77.0   67.0       27.0  35.444444    4.0    6.0   \n",
       "9       9   84.0   71.0       28.0  35.444444    4.0    6.0   \n",
       "10     10   84.0   71.0       28.0  35.444444    4.0    6.0   \n",
       "11     11  121.0   73.0       30.0  35.444444    4.0    6.0   \n",
       "12     12  121.0   71.0       30.0  35.444444    4.0    6.0   \n",
       "13     13  119.0  100.0       28.0  35.444444    4.0    6.0   \n",
       "14     14  119.0  100.0       28.0  35.444444    4.0    6.0   \n",
       "15     15  119.0   60.0       27.0  35.444444    4.0    6.0   \n",
       "16     16   86.0   64.0       24.0  36.777778    4.0    6.0   \n",
       "17     17   86.0   64.0       29.0  36.777778    4.0    6.0   \n",
       "18     18   88.0   49.0       38.0  36.166667    4.0    6.0   \n",
       "19     19   87.0   63.0       21.0  36.166667    4.0    6.0   \n",
       "20     20  161.0   68.0       30.0  36.166667    4.0    6.0   \n",
       "21     21   91.0   73.0       39.0  37.333333    4.0    6.0   \n",
       "22     22   91.0   83.0       39.0  37.333333    4.0    6.0   \n",
       "23     23   92.0   78.0       34.0  37.333333    4.0    6.0   \n",
       "24      0   86.0   77.0       19.0  36.600000    4.0    6.0   \n",
       "25      1   85.0   78.0       23.0  36.600000    4.0    6.0   \n",
       "26      2   83.0   72.0       18.0  36.600000    4.0    6.0   \n",
       "27      3   89.0   70.0        9.0  36.600000    4.0    6.0   \n",
       "28      4   98.0   69.0       25.0  36.600000    4.0    6.0   \n",
       "29      5   97.0   58.0        8.0  37.777778    4.0    6.0   \n",
       "30      6  107.0   70.0       28.0  37.777778    4.0    6.0   \n",
       "31      7  103.0   65.0       18.0  37.777778    4.0    6.0   \n",
       "32      8  100.0   67.0       19.0  37.777778    4.0    6.0   \n",
       "33      9  102.0   64.0       18.0  37.777778    4.0    6.0   \n",
       "34     10   99.0   64.0       19.0  37.777778    4.0    6.0   \n",
       "35     11   99.0   69.0       19.0  37.777778    4.0    6.0   \n",
       "36     12  104.0   70.0       19.0  37.777778    4.0    6.0   \n",
       "37     13  101.0   64.0       10.0  37.222222    4.0    6.0   \n",
       "38     14  101.0   66.0       11.0  37.222222    4.0    6.0   \n",
       "39     15  104.0   69.0       22.0  37.222222    4.0    6.0   \n",
       "40     16  105.0   66.0       18.0  37.222222    4.0    6.0   \n",
       "41     17  104.0   66.0       23.0  37.277778    4.0    6.0   \n",
       "42     18  106.0   67.0       20.0  37.277778    4.0    6.0   \n",
       "43     19  106.0   67.0       21.0  37.277778    4.0    6.0   \n",
       "44     20  116.0   72.0       20.0  37.277778    4.0    6.0   \n",
       "45     21  120.0   68.0       19.0  37.333333    4.0    6.0   \n",
       "46     22  107.0   71.0       20.0  37.333333    4.0    6.0   \n",
       "47     23  110.0   63.0       16.0  37.333333    4.0    6.0   \n",
       "48      0   60.0   77.0       14.0  36.600000    4.0    6.0   \n",
       "49      1   68.0   71.0       16.0  36.600000    4.0    6.0   \n",
       "\n",
       "    gcs_total_carevue  gcs_v        AGE  PRELOS  \n",
       "0                15.0    5.0  77.334247     0.0  \n",
       "1                15.0    5.0  77.334247     0.0  \n",
       "2                15.0    5.0  77.334247     0.0  \n",
       "3                15.0    5.0  77.334247     0.0  \n",
       "4                15.0    5.0  77.334247     0.0  \n",
       "5                15.0    5.0  77.334247     0.0  \n",
       "6                15.0    5.0  77.334247     0.0  \n",
       "7                15.0    5.0  77.334247     0.0  \n",
       "8                15.0    5.0  77.334247     0.0  \n",
       "9                15.0    5.0  77.334247     0.0  \n",
       "10               15.0    5.0  77.334247     0.0  \n",
       "11               15.0    5.0  77.334247     0.0  \n",
       "12               15.0    5.0  77.334247     0.0  \n",
       "13               15.0    5.0  77.334247     0.0  \n",
       "14               15.0    5.0  77.334247     0.0  \n",
       "15               15.0    5.0  77.334247     0.0  \n",
       "16               15.0    5.0  77.334247     0.0  \n",
       "17               15.0    5.0  77.334247     0.0  \n",
       "18               15.0    5.0  77.334247     0.0  \n",
       "19               15.0    5.0  77.334247     0.0  \n",
       "20               15.0    5.0  77.334247     0.0  \n",
       "21               15.0    5.0  77.334247     0.0  \n",
       "22               15.0    5.0  77.334247     0.0  \n",
       "23               15.0    5.0  77.334247     0.0  \n",
       "24               15.0    1.0  80.717808     5.9  \n",
       "25               15.0    1.0  80.717808     5.9  \n",
       "26               15.0    1.0  80.717808     5.9  \n",
       "27               15.0    1.0  80.717808     5.9  \n",
       "28               15.0    1.0  80.717808     5.9  \n",
       "29               15.0    1.0  80.717808     5.9  \n",
       "30               15.0    1.0  80.717808     5.9  \n",
       "31               15.0    1.0  80.717808     5.9  \n",
       "32               15.0    1.0  80.717808     5.9  \n",
       "33               15.0    1.0  80.717808     5.9  \n",
       "34               15.0    1.0  80.717808     5.9  \n",
       "35               15.0    1.0  80.717808     5.9  \n",
       "36               15.0    1.0  80.717808     5.9  \n",
       "37               15.0    1.0  80.717808     5.9  \n",
       "38               15.0    1.0  80.717808     5.9  \n",
       "39               15.0    1.0  80.717808     5.9  \n",
       "40               15.0    1.0  80.717808     5.9  \n",
       "41               15.0    1.0  80.717808     5.9  \n",
       "42               15.0    1.0  80.717808     5.9  \n",
       "43               15.0    1.0  80.717808     5.9  \n",
       "44               15.0    1.0  80.717808     5.9  \n",
       "45               15.0    5.0  80.717808     5.9  \n",
       "46               15.0    5.0  80.717808     5.9  \n",
       "47               15.0    5.0  80.717808     5.9  \n",
       "48               15.0    5.0  56.183562     0.0  \n",
       "49               15.0    5.0  56.183562     0.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_OneBigDataset.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hours                0\n",
       "HRATE                0\n",
       "MAP                  0\n",
       "RESP_RATE            0\n",
       "TEMP_C               0\n",
       "gcs_e                0\n",
       "gcs_m                0\n",
       "gcs_total_carevue    0\n",
       "gcs_v                0\n",
       "AGE                  0\n",
       "PRELOS               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_OneBigDataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "result_OneBigDataset = np.array(result_OneBigDataset.drop(['Hours'],axis=1))\n",
    "scaler = StandardScaler() #normalizar\n",
    "result_OneBigDataset = scaler.fit_transform(result_OneBigDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sin normalizar:\n",
    "#train_input = np.array(result_OneBigDataset.drop(['Hours'],axis=1)).reshape(y_true.shape[0], 24, 10)\n",
    "#Normalizado:\n",
    "train_input = result_OneBigDataset.reshape(y_true.shape[0], 24, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17903, 24, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(y_true)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17903, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_OneBigDataset_test = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/mortality/result_OneBigDataset_test.csv')\n",
    "y_true_test = pd.read_csv('/data/codi/OASIS/timesSeriesv2_cohorte/data/mortality/result_OneBigDataset_y_true_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>HRATE</th>\n",
       "      <th>MAP</th>\n",
       "      <th>RESP_RATE</th>\n",
       "      <th>TEMP_C</th>\n",
       "      <th>gcs_e</th>\n",
       "      <th>gcs_m</th>\n",
       "      <th>gcs_total_carevue</th>\n",
       "      <th>gcs_v</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PRELOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>62.666698</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>65.666702</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.222198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.222198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>67.333298</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>84.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>95.0</td>\n",
       "      <td>65.666702</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>94.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>108.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>96.0</td>\n",
       "      <td>67.333298</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>93.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57.666698</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>87.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.055599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>88.0</td>\n",
       "      <td>69.666702</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.666702</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>99.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>105.0</td>\n",
       "      <td>63.666698</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>81.0</td>\n",
       "      <td>69.333298</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>89.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>78.0</td>\n",
       "      <td>65.666702</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68.333298</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.888901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.213699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.555599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.555599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>61.0</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>35.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7</td>\n",
       "      <td>64.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.333302</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>76.0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11</td>\n",
       "      <td>82.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>12</td>\n",
       "      <td>83.0</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13</td>\n",
       "      <td>83.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35.777802</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>79.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>35.555599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>75.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>35.555599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17</td>\n",
       "      <td>77.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>18</td>\n",
       "      <td>76.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>19</td>\n",
       "      <td>76.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20</td>\n",
       "      <td>79.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>21</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22</td>\n",
       "      <td>87.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.388901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23</td>\n",
       "      <td>81.0</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.222198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.967177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.331507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.331507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hours  HRATE         MAP  RESP_RATE     TEMP_C  gcs_e  gcs_m  \\\n",
       "0       0   81.0   67.000000       23.0  36.600000    4.0    6.0   \n",
       "1       1   76.0   62.666698       21.0  36.333302    4.0    6.0   \n",
       "2       2   79.0   65.666702       20.0  36.333302    4.0    6.0   \n",
       "3       3   72.0   65.000000       20.0  36.222198    4.0    6.0   \n",
       "4       4   74.0   71.000000       21.0  36.222198    4.0    6.0   \n",
       "5       5   78.0   67.333298       20.0  36.388901    4.0    6.0   \n",
       "6       6   84.0   64.000000       21.0  36.388901    4.0    6.0   \n",
       "7       7   95.0   65.666702       20.0  36.388901    4.0    6.0   \n",
       "8       8   94.0   62.000000       19.0  37.055599    4.0    6.0   \n",
       "9       9  108.0   75.000000       21.0  37.055599    4.0    6.0   \n",
       "10     10   96.0   67.333298       18.0  37.055599    4.0    6.0   \n",
       "11     11   93.0   62.000000       19.0  37.055599    4.0    6.0   \n",
       "12     12   90.0   57.666698       18.0  37.055599    4.0    6.0   \n",
       "13     13   87.0   70.000000       20.0  37.055599    4.0    6.0   \n",
       "14     14   88.0   69.666702       14.0  37.000000    4.0    6.0   \n",
       "15     15  102.0   69.666702       22.0  37.000000    4.0    6.0   \n",
       "16     16   99.0   70.000000       22.0  37.000000    4.0    6.0   \n",
       "17     17  105.0   63.666698       16.0  37.000000    3.0    6.0   \n",
       "18     18   81.0   69.333298       16.0  36.888901    3.0    5.0   \n",
       "19     19   85.0   86.000000       15.0  36.888901    3.0    5.0   \n",
       "20     20   89.0   70.000000       18.0  36.888901    3.0    4.0   \n",
       "21     21   79.0   68.000000       16.0  36.888901    3.0    4.0   \n",
       "22     22   78.0   65.666702       18.0  36.888901    3.0    5.0   \n",
       "23     23   74.0   68.333298       14.0  36.888901    3.0    5.0   \n",
       "24      0   60.0   74.000000       15.0  36.600000    4.0    6.0   \n",
       "25      1   59.0   73.000000       13.0  36.600000    3.0    6.0   \n",
       "26      2   60.0   78.000000       10.0  36.600000    4.0    6.0   \n",
       "27      3   60.0   77.000000       12.0  36.600000    3.0    6.0   \n",
       "28      4   57.0   59.000000       12.0  35.555599    3.0    6.0   \n",
       "29      5   55.0   68.000000       17.0  35.555599    3.0    6.0   \n",
       "30      6   61.0   83.000000       16.0  35.555599    4.0    6.0   \n",
       "31      7   64.0   73.000000       18.0  36.333302    4.0    6.0   \n",
       "32      8   62.0   85.000000       14.0  36.333302    3.0    6.0   \n",
       "33      9   88.0   59.000000       16.0  36.333302    4.0    6.0   \n",
       "34     10   76.0  102.000000       16.0  35.777802    4.0    6.0   \n",
       "35     11   82.0   71.000000       18.0  35.777802    4.0    6.0   \n",
       "36     12   83.0   73.000000       10.0  35.777802    3.0    6.0   \n",
       "37     13   83.0   66.000000       13.0  35.777802    3.0    6.0   \n",
       "38     14   79.0   72.000000       17.0  35.555599    4.0    6.0   \n",
       "39     15   70.0   60.000000       11.0  35.555599    4.0    6.0   \n",
       "40     16   75.0   81.000000       16.0  35.555599    3.0    6.0   \n",
       "41     17   77.0   72.000000       17.0  36.388901    3.0    6.0   \n",
       "42     18   76.0   72.000000       11.0  36.388901    3.0    6.0   \n",
       "43     19   76.0   72.000000       14.0  36.388901    3.0    6.0   \n",
       "44     20   79.0   72.000000       10.0  36.388901    4.0    6.0   \n",
       "45     21   78.0   78.000000       17.0  36.388901    3.0    6.0   \n",
       "46     22   87.0   75.000000       14.0  36.388901    3.0    6.0   \n",
       "47     23   81.0   85.000000       14.0  37.222198    4.0    6.0   \n",
       "48      0   64.0   62.000000       25.0  36.600000    4.0    6.0   \n",
       "49      1   70.0   63.000000       23.0  36.600000    4.0    6.0   \n",
       "\n",
       "    gcs_total_carevue  gcs_v        AGE  PRELOS  \n",
       "0                15.0    5.0  36.213699     0.0  \n",
       "1                15.0    5.0  36.213699     0.0  \n",
       "2                15.0    5.0  36.213699     0.0  \n",
       "3                15.0    5.0  36.213699     0.0  \n",
       "4                15.0    5.0  36.213699     0.0  \n",
       "5                15.0    5.0  36.213699     0.0  \n",
       "6                15.0    5.0  36.213699     0.0  \n",
       "7                15.0    5.0  36.213699     0.0  \n",
       "8                14.0    4.0  36.213699     0.0  \n",
       "9                14.0    4.0  36.213699     0.0  \n",
       "10               14.0    4.0  36.213699     0.0  \n",
       "11               14.0    4.0  36.213699     0.0  \n",
       "12               14.0    4.0  36.213699     0.0  \n",
       "13               14.0    4.0  36.213699     0.0  \n",
       "14               14.0    4.0  36.213699     0.0  \n",
       "15               14.0    4.0  36.213699     0.0  \n",
       "16               14.0    4.0  36.213699     0.0  \n",
       "17               13.0    4.0  36.213699     0.0  \n",
       "18               12.0    4.0  36.213699     0.0  \n",
       "19               12.0    4.0  36.213699     0.0  \n",
       "20               11.0    4.0  36.213699     0.0  \n",
       "21               11.0    4.0  36.213699     0.0  \n",
       "22               12.0    4.0  36.213699     0.0  \n",
       "23               12.0    4.0  36.213699     0.0  \n",
       "24               15.0    5.0  96.967177     0.0  \n",
       "25               14.0    5.0  96.967177     0.0  \n",
       "26               15.0    5.0  96.967177     0.0  \n",
       "27               13.0    4.0  96.967177     0.0  \n",
       "28               13.0    4.0  96.967177     0.0  \n",
       "29               13.0    4.0  96.967177     0.0  \n",
       "30               14.0    4.0  96.967177     0.0  \n",
       "31               14.0    4.0  96.967177     0.0  \n",
       "32               13.0    4.0  96.967177     0.0  \n",
       "33               14.0    4.0  96.967177     0.0  \n",
       "34               14.0    4.0  96.967177     0.0  \n",
       "35               14.0    4.0  96.967177     0.0  \n",
       "36               13.0    4.0  96.967177     0.0  \n",
       "37               13.0    4.0  96.967177     0.0  \n",
       "38               14.0    4.0  96.967177     0.0  \n",
       "39               14.0    4.0  96.967177     0.0  \n",
       "40               14.0    5.0  96.967177     0.0  \n",
       "41               14.0    5.0  96.967177     0.0  \n",
       "42               13.0    4.0  96.967177     0.0  \n",
       "43               13.0    4.0  96.967177     0.0  \n",
       "44               14.0    4.0  96.967177     0.0  \n",
       "45               10.0    1.0  96.967177     0.0  \n",
       "46               10.0    1.0  96.967177     0.0  \n",
       "47               14.0    4.0  96.967177     0.0  \n",
       "48               15.0    5.0  77.331507     0.0  \n",
       "49               15.0    5.0  77.331507     0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_OneBigDataset_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "result_OneBigDataset_test = np.array(result_OneBigDataset_test.drop(['Hours'],axis=1))\n",
    "scaler = StandardScaler()\n",
    "result_OneBigDataset_test = scaler.fit_transform(result_OneBigDataset_test)\n",
    "test_input = result_OneBigDataset_test.reshape(y_true_test.shape[0], 24, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.74732223e-03, -7.27005752e-01,  6.46345385e-01, ...,\n",
       "          7.80504035e-01, -1.66137586e+00, -3.27468222e-01],\n",
       "        [-3.88666564e-03, -1.01600694e+00,  3.02684882e-01, ...,\n",
       "          7.80504035e-01, -1.66137586e+00, -3.27468222e-01],\n",
       "        [-3.80305959e-03, -8.15927471e-01,  1.30854631e-01, ...,\n",
       "          7.80504035e-01, -1.66137586e+00, -3.27468222e-01],\n",
       "        ...,\n",
       "        [-3.80305959e-03, -6.60312682e-01, -5.56466376e-01, ...,\n",
       "          2.41431713e-01, -1.66137586e+00, -3.27468222e-01],\n",
       "        [-3.83092828e-03, -8.15927471e-01, -2.12805873e-01, ...,\n",
       "          2.41431713e-01, -1.66137586e+00, -3.27468222e-01],\n",
       "        [-3.94240300e-03, -6.38084034e-01, -9.00126879e-01, ...,\n",
       "          2.41431713e-01, -1.66137586e+00, -3.27468222e-01]],\n",
       "\n",
       "       [[-4.33256455e-03, -2.60154262e-01, -7.28296627e-01, ...,\n",
       "          7.80504035e-01,  1.82930468e+00, -3.27468222e-01],\n",
       "        [-4.36043324e-03, -3.26847332e-01, -1.07195713e+00, ...,\n",
       "          7.80504035e-01,  1.82930468e+00, -3.27468222e-01],\n",
       "        [-4.33256455e-03,  6.61801754e-03, -1.58744789e+00, ...,\n",
       "          7.80504035e-01,  1.82930468e+00, -3.27468222e-01],\n",
       "        ...,\n",
       "        [-3.83092828e-03,  6.61801754e-03, -3.84636124e-01, ...,\n",
       "         -1.37578525e+00,  1.82930468e+00, -3.27468222e-01],\n",
       "        [-3.58011014e-03, -1.93461192e-01, -9.00126879e-01, ...,\n",
       "         -1.37578525e+00,  1.82930468e+00, -3.27468222e-01],\n",
       "        [-3.74732223e-03,  4.73469507e-01, -9.00126879e-01, ...,\n",
       "          2.41431713e-01,  1.82930468e+00, -3.27468222e-01]],\n",
       "\n",
       "       [[-4.22108983e-03, -1.06047110e+00,  9.90005889e-01, ...,\n",
       "          7.80504035e-01,  7.01108340e-01, -3.27468222e-01],\n",
       "        [-4.05387773e-03, -9.93778032e-01,  6.46345385e-01, ...,\n",
       "          7.80504035e-01,  7.01108340e-01, -3.27468222e-01],\n",
       "        [-3.94240300e-03, -9.27084962e-01,  4.74515134e-01, ...,\n",
       "          7.80504035e-01,  7.01108340e-01, -3.27468222e-01],\n",
       "        ...,\n",
       "        [-4.13748378e-03, -7.93698822e-01,  6.46345385e-01, ...,\n",
       "          7.80504035e-01,  7.01108340e-01, -3.27468222e-01],\n",
       "        [-4.02600905e-03, -3.26847332e-01,  6.46345385e-01, ...,\n",
       "          7.80504035e-01,  7.01108340e-01, -3.27468222e-01],\n",
       "        [-4.08174642e-03, -1.39393645e+00,  6.46345385e-01, ...,\n",
       "          7.80504035e-01,  7.01108340e-01, -3.27468222e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.60797882e-03, -6.00750524e-02, -4.09756210e-02, ...,\n",
       "          7.80504035e-01, -1.28751526e+00, -3.27468222e-01],\n",
       "        [-3.60797882e-03,  5.27537055e+00, -4.09756210e-02, ...,\n",
       "          7.80504035e-01, -1.28751526e+00, -3.27468222e-01],\n",
       "        [-3.60797882e-03,  2.14079626e+00, -4.09756210e-02, ...,\n",
       "          7.80504035e-01, -1.28751526e+00, -3.27468222e-01],\n",
       "        ...,\n",
       "        [-2.96699913e-03,  1.94071705e+00,  3.39562941e+00, ...,\n",
       "         -8.36712931e-01, -1.28751526e+00, -3.27468222e-01],\n",
       "        [-2.85552440e-03,  2.20748933e+00,  2.70830840e+00, ...,\n",
       "          7.80504035e-01, -1.28751526e+00, -3.27468222e-01],\n",
       "        [-2.85552440e-03,  1.27378635e+00,  3.73928991e+00, ...,\n",
       "          7.80504035e-01, -1.28751526e+00, -3.27468222e-01]],\n",
       "\n",
       "       [[-3.27355464e-03, -6.00750524e-02, -5.56466376e-01, ...,\n",
       "          7.80504035e-01,  5.05756335e-01, -3.27468222e-01],\n",
       "        [-3.58011014e-03,  1.34047942e+00,  3.02684882e-01, ...,\n",
       "          7.80504035e-01,  5.05756335e-01, -3.27468222e-01],\n",
       "        [-3.63584750e-03,  9.40320997e-01,  4.74515134e-01, ...,\n",
       "          7.80504035e-01,  5.05756335e-01, -3.27468222e-01],\n",
       "        ...,\n",
       "        [-3.44076673e-03, -7.93698822e-01,  3.02684882e-01, ...,\n",
       "         -1.37578525e+00,  5.05756335e-01, -3.27468222e-01],\n",
       "        [-3.52437277e-03, -6.00750524e-02, -2.12805873e-01, ...,\n",
       "         -1.37578525e+00,  5.05756335e-01, -3.27468222e-01],\n",
       "        [-3.66371618e-03, -1.46062952e+00,  3.02684882e-01, ...,\n",
       "         -1.37578525e+00,  5.05756335e-01, -3.27468222e-01]],\n",
       "\n",
       "       [[-3.60797882e-03, -6.00750524e-02, -4.09756210e-02, ...,\n",
       "          7.80504035e-01,  1.25741292e+00, -3.27468222e-01],\n",
       "        [-3.60797882e-03, -6.00750524e-02, -4.09756210e-02, ...,\n",
       "          7.80504035e-01,  1.25741292e+00, -3.27468222e-01],\n",
       "        [-4.36043324e-03, -1.06047110e+00, -2.12805873e-01, ...,\n",
       "          7.80504035e-01,  1.25741292e+00, -3.27468222e-01],\n",
       "        ...,\n",
       "        [-4.49977665e-03, -2.06086715e+00, -3.84636124e-01, ...,\n",
       "         -1.37578525e+00,  1.25741292e+00, -3.27468222e-01],\n",
       "        [-4.49977665e-03, -2.06086715e+00, -3.84636124e-01, ...,\n",
       "         -1.37578525e+00,  1.25741292e+00, -3.27468222e-01],\n",
       "        [-4.55551401e-03, -1.79409487e+00, -2.12805873e-01, ...,\n",
       "         -1.37578525e+00,  1.25741292e+00, -3.27468222e-01]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3236, 24, 10)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_test = np.array(y_true_test)\n",
    "y_true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3236, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3236,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_true_test.ravel()\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para Cross Validation, usar el dataset completo # Arthur\n",
    "y = (np.concatenate([y_true, y_true_test], axis=0))\n",
    "X = (np.concatenate([result_OneBigDataset, result_OneBigDataset_test], axis=0)).reshape(y.shape[0], 24, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21139, 24, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21139, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_output_dir = '/data/codi/OASIS/timesSeriesv2_cohorte/data/mortality/CV/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM without class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 20:31:17.634325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-26 20:31:17.634369: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-26 20:31:21.744964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-26 20:31:21.744999: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-26 20:31:21.745029: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sony-vaio): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "560/560 [==============================] - 15s 22ms/step - loss: 0.4005 - accuracy: 0.8417\n",
      "Epoch 2/8\n",
      "560/560 [==============================] - 11s 20ms/step - loss: 0.3443 - accuracy: 0.8651\n",
      "Epoch 3/8\n",
      "560/560 [==============================] - 10s 18ms/step - loss: 0.3401 - accuracy: 0.8649\n",
      "Epoch 4/8\n",
      "560/560 [==============================] - 10s 18ms/step - loss: 0.3394 - accuracy: 0.8649\n",
      "Epoch 5/8\n",
      "560/560 [==============================] - 10s 18ms/step - loss: 0.3340 - accuracy: 0.8668\n",
      "Epoch 6/8\n",
      "560/560 [==============================] - 10s 19ms/step - loss: 0.3356 - accuracy: 0.8666\n",
      "Epoch 7/8\n",
      "560/560 [==============================] - 11s 19ms/step - loss: 0.3342 - accuracy: 0.8659\n",
      "Epoch 8/8\n",
      "560/560 [==============================] - 10s 19ms/step - loss: 0.3327 - accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_input, y_true, epochs=8, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 20)               1680      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,701\n",
      "Trainable params: 1,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "test_output = model.predict(test_input, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20526809],\n",
       "       [0.14530939],\n",
       "       [0.31590798],\n",
       "       ...,\n",
       "       [0.01459944],\n",
       "       [0.20890284],\n",
       "       [0.51131535]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(0 survived, 1 dead) predicted:  [0. 0. 0. ... 0. 0. 1.]\n",
      "true labels:  [1. 0. 0. ... 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (test_output.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "print('labels(0 survived, 1 dead) predicted: ', y_pred)\n",
    "print('true labels: ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 88.1644\n",
      "AUC-ROC:  0.7437200342305781\n",
      "AUC-PR:  0.29993549339638587\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "test_ac=np.round(metrics.accuracy_score(y_test, y_pred)*100,4)\n",
    "print(\"Accuracy test:\",test_ac)\n",
    "auroc = metrics.roc_auc_score(y_test, test_output)\n",
    "print(\"AUC-ROC: \", auroc)\n",
    "(precisions, recalls, thresholds) = metrics.precision_recall_curve(y_test, test_output)\n",
    "auprc = metrics.auc(recalls, precisions)\n",
    "print(\"AUC-PR: \", auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2804   58]\n",
      " [ 325   49]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format((test_ac))\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(test_ac)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57826227, 3.69438712])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWeight = class_weight.compute_class_weight('balanced',np.unique(np.ravel(y_true)),np.ravel(y_true))\n",
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5782622739018087, 1: 3.694387123400743}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWeight = {i : classWeight[i] for i in range(2)}  #convert to dictionary in order to fit to keras model\n",
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "560/560 [==============================] - 8s 10ms/step - loss: 0.6329 - accuracy: 0.6629\n",
      "Epoch 2/8\n",
      "560/560 [==============================] - 6s 10ms/step - loss: 0.5669 - accuracy: 0.7138\n",
      "Epoch 3/8\n",
      "560/560 [==============================] - 5s 9ms/step - loss: 0.5735 - accuracy: 0.7064\n",
      "Epoch 4/8\n",
      "560/560 [==============================] - 6s 10ms/step - loss: 0.5667 - accuracy: 0.7070\n",
      "Epoch 5/8\n",
      "560/560 [==============================] - 5s 10ms/step - loss: 0.5636 - accuracy: 0.7150\n",
      "Epoch 6/8\n",
      "560/560 [==============================] - 5s 9ms/step - loss: 0.5517 - accuracy: 0.7100\n",
      "Epoch 7/8\n",
      "560/560 [==============================] - 6s 11ms/step - loss: 0.5560 - accuracy: 0.7141\n",
      "Epoch 8/8\n",
      "560/560 [==============================] - 5s 10ms/step - loss: 0.5423 - accuracy: 0.7268\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_input, y_true, epochs=8, verbose=1,class_weight=classWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 20)                1680      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,701\n",
      "Trainable params: 1,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test Dataset, same process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_output = model.predict(test_input, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72365683]\n",
      " [0.49774703]\n",
      " [0.79233277]\n",
      " ...\n",
      " [0.06404442]\n",
      " [0.5926774 ]\n",
      " [0.89771557]]\n"
     ]
    }
   ],
   "source": [
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (test_output.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(0 survived, 1 dead) predicted:  [1. 0. 1. ... 0. 1. 1.]\n",
      "true labels:  [1. 0. 0. ... 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('labels(0 survived, 1 dead) predicted: ', y_pred)\n",
    "print('true labels: ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 61.9283\n"
     ]
    }
   ],
   "source": [
    "test_ac=np.round(metrics.accuracy_score(y_test, y_pred)*100,4)\n",
    "print(\"Accuracy test:\",test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manera 2 de calcular Accuracy:\n",
    "#https://keras.io/api/metrics/accuracy_metrics/\n",
    "#import tensorflow as tf\n",
    "#m = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "#m.update_state(y_true_test,test_output)\n",
    "#score = m.result().numpy()\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC:  0.7376390617234124\n",
      "AUC-PR:  0.2914674011641614\n"
     ]
    }
   ],
   "source": [
    "auroc = metrics.roc_auc_score(y_test, test_output)\n",
    "print(\"AUC-ROC: \", auroc)\n",
    "(precisions, recalls, thresholds) = metrics.precision_recall_curve(y_test, test_output)\n",
    "auprc = metrics.auc(recalls, precisions)\n",
    "print(\"AUC-PR: \", auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1726 1136]\n",
      " [  96  278]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format((test_ac))\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(test_ac)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arthur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def kfoldclustered(classifier, X, y, weighted=False):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f = 0;\n",
    "    aucprs = []\n",
    "    aucrocs = []\n",
    "    accuracies = []\n",
    "    for train, test in cv.split(X, y): #train and test are indexes\n",
    "        f += 1\n",
    "        print('KFold ',f,' ---')\n",
    "        train_input = X[train]\n",
    "        test_input = X[test]\n",
    "        y_true = y[train]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        print(\"---- Clustering Train ---- \")\n",
    "        # Apply TSCK\n",
    "        n_clusters = 3\n",
    "        kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", n_init=1, max_iter=10, random_state=42)\n",
    "        # Fit to the training data\n",
    "        kmeans.fit(train_input)\n",
    "        # Generate out clusters\n",
    "        train_cluster = kmeans.predict(train_input)\n",
    "        # Add predicted cluster and y regression label to our training DataFrame\n",
    "        train_df = list(zip(train_cluster, y_true, train_input))\n",
    "        ls = sorted(train_df, key=lambda t: t[0])\n",
    "        # Unzip sorted data\n",
    "        cluster, y_, data = zip(*ls)\n",
    "        data = np.array(data)\n",
    "        y_ = np.array(y_)\n",
    "        # Getting indexes of clusters division\n",
    "        c = 0\n",
    "        ind = []\n",
    "        for i in range(len(ls)):\n",
    "            if ls[i][0] > c:\n",
    "                c = ls[i][0]\n",
    "                ind.append(i)\n",
    "        # Removing clusters with less than 10 samples\n",
    "        d1 = ind[0]\n",
    "        d2 = ind[1] - ind[0]\n",
    "        d3 = y_true.shape[0] - ind[1]\n",
    "        cluster_centers_ = kmeans.cluster_centers_\n",
    "        if d1 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 1\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[:ind[0]] = c\n",
    "            ind = ind[1:]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 0, 0)\n",
    "        elif d2 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[ind[0]:ind[1]] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 1, 0)\n",
    "        elif d3 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 1\n",
    "            train_cluster[ind[1]:] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 2, 0)\n",
    "            \n",
    "        print(\"-------- LSTM fitting for each cluster --------\")\n",
    "        i=0\n",
    "        if weighted:\n",
    "            classWeight = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                classes=np.unique(np.ravel(y_true)),\n",
    "                                                y=np.ravel(y_true))\n",
    "            classWeight = {i : classWeight[i] for i in range(2)}\n",
    "        for index in range(len(ind)+1):\n",
    "            if index == 0:\n",
    "                cluster_X = data[:ind[index],:,:]\n",
    "                cluster_Y = y_[:ind[index]]\n",
    "            elif index == len(ind):\n",
    "                cluster_X = data[ind[index-1]:,:,:]\n",
    "                cluster_Y = y_[ind[index-1]:]\n",
    "            else:\n",
    "                cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "                cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "            print(cluster_X.shape)\n",
    "            \n",
    "            # LSTM\n",
    "            model = Sequential()\n",
    "            model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "            \n",
    "            # LSTM\n",
    "            if weighted == False:\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_tsck{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "            # LSTM_W\n",
    "            else:\n",
    "                print(\"PASSOU\")\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_tsck{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, class_weight=classWeight, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "                \n",
    "        testkfold(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted)\n",
    "    return aucprs, aucrocs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def testkfold(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted):\n",
    "    # Create initial test data to store assigned clusters\n",
    "    test_df = list(zip(train_cluster, y_test, test_input))\n",
    "    # Test sample es asignado al cluster correspondiente mediante Distancia euclidiana y se aplica el modelo correspondiente\n",
    "    print(\"Assigning each test sample to the closest cluster centroid...\")\n",
    "    new_cluster = [0 for i in range(y_test.shape[0])]\n",
    "    for row in range(len(test_df)):\n",
    "        min_distance = float('inf')\n",
    "        closest_cluster = None\n",
    "        for k in range(cluster_centers_.shape[0]):\n",
    "            # Check if the assigned cluster has more than 100 samples\n",
    "            # if train_clusters_df[k].shape[0] > 100: # Probar sin limite\n",
    "            distance = np.linalg.norm(cluster_centers_[k]-test_df[row][2])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_cluster = k\n",
    "        # Assign cluster to test sample\n",
    "        new_cluster[row] = closest_cluster\n",
    "    # Sort test data\n",
    "    test_df = list(zip(new_cluster, y_test, test_input))\n",
    "    ls_test = sorted(test_df, key=lambda t: t[0])\n",
    "    # Unzip sorted data\n",
    "    cluster_t, y_t, data_t = zip(*ls_test)\n",
    "    data_t = np.array(data_t)\n",
    "    y_t = np.array(y_t)\n",
    "    # Getting indexes\n",
    "    c = 0\n",
    "    ind_t = []\n",
    "    for i in range(len(ls_test)):\n",
    "        if ls_test[i][0] > c:\n",
    "            c = ls_test[i][0]\n",
    "            ind_t.append(i)\n",
    "    print(\"-------- Train metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data[:ind[index],:,:]\n",
    "            cluster_Y = y_[:ind[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data[ind[index-1]:,:,:]\n",
    "            cluster_Y = y_[ind[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "            cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_tsck{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_tsck{}.h5'.format(f,i))\n",
    "        \n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            train_X_probs = np.array(classes_prob)\n",
    "            y_train = np.array(cluster_Y)\n",
    "        else: \n",
    "            train_X_probs = np.concatenate((train_X_probs, classes_prob))\n",
    "            y_train = np.concatenate((y_train, cluster_Y))\n",
    "        i += 1\n",
    "\n",
    "    # Test metrics\n",
    "    print(\"-------- Test metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind_t)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data_t[:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[:ind_t[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data_t[ind_t[index-1]:,:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data_t[ind_t[index-1]:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:ind_t[index]]\n",
    "            \n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_tsck{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_tsck{}.h5'.format(f,i))\n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            test_X_probs = np.array(classes_prob)\n",
    "            test_y = np.array(cluster_Y)\n",
    "        else: \n",
    "            test_X_probs = np.concatenate((test_X_probs, classes_prob))\n",
    "            test_y = np.concatenate((test_y, cluster_Y))\n",
    "        print('----- cluster ',i,' -----')\n",
    "        print(cluster_X.shape)\n",
    "        # Test metrics\n",
    "        y_pred1 = (classes_prob.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "        test_ac1=np.round(metrics.accuracy_score(cluster_Y, y_pred1)*100,4)\n",
    "        print(\"Accuracy test cluster \",i,\":\", test_ac1)\n",
    "        i += 1\n",
    "    print(\"------ Metrics ------\")\n",
    "    # Train metrics\n",
    "    y_pred_train = (train_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred_train)\n",
    "    print('true labels: ', y_train)\n",
    "    # Test metrics\n",
    "    y_pred = (test_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred)\n",
    "    print('true labels: ', test_y)\n",
    "    #\n",
    "    train_ac=np.round(metrics.accuracy_score(y_train, y_pred_train)*100,4)\n",
    "    print(\"Accuracy train:\", train_ac)\n",
    "    test_ac=np.round(metrics.accuracy_score(test_y, y_pred)*100,4)\n",
    "    print(\"Accuracy test:\", test_ac)\n",
    "    auroc = metrics.roc_auc_score(test_y, test_X_probs)\n",
    "    print(\"AUC-ROC: \", auroc)\n",
    "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(test_y, test_X_probs)\n",
    "    auprc = metrics.auc(recalls, precisions)\n",
    "    print(\"AUC-PR: \", auprc)\n",
    "    aucprs.append(auprc)\n",
    "    aucrocs.append(auroc)\n",
    "    accuracies.append(test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(1461, 24, 10)\n",
      "Epoch 1/10\n",
      "39/39 [==============================] - 5s 34ms/step - loss: 0.6429 - accuracy: 0.6575 - val_loss: 0.5644 - val_accuracy: 0.7909\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.5573 - accuracy: 0.7768 - val_loss: 0.5200 - val_accuracy: 0.7909\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.5332 - accuracy: 0.7776 - val_loss: 0.5035 - val_accuracy: 0.7909\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.5198 - accuracy: 0.7768 - val_loss: 0.4930 - val_accuracy: 0.7909\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.5092 - accuracy: 0.7768 - val_loss: 0.4841 - val_accuracy: 0.7909\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.4934 - accuracy: 0.7776 - val_loss: 0.4770 - val_accuracy: 0.7909\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.4839 - accuracy: 0.7760 - val_loss: 0.4704 - val_accuracy: 0.7909\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.4793 - accuracy: 0.7776 - val_loss: 0.4653 - val_accuracy: 0.7909\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.4650 - accuracy: 0.7776 - val_loss: 0.4611 - val_accuracy: 0.7909\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.4627 - accuracy: 0.7744 - val_loss: 0.4580 - val_accuracy: 0.7909\n",
      "(12084, 24, 10)\n",
      "Epoch 1/10\n",
      "321/321 [==============================] - 9s 20ms/step - loss: 0.3553 - accuracy: 0.8912 - val_loss: 0.2621 - val_accuracy: 0.9173\n",
      "Epoch 2/10\n",
      "321/321 [==============================] - 6s 17ms/step - loss: 0.2935 - accuracy: 0.9043 - val_loss: 0.2550 - val_accuracy: 0.9173\n",
      "Epoch 3/10\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.2854 - accuracy: 0.9040 - val_loss: 0.2534 - val_accuracy: 0.9173\n",
      "Epoch 4/10\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.2830 - accuracy: 0.9048 - val_loss: 0.2540 - val_accuracy: 0.9173\n",
      "Epoch 5/10\n",
      "321/321 [==============================] - 6s 19ms/step - loss: 0.2820 - accuracy: 0.9047 - val_loss: 0.2598 - val_accuracy: 0.9167\n",
      "Epoch 6/10\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9046Restoring model weights from the end of the best epoch: 3.\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.2800 - accuracy: 0.9046 - val_loss: 0.2634 - val_accuracy: 0.9184\n",
      "Epoch 6: early stopping\n",
      "(3366, 24, 10)\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - 6s 28ms/step - loss: 0.6308 - accuracy: 0.6561 - val_loss: 0.5040 - val_accuracy: 0.8040\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 0.5413 - accuracy: 0.7655 - val_loss: 0.4746 - val_accuracy: 0.8040\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.5210 - accuracy: 0.7679 - val_loss: 0.4599 - val_accuracy: 0.8040\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 0.5002 - accuracy: 0.7690 - val_loss: 0.4492 - val_accuracy: 0.8040\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 0.4942 - accuracy: 0.7665 - val_loss: 0.4412 - val_accuracy: 0.8079\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 2s 20ms/step - loss: 0.4889 - accuracy: 0.7672 - val_loss: 0.4426 - val_accuracy: 0.8139\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 0.4829 - accuracy: 0.7704 - val_loss: 0.4331 - val_accuracy: 0.8099\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.4772 - accuracy: 0.7609 - val_loss: 0.4309 - val_accuracy: 0.8099\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.4775 - accuracy: 0.7718 - val_loss: 0.4310 - val_accuracy: 0.8158\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.4667 - accuracy: 0.7791 - val_loss: 0.4295 - val_accuracy: 0.8119\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "46/46 [==============================] - 1s 7ms/step\n",
      "378/378 [==============================] - 3s 6ms/step\n",
      "106/106 [==============================] - 1s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "11/11 [==============================] - 0s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(330, 24, 10)\n",
      "Accuracy test cluster  0 : 72.7273\n",
      "98/98 [==============================] - 1s 7ms/step\n",
      "----- cluster  1  -----\n",
      "(3113, 24, 10)\n",
      "Accuracy test cluster  1 : 90.1703\n",
      "25/25 [==============================] - 1s 9ms/step\n",
      "----- cluster  2  -----\n",
      "(785, 24, 10)\n",
      "Accuracy test cluster  2 : 76.9427\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 87.0853\n",
      "Accuracy test: 86.3529\n",
      "AUC-ROC:  0.7704547602843314\n",
      "AUC-PR:  0.36626203681705416\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(3880, 24, 10)\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 5s 22ms/step - loss: 0.5328 - accuracy: 0.7899 - val_loss: 0.4638 - val_accuracy: 0.8265\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 2s 19ms/step - loss: 0.4620 - accuracy: 0.8281 - val_loss: 0.4444 - val_accuracy: 0.8265\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.4443 - accuracy: 0.8293 - val_loss: 0.4312 - val_accuracy: 0.8265\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.4300 - accuracy: 0.8272 - val_loss: 0.4206 - val_accuracy: 0.8299\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.4220 - accuracy: 0.8290 - val_loss: 0.4130 - val_accuracy: 0.8299\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 2s 18ms/step - loss: 0.4208 - accuracy: 0.8275 - val_loss: 0.4087 - val_accuracy: 0.8299\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.4076 - accuracy: 0.8290 - val_loss: 0.4054 - val_accuracy: 0.8299\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.4055 - accuracy: 0.8311 - val_loss: 0.4035 - val_accuracy: 0.8282\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 2s 20ms/step - loss: 0.4069 - accuracy: 0.8302 - val_loss: 0.4034 - val_accuracy: 0.8247\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 2s 19ms/step - loss: 0.4063 - accuracy: 0.8275 - val_loss: 0.4028 - val_accuracy: 0.8265\n",
      "(2732, 24, 10)\n",
      "Epoch 1/10\n",
      "73/73 [==============================] - 5s 24ms/step - loss: 0.5642 - accuracy: 0.7412 - val_loss: 0.5043 - val_accuracy: 0.7829\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.5437 - accuracy: 0.7455 - val_loss: 0.4899 - val_accuracy: 0.7829\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.5325 - accuracy: 0.7429 - val_loss: 0.4797 - val_accuracy: 0.7829\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.5223 - accuracy: 0.7463 - val_loss: 0.4739 - val_accuracy: 0.7902\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.5137 - accuracy: 0.7446 - val_loss: 0.4671 - val_accuracy: 0.7951\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 3s 41ms/step - loss: 0.5097 - accuracy: 0.7481 - val_loss: 0.4636 - val_accuracy: 0.7854\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 3s 36ms/step - loss: 0.4988 - accuracy: 0.7476 - val_loss: 0.4577 - val_accuracy: 0.7878\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 0.4939 - accuracy: 0.7571 - val_loss: 0.4585 - val_accuracy: 0.7854\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4932 - accuracy: 0.7519 - val_loss: 0.4547 - val_accuracy: 0.7854\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.4857 - accuracy: 0.7597 - val_loss: 0.4557 - val_accuracy: 0.7805\n",
      "(10299, 24, 10)\n",
      "Epoch 1/10\n",
      "274/274 [==============================] - 8s 20ms/step - loss: 0.3220 - accuracy: 0.9099 - val_loss: 0.2263 - val_accuracy: 0.9327\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 5s 18ms/step - loss: 0.2775 - accuracy: 0.9119 - val_loss: 0.2245 - val_accuracy: 0.9320\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 5s 18ms/step - loss: 0.2709 - accuracy: 0.9123 - val_loss: 0.2255 - val_accuracy: 0.9320\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 5s 20ms/step - loss: 0.2681 - accuracy: 0.9131 - val_loss: 0.2274 - val_accuracy: 0.9327\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9123Restoring model weights from the end of the best epoch: 2.\n",
      "274/274 [==============================] - 5s 20ms/step - loss: 0.2666 - accuracy: 0.9123 - val_loss: 0.2316 - val_accuracy: 0.9340\n",
      "Epoch 5: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "86/86 [==============================] - 1s 7ms/step\n",
      "322/322 [==============================] - 2s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "33/33 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(1044, 24, 10)\n",
      "Accuracy test cluster  0 : 82.1839\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(630, 24, 10)\n",
      "Accuracy test cluster  1 : 74.2857\n",
      "80/80 [==============================] - 1s 7ms/step\n",
      "----- cluster  2  -----\n",
      "(2554, 24, 10)\n",
      "Accuracy test cluster  2 : 91.2686\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 87.174\n",
      "Accuracy test: 86.4948\n",
      "AUC-ROC:  0.7934405329274967\n",
      "AUC-PR:  0.38288389666568506\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(2922, 24, 10)\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 5s 25ms/step - loss: 0.5870 - accuracy: 0.7241 - val_loss: 0.4961 - val_accuracy: 0.7927\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.5546 - accuracy: 0.7402 - val_loss: 0.4798 - val_accuracy: 0.7927\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.5420 - accuracy: 0.7431 - val_loss: 0.4658 - val_accuracy: 0.7927\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.5293 - accuracy: 0.7410 - val_loss: 0.4602 - val_accuracy: 0.7995\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.5235 - accuracy: 0.7394 - val_loss: 0.4581 - val_accuracy: 0.8018\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.5206 - accuracy: 0.7479 - val_loss: 0.4490 - val_accuracy: 0.8018\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.5116 - accuracy: 0.7431 - val_loss: 0.4515 - val_accuracy: 0.7995\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.5106 - accuracy: 0.7463 - val_loss: 0.4505 - val_accuracy: 0.7995\n",
      "Epoch 9/10\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.5067 - accuracy: 0.7451Restoring model weights from the end of the best epoch: 6.\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.5049 - accuracy: 0.7471 - val_loss: 0.4529 - val_accuracy: 0.7995\n",
      "Epoch 9: early stopping\n",
      "(5886, 24, 10)\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 7s 20ms/step - loss: 0.3590 - accuracy: 0.8957 - val_loss: 0.2561 - val_accuracy: 0.9185\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.2893 - accuracy: 0.9027 - val_loss: 0.2424 - val_accuracy: 0.9185\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.2770 - accuracy: 0.9033 - val_loss: 0.2343 - val_accuracy: 0.9185\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.2703 - accuracy: 0.9043 - val_loss: 0.2312 - val_accuracy: 0.9185\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.2638 - accuracy: 0.9047 - val_loss: 0.2305 - val_accuracy: 0.9173\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.2636 - accuracy: 0.9037 - val_loss: 0.2316 - val_accuracy: 0.9162\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.2618 - accuracy: 0.9053 - val_loss: 0.2324 - val_accuracy: 0.9162\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9055Restoring model weights from the end of the best epoch: 5.\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.2593 - accuracy: 0.9055 - val_loss: 0.2339 - val_accuracy: 0.9151\n",
      "Epoch 8: early stopping\n",
      "(8103, 24, 10)\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 7s 20ms/step - loss: 0.3784 - accuracy: 0.8799 - val_loss: 0.3312 - val_accuracy: 0.8890\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 0.3477 - accuracy: 0.8799 - val_loss: 0.3231 - val_accuracy: 0.8890\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3413 - accuracy: 0.8799 - val_loss: 0.3178 - val_accuracy: 0.8890\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 8s 39ms/step - loss: 0.3379 - accuracy: 0.8801 - val_loss: 0.3161 - val_accuracy: 0.8890\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 6s 27ms/step - loss: 0.3354 - accuracy: 0.8793 - val_loss: 0.3162 - val_accuracy: 0.8898\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3342 - accuracy: 0.8802 - val_loss: 0.3168 - val_accuracy: 0.8898\n",
      "Epoch 7/10\n",
      "213/216 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.8812Restoring model weights from the end of the best epoch: 4.\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3307 - accuracy: 0.8806 - val_loss: 0.3191 - val_accuracy: 0.8890\n",
      "Epoch 7: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "92/92 [==============================] - 1s 5ms/step\n",
      "184/184 [==============================] - 1s 5ms/step\n",
      "254/254 [==============================] - 2s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "30/30 [==============================] - 0s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(956, 24, 10)\n",
      "Accuracy test cluster  0 : 75.1046\n",
      "37/37 [==============================] - 1s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(1176, 24, 10)\n",
      "Accuracy test cluster  1 : 93.9626\n",
      "66/66 [==============================] - 1s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(2096, 24, 10)\n",
      "Accuracy test cluster  2 : 88.5496\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 86.8192\n",
      "Accuracy test: 87.0151\n",
      "AUC-ROC:  0.788776753715523\n",
      "AUC-PR:  0.3876778820247252\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(1800, 24, 10)\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 6s 34ms/step - loss: 0.6651 - accuracy: 0.6078 - val_loss: 0.5721 - val_accuracy: 0.7704\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6305 - accuracy: 0.6784 - val_loss: 0.5442 - val_accuracy: 0.7667\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6138 - accuracy: 0.6758 - val_loss: 0.5339 - val_accuracy: 0.7741\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6130 - accuracy: 0.6856 - val_loss: 0.5253 - val_accuracy: 0.7704\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6001 - accuracy: 0.6863 - val_loss: 0.5149 - val_accuracy: 0.7704\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.5907 - accuracy: 0.6817 - val_loss: 0.5101 - val_accuracy: 0.7741\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.5773 - accuracy: 0.6980 - val_loss: 0.4945 - val_accuracy: 0.7741\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.5759 - accuracy: 0.6876 - val_loss: 0.4898 - val_accuracy: 0.7778\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 20ms/step - loss: 0.5712 - accuracy: 0.6889 - val_loss: 0.4882 - val_accuracy: 0.7889\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.5629 - accuracy: 0.6974 - val_loss: 0.4796 - val_accuracy: 0.7852\n",
      "(11638, 24, 10)\n",
      "Epoch 1/10\n",
      "310/310 [==============================] - 18s 43ms/step - loss: 0.3548 - accuracy: 0.8895 - val_loss: 0.2735 - val_accuracy: 0.9124\n",
      "Epoch 2/10\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.3035 - accuracy: 0.8973 - val_loss: 0.2696 - val_accuracy: 0.9118\n",
      "Epoch 3/10\n",
      "310/310 [==============================] - 7s 23ms/step - loss: 0.2988 - accuracy: 0.8969 - val_loss: 0.2684 - val_accuracy: 0.9124\n",
      "Epoch 4/10\n",
      "310/310 [==============================] - 7s 23ms/step - loss: 0.2946 - accuracy: 0.8973 - val_loss: 0.2658 - val_accuracy: 0.9112\n",
      "Epoch 5/10\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 0.2938 - accuracy: 0.8973 - val_loss: 0.2649 - val_accuracy: 0.9118\n",
      "Epoch 6/10\n",
      "310/310 [==============================] - 9s 30ms/step - loss: 0.2944 - accuracy: 0.8973 - val_loss: 0.2682 - val_accuracy: 0.9118\n",
      "Epoch 7/10\n",
      "310/310 [==============================] - 14s 46ms/step - loss: 0.2911 - accuracy: 0.8972 - val_loss: 0.2683 - val_accuracy: 0.9124\n",
      "Epoch 8/10\n",
      "309/310 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8970Restoring model weights from the end of the best epoch: 5.\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.2920 - accuracy: 0.8971 - val_loss: 0.2771 - val_accuracy: 0.9112\n",
      "Epoch 8: early stopping\n",
      "(3473, 24, 10)\n",
      "Epoch 1/10\n",
      "93/93 [==============================] - 5s 21ms/step - loss: 0.5094 - accuracy: 0.8232 - val_loss: 0.4043 - val_accuracy: 0.8656\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.4355 - accuracy: 0.8360 - val_loss: 0.3753 - val_accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.4212 - accuracy: 0.8364 - val_loss: 0.3631 - val_accuracy: 0.8656\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.4078 - accuracy: 0.8364 - val_loss: 0.3562 - val_accuracy: 0.8656\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 2s 16ms/step - loss: 0.3955 - accuracy: 0.8364 - val_loss: 0.3523 - val_accuracy: 0.8676\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.3922 - accuracy: 0.8360 - val_loss: 0.3482 - val_accuracy: 0.8676\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.3875 - accuracy: 0.8347 - val_loss: 0.3489 - val_accuracy: 0.8695\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.3843 - accuracy: 0.8371 - val_loss: 0.3482 - val_accuracy: 0.8618\n",
      "Epoch 9/10\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.8342Restoring model weights from the end of the best epoch: 6.\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.3860 - accuracy: 0.8343 - val_loss: 0.3485 - val_accuracy: 0.8599\n",
      "Epoch 9: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "57/57 [==============================] - 1s 4ms/step\n",
      "364/364 [==============================] - 2s 4ms/step\n",
      "109/109 [==============================] - 1s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "14/14 [==============================] - 0s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(422, 24, 10)\n",
      "Accuracy test cluster  0 : 72.0379\n",
      "87/87 [==============================] - 1s 4ms/step\n",
      "----- cluster  1  -----\n",
      "(2753, 24, 10)\n",
      "Accuracy test cluster  1 : 90.8463\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "----- cluster  2  -----\n",
      "(1053, 24, 10)\n",
      "Accuracy test cluster  2 : 86.0399\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 86.7956\n",
      "Accuracy test: 87.772\n",
      "AUC-ROC:  0.7809994117860496\n",
      "AUC-PR:  0.3545150960616736\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(4237, 24, 10)\n",
      "Epoch 1/10\n",
      "113/113 [==============================] - 5s 20ms/step - loss: 0.4818 - accuracy: 0.8095 - val_loss: 0.4489 - val_accuracy: 0.8192\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4521 - accuracy: 0.8106 - val_loss: 0.4302 - val_accuracy: 0.8192\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 2s 17ms/step - loss: 0.4397 - accuracy: 0.8112 - val_loss: 0.4191 - val_accuracy: 0.8208\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4272 - accuracy: 0.8076 - val_loss: 0.4148 - val_accuracy: 0.8208\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4244 - accuracy: 0.8106 - val_loss: 0.4122 - val_accuracy: 0.8208\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4231 - accuracy: 0.8112 - val_loss: 0.4118 - val_accuracy: 0.8223\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4194 - accuracy: 0.8142 - val_loss: 0.4112 - val_accuracy: 0.8223\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4205 - accuracy: 0.8151 - val_loss: 0.4103 - val_accuracy: 0.8192\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4182 - accuracy: 0.8117 - val_loss: 0.4107 - val_accuracy: 0.8176\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.4115 - accuracy: 0.8145 - val_loss: 0.4108 - val_accuracy: 0.8192\n",
      "(10511, 24, 10)\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 8s 18ms/step - loss: 0.3101 - accuracy: 0.9124 - val_loss: 0.2326 - val_accuracy: 0.9309\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.2792 - accuracy: 0.9125 - val_loss: 0.2280 - val_accuracy: 0.9309\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.2719 - accuracy: 0.9125 - val_loss: 0.2251 - val_accuracy: 0.9309\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.2653 - accuracy: 0.9127 - val_loss: 0.2285 - val_accuracy: 0.9315\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.2665 - accuracy: 0.9129 - val_loss: 0.2314 - val_accuracy: 0.9315\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9130Restoring model weights from the end of the best epoch: 3.\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.2607 - accuracy: 0.9130 - val_loss: 0.2357 - val_accuracy: 0.9302\n",
      "Epoch 6: early stopping\n",
      "(2164, 24, 10)\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 24ms/step - loss: 0.5818 - accuracy: 0.7303 - val_loss: 0.5180 - val_accuracy: 0.7877\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.5466 - accuracy: 0.7406 - val_loss: 0.4996 - val_accuracy: 0.7877\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.5319 - accuracy: 0.7412 - val_loss: 0.4891 - val_accuracy: 0.7877\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.5203 - accuracy: 0.7412 - val_loss: 0.4790 - val_accuracy: 0.7877\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.5121 - accuracy: 0.7428 - val_loss: 0.4797 - val_accuracy: 0.7969\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 1s 17ms/step - loss: 0.5051 - accuracy: 0.7466 - val_loss: 0.4686 - val_accuracy: 0.7969\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.5010 - accuracy: 0.7471 - val_loss: 0.4666 - val_accuracy: 0.7969\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.4943 - accuracy: 0.7553 - val_loss: 0.4699 - val_accuracy: 0.7938\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.4906 - accuracy: 0.7499 - val_loss: 0.4668 - val_accuracy: 0.7815\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.4936 - accuracy: 0.7504 - val_loss: 0.4635 - val_accuracy: 0.7877\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 1s 4ms/step\n",
      "329/329 [==============================] - 2s 4ms/step\n",
      "68/68 [==============================] - 1s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "----- cluster  0  -----\n",
      "(1100, 24, 10)\n",
      "Accuracy test cluster  0 : 80.8182\n",
      "82/82 [==============================] - 1s 4ms/step\n",
      "----- cluster  1  -----\n",
      "(2603, 24, 10)\n",
      "Accuracy test cluster  1 : 91.2025\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(524, 24, 10)\n",
      "Accuracy test cluster  2 : 76.5267\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 87.0684\n",
      "Accuracy test: 86.6809\n",
      "AUC-ROC:  0.7775863473128204\n",
      "AUC-PR:  0.3498176749932243\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM\n",
    "aucprs, aucrocs, accuracies = kfoldclustered(\"LSTM\", X, y, weighted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aucpr scores: [0.36626203681705416, 0.38288389666568506, 0.3876778820247252, 0.3545150960616736, 0.3498176749932243]\n",
      "0.3682 mean aucpr with a standard deviation of 0.0150\n",
      "aucroc scores: [0.7704547602843314, 0.7934405329274967, 0.788776753715523, 0.7809994117860496, 0.7775863473128204]\n",
      "0.7823 mean aucroc with a standard deviation of 0.0081\n",
      "accuracy scores: [86.3529, 86.4948, 87.0151, 87.772, 86.6809]\n",
      "86.8631 mean accuracy with a standard deviation of 0.5056\n"
     ]
    }
   ],
   "source": [
    "print ('aucpr scores:', aucprs)\n",
    "print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs), np.std(aucprs)))\n",
    "\n",
    "print ('aucroc scores:', aucrocs)\n",
    "print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs), np.std(aucrocs)))\n",
    "\n",
    "print ('accuracy scores:', accuracies)\n",
    "print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(1461, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "39/39 [==============================] - 4s 30ms/step - loss: 0.8466 - accuracy: 0.2393 - val_loss: 0.9246 - val_accuracy: 0.2091\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.8144 - accuracy: 0.2409 - val_loss: 0.9044 - val_accuracy: 0.2091\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7964 - accuracy: 0.2716 - val_loss: 0.8908 - val_accuracy: 0.2091\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7837 - accuracy: 0.2836 - val_loss: 0.8781 - val_accuracy: 0.2227\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7718 - accuracy: 0.3207 - val_loss: 0.8456 - val_accuracy: 0.2864\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7608 - accuracy: 0.3844 - val_loss: 0.8462 - val_accuracy: 0.3182\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7541 - accuracy: 0.3876 - val_loss: 0.8282 - val_accuracy: 0.3636\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7262 - accuracy: 0.4448 - val_loss: 0.7969 - val_accuracy: 0.4455\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7344 - accuracy: 0.4658 - val_loss: 0.7829 - val_accuracy: 0.4636\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.7185 - accuracy: 0.4803 - val_loss: 0.7777 - val_accuracy: 0.4818\n",
      "(12084, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "321/321 [==============================] - 9s 19ms/step - loss: 0.5756 - accuracy: 0.7803 - val_loss: 0.4893 - val_accuracy: 0.8621\n",
      "Epoch 2/10\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.5352 - accuracy: 0.7985 - val_loss: 0.4903 - val_accuracy: 0.7910\n",
      "Epoch 3/10\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.5231 - accuracy: 0.7616 - val_loss: 0.4769 - val_accuracy: 0.7766\n",
      "Epoch 4/10\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.5164 - accuracy: 0.7702 - val_loss: 0.5219 - val_accuracy: 0.7226\n",
      "Epoch 5/10\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.5136 - accuracy: 0.7689 - val_loss: 0.5120 - val_accuracy: 0.7253\n",
      "Epoch 6/10\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.7691Restoring model weights from the end of the best epoch: 3.\n",
      "321/321 [==============================] - 6s 18ms/step - loss: 0.5127 - accuracy: 0.7691 - val_loss: 0.5505 - val_accuracy: 0.6939\n",
      "Epoch 6: early stopping\n",
      "(3366, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - 5s 23ms/step - loss: 0.8600 - accuracy: 0.3296 - val_loss: 0.9344 - val_accuracy: 0.2020\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 0.8040 - accuracy: 0.2768 - val_loss: 0.9204 - val_accuracy: 0.2416\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 0.7819 - accuracy: 0.3188 - val_loss: 0.8955 - val_accuracy: 0.2871\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 0.7695 - accuracy: 0.3681 - val_loss: 0.8774 - val_accuracy: 0.3426\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.7412 - accuracy: 0.4341 - val_loss: 0.8296 - val_accuracy: 0.4079\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 0.7319 - accuracy: 0.4799 - val_loss: 0.8614 - val_accuracy: 0.4297\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 0.7156 - accuracy: 0.4907 - val_loss: 0.8427 - val_accuracy: 0.4614\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.5159Restoring model weights from the end of the best epoch: 5.\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.7143 - accuracy: 0.5159 - val_loss: 0.8415 - val_accuracy: 0.4812\n",
      "Epoch 8: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "46/46 [==============================] - 1s 5ms/step\n",
      "378/378 [==============================] - 2s 5ms/step\n",
      "106/106 [==============================] - 1s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "11/11 [==============================] - 0s 5ms/step\n",
      "98/98 [==============================] - 1s 5ms/step\n",
      "25/25 [==============================] - 0s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 1. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 70.5103\n",
      "Accuracy test: 69.4418\n",
      "AUC-ROC:  0.769314078569268\n",
      "AUC-PR:  0.3674493533739938\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(3880, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 5s 20ms/step - loss: 0.7865 - accuracy: 0.5494 - val_loss: 0.7768 - val_accuracy: 0.1838\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.7468 - accuracy: 0.3014 - val_loss: 0.7678 - val_accuracy: 0.2388\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.7336 - accuracy: 0.3736 - val_loss: 0.7701 - val_accuracy: 0.3144\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.7094 - accuracy: 0.4415 - val_loss: 0.7033 - val_accuracy: 0.5292\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6896 - accuracy: 0.5249 - val_loss: 0.7128 - val_accuracy: 0.5533\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6816 - accuracy: 0.5625 - val_loss: 0.7128 - val_accuracy: 0.5687\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6696 - accuracy: 0.5794 - val_loss: 0.6797 - val_accuracy: 0.6031\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6600 - accuracy: 0.5573 - val_loss: 0.6712 - val_accuracy: 0.6186\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6625 - accuracy: 0.5943 - val_loss: 0.6884 - val_accuracy: 0.6031\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.6553 - accuracy: 0.5919 - val_loss: 0.6845 - val_accuracy: 0.6048\n",
      "(2732, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "73/73 [==============================] - 4s 22ms/step - loss: 0.8629 - accuracy: 0.3286 - val_loss: 0.9575 - val_accuracy: 0.2171\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.8319 - accuracy: 0.2739 - val_loss: 0.9475 - val_accuracy: 0.2244\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.8012 - accuracy: 0.3316 - val_loss: 0.9063 - val_accuracy: 0.3073\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.7699 - accuracy: 0.4083 - val_loss: 0.8772 - val_accuracy: 0.3780\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.7537 - accuracy: 0.4703 - val_loss: 0.8770 - val_accuracy: 0.4122\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 1s 20ms/step - loss: 0.7433 - accuracy: 0.5108 - val_loss: 0.8692 - val_accuracy: 0.4585\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.7361 - accuracy: 0.5233 - val_loss: 0.8958 - val_accuracy: 0.4439\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.7254 - accuracy: 0.5345 - val_loss: 0.8840 - val_accuracy: 0.4707\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 2s 25ms/step - loss: 0.7319 - accuracy: 0.5177 - val_loss: 0.8458 - val_accuracy: 0.4902\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.7243 - accuracy: 0.5375 - val_loss: 0.8674 - val_accuracy: 0.4805\n",
      "(10299, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "274/274 [==============================] - 8s 20ms/step - loss: 0.5585 - accuracy: 0.8305 - val_loss: 0.4693 - val_accuracy: 0.9146\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 5s 18ms/step - loss: 0.5267 - accuracy: 0.8421 - val_loss: 0.4485 - val_accuracy: 0.8751\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 5s 18ms/step - loss: 0.5113 - accuracy: 0.8160 - val_loss: 0.4530 - val_accuracy: 0.8162\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 5s 17ms/step - loss: 0.5013 - accuracy: 0.8067 - val_loss: 0.4868 - val_accuracy: 0.7638\n",
      "Epoch 5/10\n",
      "273/274 [============================>.] - ETA: 0s - loss: 0.4926 - accuracy: 0.7965Restoring model weights from the end of the best epoch: 2.\n",
      "274/274 [==============================] - 5s 17ms/step - loss: 0.4925 - accuracy: 0.7963 - val_loss: 0.5235 - val_accuracy: 0.7172\n",
      "Epoch 5: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "86/86 [==============================] - 1s 5ms/step\n",
      "322/322 [==============================] - 2s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "33/33 [==============================] - 0s 5ms/step\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "80/80 [==============================] - 1s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 1. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 75.4361\n",
      "Accuracy test: 74.9527\n",
      "AUC-ROC:  0.7883527955617345\n",
      "AUC-PR:  0.376578202239015\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(2922, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 4s 23ms/step - loss: 0.8730 - accuracy: 0.3021 - val_loss: 0.9753 - val_accuracy: 0.2073\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.8297 - accuracy: 0.2735 - val_loss: 0.9468 - val_accuracy: 0.2096\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.8134 - accuracy: 0.2984 - val_loss: 0.9462 - val_accuracy: 0.2301\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.8008 - accuracy: 0.3234 - val_loss: 0.9221 - val_accuracy: 0.2642\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.7867 - accuracy: 0.3669 - val_loss: 0.9082 - val_accuracy: 0.3098\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.7763 - accuracy: 0.3971 - val_loss: 0.8721 - val_accuracy: 0.3827\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.7681 - accuracy: 0.4209 - val_loss: 0.8689 - val_accuracy: 0.4077\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.7576 - accuracy: 0.4511 - val_loss: 0.8777 - val_accuracy: 0.4282\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.7578 - accuracy: 0.4478 - val_loss: 0.8744 - val_accuracy: 0.4419\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.7497 - accuracy: 0.4776 - val_loss: 0.8671 - val_accuracy: 0.4487\n",
      "(5886, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 6s 21ms/step - loss: 0.5791 - accuracy: 0.8397 - val_loss: 0.4965 - val_accuracy: 0.8992\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.5306 - accuracy: 0.8475 - val_loss: 0.4765 - val_accuracy: 0.8550\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.5016 - accuracy: 0.8059 - val_loss: 0.4090 - val_accuracy: 0.8573\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.4801 - accuracy: 0.8121 - val_loss: 0.4097 - val_accuracy: 0.8437\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.4751 - accuracy: 0.8007 - val_loss: 0.3918 - val_accuracy: 0.8471\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.4689 - accuracy: 0.8051 - val_loss: 0.4174 - val_accuracy: 0.8301\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.4609 - accuracy: 0.8001 - val_loss: 0.3923 - val_accuracy: 0.8381\n",
      "Epoch 8/10\n",
      "154/157 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.8042Restoring model weights from the end of the best epoch: 5.\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.4660 - accuracy: 0.8029 - val_loss: 0.4045 - val_accuracy: 0.8211\n",
      "Epoch 8: early stopping\n",
      "(8103, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 7s 20ms/step - loss: 0.6537 - accuracy: 0.5847 - val_loss: 0.6333 - val_accuracy: 0.7582\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.6228 - accuracy: 0.7060 - val_loss: 0.6200 - val_accuracy: 0.7179\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.6047 - accuracy: 0.7291 - val_loss: 0.6358 - val_accuracy: 0.6801\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5916 - accuracy: 0.7254 - val_loss: 0.6813 - val_accuracy: 0.6217\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5924 - accuracy: 0.7164 - val_loss: 0.6181 - val_accuracy: 0.6826\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5822 - accuracy: 0.7349 - val_loss: 0.6772 - val_accuracy: 0.6209\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5834 - accuracy: 0.7113 - val_loss: 0.6052 - val_accuracy: 0.6883\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5809 - accuracy: 0.7293 - val_loss: 0.6313 - val_accuracy: 0.6637\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5858 - accuracy: 0.7185 - val_loss: 0.6047 - val_accuracy: 0.6867\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5837 - accuracy: 0.7330 - val_loss: 0.6675 - val_accuracy: 0.6266\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "92/92 [==============================] - 1s 5ms/step\n",
      "184/184 [==============================] - 1s 5ms/step\n",
      "254/254 [==============================] - 2s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "30/30 [==============================] - 0s 5ms/step\n",
      "37/37 [==============================] - 1s 5ms/step\n",
      "66/66 [==============================] - 1s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 1. 1. ... 0. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 1. ... 1. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 70.9775\n",
      "Accuracy test: 71.1684\n",
      "AUC-ROC:  0.7972137093928165\n",
      "AUC-PR:  0.37975814067858527\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(1800, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 4s 25ms/step - loss: 0.9527 - accuracy: 0.3314 - val_loss: 0.9858 - val_accuracy: 0.2296\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8798 - accuracy: 0.3170 - val_loss: 1.1019 - val_accuracy: 0.2296\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8654 - accuracy: 0.3163 - val_loss: 1.1097 - val_accuracy: 0.2296\n",
      "Epoch 4/10\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.8533 - accuracy: 0.3118Restoring model weights from the end of the best epoch: 1.\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8553 - accuracy: 0.3176 - val_loss: 1.0687 - val_accuracy: 0.2296\n",
      "Epoch 4: early stopping\n",
      "(11638, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "310/310 [==============================] - 8s 17ms/step - loss: 0.5905 - accuracy: 0.8285 - val_loss: 0.5472 - val_accuracy: 0.8121\n",
      "Epoch 2/10\n",
      "310/310 [==============================] - 5s 15ms/step - loss: 0.5522 - accuracy: 0.7836 - val_loss: 0.5179 - val_accuracy: 0.7554\n",
      "Epoch 3/10\n",
      "310/310 [==============================] - 5s 15ms/step - loss: 0.5353 - accuracy: 0.7624 - val_loss: 0.5190 - val_accuracy: 0.7245\n",
      "Epoch 4/10\n",
      "310/310 [==============================] - 5s 16ms/step - loss: 0.5305 - accuracy: 0.7522 - val_loss: 0.4967 - val_accuracy: 0.7371\n",
      "Epoch 5/10\n",
      "310/310 [==============================] - 5s 15ms/step - loss: 0.5264 - accuracy: 0.7512 - val_loss: 0.5682 - val_accuracy: 0.6672\n",
      "Epoch 6/10\n",
      "310/310 [==============================] - 5s 15ms/step - loss: 0.5243 - accuracy: 0.7478 - val_loss: 0.5437 - val_accuracy: 0.6844\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/310 [============================>.] - ETA: 0s - loss: 0.5239 - accuracy: 0.7539Restoring model weights from the end of the best epoch: 4.\n",
      "310/310 [==============================] - 5s 15ms/step - loss: 0.5229 - accuracy: 0.7543 - val_loss: 0.5612 - val_accuracy: 0.6695\n",
      "Epoch 7: early stopping\n",
      "(3473, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "93/93 [==============================] - 4s 20ms/step - loss: 0.7393 - accuracy: 0.4624 - val_loss: 0.7603 - val_accuracy: 0.2956\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.7102 - accuracy: 0.4387 - val_loss: 0.7628 - val_accuracy: 0.3762\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.6882 - accuracy: 0.5051 - val_loss: 0.7219 - val_accuracy: 0.4971\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.6615 - accuracy: 0.5752 - val_loss: 0.7253 - val_accuracy: 0.5125\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 1s 16ms/step - loss: 0.6432 - accuracy: 0.5969 - val_loss: 0.6774 - val_accuracy: 0.5931\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.6403 - accuracy: 0.6084 - val_loss: 0.6715 - val_accuracy: 0.6219\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.6348 - accuracy: 0.6050 - val_loss: 0.6554 - val_accuracy: 0.6257\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.6369 - accuracy: 0.6260 - val_loss: 0.6955 - val_accuracy: 0.6065\n",
      "Epoch 9/10\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.6262 - accuracy: 0.6389 - val_loss: 0.6815 - val_accuracy: 0.6200\n",
      "Epoch 10/10\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.6241 - accuracy: 0.6369Restoring model weights from the end of the best epoch: 7.\n",
      "93/93 [==============================] - 1s 15ms/step - loss: 0.6239 - accuracy: 0.6365 - val_loss: 0.7033 - val_accuracy: 0.6008\n",
      "Epoch 10: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "57/57 [==============================] - 1s 4ms/step\n",
      "364/364 [==============================] - 2s 4ms/step\n",
      "109/109 [==============================] - 1s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "14/14 [==============================] - 0s 4ms/step\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "33/33 [==============================] - 0s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 69.919\n",
      "Accuracy test: 71.334\n",
      "AUC-ROC:  0.7728569158067039\n",
      "AUC-PR:  0.3092122944054935\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(4237, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "113/113 [==============================] - 5s 19ms/step - loss: 0.8427 - accuracy: 0.5315 - val_loss: 0.7855 - val_accuracy: 0.2107\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.7659 - accuracy: 0.2913 - val_loss: 0.8014 - val_accuracy: 0.2217\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.7434 - accuracy: 0.3493 - val_loss: 0.7746 - val_accuracy: 0.4057\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.7191 - accuracy: 0.4685 - val_loss: 0.7764 - val_accuracy: 0.4654\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.6975 - accuracy: 0.5232 - val_loss: 0.7473 - val_accuracy: 0.5425\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.6852 - accuracy: 0.5604 - val_loss: 0.7328 - val_accuracy: 0.5660\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.6820 - accuracy: 0.5723 - val_loss: 0.7340 - val_accuracy: 0.5708\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.6740 - accuracy: 0.5732 - val_loss: 0.7195 - val_accuracy: 0.5818\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.6722 - accuracy: 0.5768 - val_loss: 0.6947 - val_accuracy: 0.5896\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 2s 16ms/step - loss: 0.6631 - accuracy: 0.5932 - val_loss: 0.7337 - val_accuracy: 0.5629\n",
      "(10511, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 7s 17ms/step - loss: 0.5516 - accuracy: 0.8758 - val_loss: 0.5140 - val_accuracy: 0.8846\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.5189 - accuracy: 0.8394 - val_loss: 0.4777 - val_accuracy: 0.8244\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.5035 - accuracy: 0.8142 - val_loss: 0.4718 - val_accuracy: 0.7876\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.4910 - accuracy: 0.7982 - val_loss: 0.4777 - val_accuracy: 0.7590\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.4870 - accuracy: 0.8003 - val_loss: 0.5096 - val_accuracy: 0.7210\n",
      "Epoch 6/10\n",
      "278/280 [============================>.] - ETA: 0s - loss: 0.4808 - accuracy: 0.7931Restoring model weights from the end of the best epoch: 3.\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.4805 - accuracy: 0.7929 - val_loss: 0.4884 - val_accuracy: 0.7368\n",
      "Epoch 6: early stopping\n",
      "(2164, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 23ms/step - loss: 0.8716 - accuracy: 0.3377 - val_loss: 0.9385 - val_accuracy: 0.2215\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.8097 - accuracy: 0.3197 - val_loss: 0.9425 - val_accuracy: 0.2554\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7941 - accuracy: 0.3904 - val_loss: 0.9514 - val_accuracy: 0.2923\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7765 - accuracy: 0.4133 - val_loss: 0.9227 - val_accuracy: 0.3692\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7611 - accuracy: 0.4432 - val_loss: 0.9350 - val_accuracy: 0.3877\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7615 - accuracy: 0.4628 - val_loss: 0.9141 - val_accuracy: 0.4308\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7445 - accuracy: 0.4812 - val_loss: 0.9084 - val_accuracy: 0.4462\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7440 - accuracy: 0.5144 - val_loss: 0.9027 - val_accuracy: 0.4677\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7340 - accuracy: 0.5258 - val_loss: 0.9303 - val_accuracy: 0.4554\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.7296 - accuracy: 0.5128 - val_loss: 0.9040 - val_accuracy: 0.4892\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "133/133 [==============================] - 1s 4ms/step\n",
      "329/329 [==============================] - 2s 4ms/step\n",
      "68/68 [==============================] - 1s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "82/82 [==============================] - 1s 4ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 1. ... 1. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 0. 1. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 71.671\n",
      "Accuracy test: 70.7121\n",
      "AUC-ROC:  0.7749220489113995\n",
      "AUC-PR:  0.34682130883568246\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM_w\n",
    "aucprs_w, aucrocs_w, accuracies_w = kfoldclustered(\"LSTM\", X, y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aucpr scores: [0.3674493533739938, 0.376578202239015, 0.37975814067858527, 0.3092122944054935, 0.34682130883568246]\n",
      "0.3560 mean aucpr with a standard deviation of 0.0260\n",
      "aucroc scores: [0.769314078569268, 0.7883527955617345, 0.7972137093928165, 0.7728569158067039, 0.7749220489113995]\n",
      "0.7805 mean aucroc with a standard deviation of 0.0105\n",
      "accuracy scores: [69.4418, 74.9527, 71.1684, 71.334, 70.7121]\n",
      "71.5218 mean accuracy with a standard deviation of 1.8392\n"
     ]
    }
   ],
   "source": [
    "print ('aucpr scores:', aucprs_w)\n",
    "print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs_w), np.std(aucprs_w)))\n",
    "\n",
    "print ('aucroc scores:', aucrocs_w)\n",
    "print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs_w), np.std(aucrocs_w)))\n",
    "\n",
    "print ('accuracy scores:', accuracies_w)\n",
    "print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies_w), np.std(accuracies_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def kfoldclusteredkmeans(classifier, X, y, weighted=False):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f = 0;\n",
    "    aucprs = []\n",
    "    aucrocs = []\n",
    "    accuracies = []\n",
    "    for train, test in cv.split(X, y): #train and test are indexes\n",
    "        f += 1\n",
    "        print('KFold ',f,' ---')\n",
    "        train_input = X[train]\n",
    "        test_input = X[test]\n",
    "        y_true = y[train]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        print(\"---- Clustering Train ---- \")\n",
    "        # Apply KMeans\n",
    "        n_clusters = 3\n",
    "        kmeans = KMeans(\n",
    "            init=\"random\",\n",
    "            n_clusters=n_clusters,\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=42\n",
    "        )\n",
    "        # Fit to the training data\n",
    "        kmeans.fit(train_input.reshape(y_true.shape[0], 24*10))\n",
    "         # Generate out clusters\n",
    "        train_cluster = kmeans.predict(train_input.reshape(y_true.shape[0], 24*10))\n",
    "        # Add predicted cluster and y regression label to our training DataFrame\n",
    "        train_df = list(zip(train_cluster, y_true, train_input))\n",
    "        ls = sorted(train_df, key=lambda t: t[0])\n",
    "        # Unzip sorted data\n",
    "        cluster, y_, data = zip(*ls)\n",
    "        data = np.array(data)\n",
    "        y_ = np.array(y_)\n",
    "        # Getting indexes of clusters division\n",
    "        c = 0\n",
    "        ind = []\n",
    "        for i in range(len(ls)):\n",
    "            if ls[i][0] > c:\n",
    "                c = ls[i][0]\n",
    "                ind.append(i)\n",
    "        # Removing clusters with less than 10 samples\n",
    "        d1 = ind[0]\n",
    "        d2 = ind[1] - ind[0]\n",
    "        d3 = y_true.shape[0] - ind[1]\n",
    "        cluster_centers_ = kmeans.cluster_centers_\n",
    "        if d1 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 1\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[:ind[0]] = c\n",
    "            ind = ind[1:]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 0, 0)\n",
    "        elif d2 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[2])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 2\n",
    "            train_cluster[ind[0]:ind[1]] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 1, 0)\n",
    "        elif d3 < 10:\n",
    "            dist1 = np.linalg.norm(kmeans.cluster_centers_[0])\n",
    "            dist2 = np.linalg.norm(kmeans.cluster_centers_[1])\n",
    "            if dist1 < dist2:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 1\n",
    "            train_cluster[ind[1]:] = c\n",
    "            ind = ind[:1]\n",
    "            cluster_centers_ = np.delete(kmeans.cluster_centers_, 2, 0)\n",
    "            \n",
    "        print(\"-------- LSTM fitting for each cluster --------\")\n",
    "        i=0\n",
    "        if weighted:\n",
    "            classWeight = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                classes=np.unique(np.ravel(y_true)),\n",
    "                                                y=np.ravel(y_true))\n",
    "            classWeight = {i : classWeight[i] for i in range(2)}\n",
    "        for index in range(len(ind)+1):\n",
    "            if index == 0:\n",
    "                cluster_X = data[:ind[index],:,:]\n",
    "                cluster_Y = y_[:ind[index]]\n",
    "            elif index == len(ind):\n",
    "                cluster_X = data[ind[index-1]:,:,:]\n",
    "                cluster_Y = y_[ind[index-1]:]\n",
    "            else:\n",
    "                cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "                cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "            print(cluster_X.shape)\n",
    "            \n",
    "            # LSTM\n",
    "            model = Sequential()\n",
    "            model.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "            \n",
    "            # LSTM\n",
    "            if weighted == False:\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_kmeans{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "            # LSTM_W\n",
    "            else:\n",
    "                print(\"PASSOU\")\n",
    "                earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3, restore_best_weights=True)\n",
    "                saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_kmeans{}.h5'.format(f,i))\n",
    "                history = model.fit(cluster_X, cluster_Y, epochs=10, verbose=1, validation_split=0.15, class_weight=classWeight, callbacks=[earlyStop])\n",
    "                model.save(saved_model_path)\n",
    "                i += 1\n",
    "                \n",
    "        testkfoldkmeans(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted)\n",
    "    return aucprs, aucrocs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def testkfoldkmeans(f, cluster_centers_, train_cluster, y_, data, ind, y_test, test_input, aucprs, aucrocs, accuracies, weighted):\n",
    "    # Create initial test data to store assigned clusters\n",
    "    test_df = list(zip(train_cluster, y_test, test_input))\n",
    "    # Test sample es asignado al cluster correspondiente mediante Distancia euclidiana y se aplica el modelo correspondiente\n",
    "    print(\"Assigning each test sample to the closest cluster centroid...\")\n",
    "    new_cluster = [0 for i in range(y_test.shape[0])]\n",
    "    for row in range(len(test_df)):\n",
    "        min_distance = float('inf')\n",
    "        closest_cluster = None\n",
    "        for k in range(cluster_centers_.shape[0]):\n",
    "            # Check if the assigned cluster has more than 100 samples\n",
    "            # if train_clusters_df[k].shape[0] > 100: # Probar sin limite\n",
    "            distance = np.linalg.norm(cluster_centers_[k]-test_df[row][2].reshape(240))\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_cluster = k\n",
    "        # Assign cluster to test sample\n",
    "        new_cluster[row] = closest_cluster\n",
    "    # Sort test data\n",
    "    test_df = list(zip(new_cluster, y_test, test_input))\n",
    "    ls_test = sorted(test_df, key=lambda t: t[0])\n",
    "    # Unzip sorted data\n",
    "    cluster_t, y_t, data_t = zip(*ls_test)\n",
    "    data_t = np.array(data_t)\n",
    "    y_t = np.array(y_t)\n",
    "    # Getting indexes\n",
    "    c = 0\n",
    "    ind_t = []\n",
    "    for i in range(len(ls_test)):\n",
    "        if ls_test[i][0] > c:\n",
    "            c = ls_test[i][0]\n",
    "            ind_t.append(i)\n",
    "    print(\"-------- Train metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data[:ind[index],:,:]\n",
    "            cluster_Y = y_[:ind[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data[ind[index-1]:,:,:]\n",
    "            cluster_Y = y_[ind[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data[ind[index-1]:ind[index],:,:]\n",
    "            cluster_Y = y_[ind[index-1]:ind[index]]\n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_kmeans{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_kmeans{}.h5'.format(f,i))\n",
    "        \n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            train_X_probs = np.array(classes_prob)\n",
    "            y_train = np.array(cluster_Y)\n",
    "        else: \n",
    "            train_X_probs = np.concatenate((train_X_probs, classes_prob))\n",
    "            y_train = np.concatenate((y_train, cluster_Y))\n",
    "        i += 1\n",
    "\n",
    "    # Test metrics\n",
    "    print(\"-------- Test metrics ---------\")\n",
    "    i = 0\n",
    "    # For each cluster, predict probabilities of class labels\n",
    "    for index in range(len(ind_t)+1):\n",
    "        if index == 0:\n",
    "            cluster_X = data_t[:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[:ind_t[index]]\n",
    "        elif index == len(ind):\n",
    "            cluster_X = data_t[ind_t[index-1]:,:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:]\n",
    "        else:\n",
    "            cluster_X = data_t[ind_t[index-1]:ind_t[index],:,:]\n",
    "            cluster_Y = y_t[ind_t[index-1]:ind_t[index]]\n",
    "            \n",
    "        if weighted:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_w_kmeans{}.h5'.format(f,i))\n",
    "        else:\n",
    "            saved_model_path = os.path.join(predictions_output_dir, 'cv{}_lstm_kmeans{}.h5'.format(f,i))\n",
    "        model = load_model(saved_model_path)\n",
    "        classes_prob = model.predict(cluster_X, verbose=1)\n",
    "        if i == 0:\n",
    "            test_X_probs = np.array(classes_prob)\n",
    "            test_y = np.array(cluster_Y)\n",
    "        else: \n",
    "            test_X_probs = np.concatenate((test_X_probs, classes_prob))\n",
    "            test_y = np.concatenate((test_y, cluster_Y))\n",
    "        print('----- cluster ',i,' -----')\n",
    "        print(cluster_X.shape)\n",
    "        # Test metrics\n",
    "        y_pred1 = (classes_prob.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "        test_ac1=np.round(metrics.accuracy_score(cluster_Y, y_pred1)*100,4)\n",
    "        print(\"Accuracy test cluster \",i,\":\", test_ac1)\n",
    "        \n",
    "        i += 1\n",
    "    print(\"------ Metrics ------\")\n",
    "    # Train metrics\n",
    "    y_pred_train = (train_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred_train)\n",
    "    print('true labels: ', y_train)\n",
    "    # Test metrics\n",
    "    y_pred = (test_X_probs.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "    print('labels(0 short stay, 1 long stay) predicted: ', y_pred)\n",
    "    print('true labels: ', test_y)\n",
    "    #\n",
    "    train_ac=np.round(metrics.accuracy_score(y_train, y_pred_train)*100,4)\n",
    "    print(\"Accuracy train:\", train_ac)\n",
    "    test_ac=np.round(metrics.accuracy_score(test_y, y_pred)*100,4)\n",
    "    print(\"Accuracy test:\", test_ac)\n",
    "    auroc = metrics.roc_auc_score(test_y, test_X_probs)\n",
    "    print(\"AUC-ROC: \", auroc)\n",
    "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(test_y, test_X_probs)\n",
    "    auprc = metrics.auc(recalls, precisions)\n",
    "    print(\"AUC-PR: \", auprc)\n",
    "    aucprs.append(auprc)\n",
    "    aucrocs.append(auroc)\n",
    "    accuracies.append(test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(481, 24, 10)\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 4s 94ms/step - loss: 0.6662 - accuracy: 0.5711 - val_loss: 0.6192 - val_accuracy: 0.7260\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 1s 53ms/step - loss: 0.6286 - accuracy: 0.7010 - val_loss: 0.5881 - val_accuracy: 0.7260\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.5969 - accuracy: 0.7426 - val_loss: 0.5754 - val_accuracy: 0.7260\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5948 - accuracy: 0.7451 - val_loss: 0.5700 - val_accuracy: 0.7260\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5813 - accuracy: 0.7475 - val_loss: 0.5676 - val_accuracy: 0.7260\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5779 - accuracy: 0.7451 - val_loss: 0.5654 - val_accuracy: 0.7260\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.5600 - accuracy: 0.7549 - val_loss: 0.5638 - val_accuracy: 0.7260\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5639 - accuracy: 0.7549 - val_loss: 0.5616 - val_accuracy: 0.7260\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5476 - accuracy: 0.7525 - val_loss: 0.5600 - val_accuracy: 0.7260\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5563 - accuracy: 0.7500 - val_loss: 0.5588 - val_accuracy: 0.7260\n",
      "(5420, 24, 10)\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 8s 30ms/step - loss: 0.7143 - accuracy: 0.5400 - val_loss: 0.5047 - val_accuracy: 0.8155\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 4s 25ms/step - loss: 0.5241 - accuracy: 0.7795 - val_loss: 0.4575 - val_accuracy: 0.8167\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 4s 27ms/step - loss: 0.4949 - accuracy: 0.7814 - val_loss: 0.4369 - val_accuracy: 0.8180\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 3s 23ms/step - loss: 0.4769 - accuracy: 0.7834 - val_loss: 0.4270 - val_accuracy: 0.8192\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 3s 22ms/step - loss: 0.4624 - accuracy: 0.7862 - val_loss: 0.4213 - val_accuracy: 0.8192\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 3s 23ms/step - loss: 0.4562 - accuracy: 0.7866 - val_loss: 0.4170 - val_accuracy: 0.8204\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 4s 27ms/step - loss: 0.4531 - accuracy: 0.7860 - val_loss: 0.4161 - val_accuracy: 0.8216\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 3s 24ms/step - loss: 0.4531 - accuracy: 0.7855 - val_loss: 0.4168 - val_accuracy: 0.8266\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 4s 25ms/step - loss: 0.4507 - accuracy: 0.7832 - val_loss: 0.4211 - val_accuracy: 0.8216\n",
      "Epoch 10/10\n",
      "142/144 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7861Restoring model weights from the end of the best epoch: 7.\n",
      "144/144 [==============================] - 4s 27ms/step - loss: 0.4485 - accuracy: 0.7851 - val_loss: 0.4173 - val_accuracy: 0.8266\n",
      "Epoch 10: early stopping\n",
      "(11010, 24, 10)\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 13s 36ms/step - loss: 0.3367 - accuracy: 0.9045 - val_loss: 0.2514 - val_accuracy: 0.9231\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 8s 27ms/step - loss: 0.2805 - accuracy: 0.9126 - val_loss: 0.2443 - val_accuracy: 0.9231\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 5s 17ms/step - loss: 0.2736 - accuracy: 0.9125 - val_loss: 0.2416 - val_accuracy: 0.9231\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 5s 17ms/step - loss: 0.2710 - accuracy: 0.9127 - val_loss: 0.2415 - val_accuracy: 0.9231\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.2693 - accuracy: 0.9126 - val_loss: 0.2423 - val_accuracy: 0.9231\n",
      "Epoch 6/10\n",
      "293/293 [==============================] - 5s 18ms/step - loss: 0.2647 - accuracy: 0.9129 - val_loss: 0.2438 - val_accuracy: 0.9231\n",
      "Epoch 7/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9128Restoring model weights from the end of the best epoch: 4.\n",
      "293/293 [==============================] - 7s 24ms/step - loss: 0.2676 - accuracy: 0.9128 - val_loss: 0.2479 - val_accuracy: 0.9225\n",
      "Epoch 7: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "16/16 [==============================] - 1s 12ms/step\n",
      "170/170 [==============================] - 2s 8ms/step\n",
      "345/345 [==============================] - 3s 8ms/step\n",
      "-------- Test metrics ---------\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(108, 24, 10)\n",
      "Accuracy test cluster  0 : 68.5185\n",
      "43/43 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(1366, 24, 10)\n",
      "Accuracy test cluster  1 : 77.3792\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(2754, 24, 10)\n",
      "Accuracy test cluster  2 : 91.5033\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 87.1149\n",
      "Accuracy test: 86.3529\n",
      "AUC-ROC:  0.7735674720205687\n",
      "AUC-PR:  0.3671300924689447\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(10934, 24, 10)\n",
      "Epoch 1/10\n",
      "291/291 [==============================] - 10s 24ms/step - loss: 0.3808 - accuracy: 0.8698 - val_loss: 0.2455 - val_accuracy: 0.9305\n",
      "Epoch 2/10\n",
      "291/291 [==============================] - 7s 26ms/step - loss: 0.2855 - accuracy: 0.9126 - val_loss: 0.2347 - val_accuracy: 0.9305\n",
      "Epoch 3/10\n",
      "291/291 [==============================] - 8s 27ms/step - loss: 0.2766 - accuracy: 0.9126 - val_loss: 0.2289 - val_accuracy: 0.9305\n",
      "Epoch 4/10\n",
      "291/291 [==============================] - 6s 21ms/step - loss: 0.2734 - accuracy: 0.9127 - val_loss: 0.2294 - val_accuracy: 0.9305\n",
      "Epoch 5/10\n",
      "291/291 [==============================] - 6s 19ms/step - loss: 0.2698 - accuracy: 0.9128 - val_loss: 0.2302 - val_accuracy: 0.9305\n",
      "Epoch 6/10\n",
      "289/291 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.9133Restoring model weights from the end of the best epoch: 3.\n",
      "291/291 [==============================] - 6s 20ms/step - loss: 0.2690 - accuracy: 0.9132 - val_loss: 0.2323 - val_accuracy: 0.9305\n",
      "Epoch 6: early stopping\n",
      "(5507, 24, 10)\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 7s 22ms/step - loss: 0.5837 - accuracy: 0.7284 - val_loss: 0.4876 - val_accuracy: 0.8065\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.5051 - accuracy: 0.7842 - val_loss: 0.4563 - val_accuracy: 0.8065\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.4787 - accuracy: 0.7853 - val_loss: 0.4394 - val_accuracy: 0.8089\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 3s 17ms/step - loss: 0.4598 - accuracy: 0.7885 - val_loss: 0.4321 - val_accuracy: 0.8150\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.4554 - accuracy: 0.7853 - val_loss: 0.4291 - val_accuracy: 0.8077\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.4532 - accuracy: 0.7863 - val_loss: 0.4266 - val_accuracy: 0.8077\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 3s 21ms/step - loss: 0.4468 - accuracy: 0.7887 - val_loss: 0.4290 - val_accuracy: 0.8077\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 4s 27ms/step - loss: 0.4455 - accuracy: 0.7885 - val_loss: 0.4281 - val_accuracy: 0.8102\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.7906Restoring model weights from the end of the best epoch: 6.\n",
      "147/147 [==============================] - 3s 22ms/step - loss: 0.4447 - accuracy: 0.7906 - val_loss: 0.4274 - val_accuracy: 0.8089\n",
      "Epoch 9: early stopping\n",
      "(470, 24, 10)\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 4s 63ms/step - loss: 0.6961 - accuracy: 0.5363 - val_loss: 0.6327 - val_accuracy: 0.7324\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.6584 - accuracy: 0.6366 - val_loss: 0.5943 - val_accuracy: 0.7465\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.6204 - accuracy: 0.6892 - val_loss: 0.5733 - val_accuracy: 0.7324\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6062 - accuracy: 0.7268 - val_loss: 0.5631 - val_accuracy: 0.7324\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.6029 - accuracy: 0.7218 - val_loss: 0.5583 - val_accuracy: 0.7324\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5818 - accuracy: 0.7268 - val_loss: 0.5546 - val_accuracy: 0.7324\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5900 - accuracy: 0.7243 - val_loss: 0.5525 - val_accuracy: 0.7324\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5910 - accuracy: 0.7343 - val_loss: 0.5512 - val_accuracy: 0.7324\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5989 - accuracy: 0.7293 - val_loss: 0.5508 - val_accuracy: 0.7324\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5960 - accuracy: 0.7243 - val_loss: 0.5504 - val_accuracy: 0.7324\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "342/342 [==============================] - 2s 5ms/step\n",
      "173/173 [==============================] - 1s 5ms/step\n",
      "15/15 [==============================] - 0s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(2766, 24, 10)\n",
      "Accuracy test cluster  0 : 90.9617\n",
      "43/43 [==============================] - 1s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(1349, 24, 10)\n",
      "Accuracy test cluster  1 : 78.6509\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(113, 24, 10)\n",
      "Accuracy test cluster  2 : 76.1062\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 87.0321\n",
      "Accuracy test: 86.6367\n",
      "AUC-ROC:  0.7918580945133872\n",
      "AUC-PR:  0.37740440173099943\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(5337, 24, 10)\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 6s 20ms/step - loss: 0.5607 - accuracy: 0.7438 - val_loss: 0.4563 - val_accuracy: 0.8102\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.4980 - accuracy: 0.7760 - val_loss: 0.4346 - val_accuracy: 0.8115\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 4s 25ms/step - loss: 0.4818 - accuracy: 0.7824 - val_loss: 0.4246 - val_accuracy: 0.8127\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 4s 26ms/step - loss: 0.4723 - accuracy: 0.7760 - val_loss: 0.4222 - val_accuracy: 0.8115\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.4672 - accuracy: 0.7765 - val_loss: 0.4194 - val_accuracy: 0.8090\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 4s 27ms/step - loss: 0.4631 - accuracy: 0.7769 - val_loss: 0.4177 - val_accuracy: 0.8102\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 3s 24ms/step - loss: 0.4605 - accuracy: 0.7828 - val_loss: 0.4154 - val_accuracy: 0.8102\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.4604 - accuracy: 0.7800 - val_loss: 0.4154 - val_accuracy: 0.8102\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.4598 - accuracy: 0.7840 - val_loss: 0.4186 - val_accuracy: 0.8127\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.7793Restoring model weights from the end of the best epoch: 7.\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.4575 - accuracy: 0.7793 - val_loss: 0.4155 - val_accuracy: 0.8077\n",
      "Epoch 10: early stopping\n",
      "(11096, 24, 10)\n",
      "Epoch 1/10\n",
      "295/295 [==============================] - 9s 22ms/step - loss: 0.3495 - accuracy: 0.8925 - val_loss: 0.2546 - val_accuracy: 0.9219\n",
      "Epoch 2/10\n",
      "295/295 [==============================] - 5s 17ms/step - loss: 0.2823 - accuracy: 0.9117 - val_loss: 0.2473 - val_accuracy: 0.9219\n",
      "Epoch 3/10\n",
      "295/295 [==============================] - 5s 17ms/step - loss: 0.2781 - accuracy: 0.9117 - val_loss: 0.2434 - val_accuracy: 0.9219\n",
      "Epoch 4/10\n",
      "295/295 [==============================] - 5s 17ms/step - loss: 0.2743 - accuracy: 0.9117 - val_loss: 0.2471 - val_accuracy: 0.9219\n",
      "Epoch 5/10\n",
      "295/295 [==============================] - 6s 20ms/step - loss: 0.2728 - accuracy: 0.9119 - val_loss: 0.2447 - val_accuracy: 0.9219\n",
      "Epoch 6/10\n",
      "295/295 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9121Restoring model weights from the end of the best epoch: 3.\n",
      "295/295 [==============================] - 7s 25ms/step - loss: 0.2706 - accuracy: 0.9121 - val_loss: 0.2494 - val_accuracy: 0.9219\n",
      "Epoch 6: early stopping\n",
      "(478, 24, 10)\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 4s 61ms/step - loss: 0.7759 - accuracy: 0.3202 - val_loss: 0.7475 - val_accuracy: 0.2361\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.7106 - accuracy: 0.4901 - val_loss: 0.6792 - val_accuracy: 0.6250\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.6707 - accuracy: 0.5985 - val_loss: 0.6263 - val_accuracy: 0.7361\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6387 - accuracy: 0.6601 - val_loss: 0.5851 - val_accuracy: 0.7778\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.6122 - accuracy: 0.7167 - val_loss: 0.5580 - val_accuracy: 0.7917\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5965 - accuracy: 0.7340 - val_loss: 0.5406 - val_accuracy: 0.7917\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.5863 - accuracy: 0.7414 - val_loss: 0.5279 - val_accuracy: 0.7917\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5857 - accuracy: 0.7266 - val_loss: 0.5197 - val_accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.5800 - accuracy: 0.7389 - val_loss: 0.5158 - val_accuracy: 0.7917\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5772 - accuracy: 0.7315 - val_loss: 0.5129 - val_accuracy: 0.7917\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "167/167 [==============================] - 1s 5ms/step\n",
      "347/347 [==============================] - 3s 6ms/step\n",
      "15/15 [==============================] - 0s 4ms/step\n",
      "-------- Test metrics ---------\n",
      "44/44 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(1404, 24, 10)\n",
      "Accuracy test cluster  0 : 79.9145\n",
      "86/86 [==============================] - 1s 4ms/step\n",
      "----- cluster  1  -----\n",
      "(2729, 24, 10)\n",
      "Accuracy test cluster  1 : 91.7919\n",
      "3/3 [==============================] - 1s 8ms/step\n",
      "----- cluster  2  -----\n",
      "(95, 24, 10)\n",
      "Accuracy test cluster  2 : 68.4211\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Accuracy train: 86.8488\n",
      "Accuracy test: 87.3226\n",
      "AUC-ROC:  0.7939284619602799\n",
      "AUC-PR:  0.37830638105328607\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(10996, 24, 10)\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 12s 28ms/step - loss: 0.3177 - accuracy: 0.9093 - val_loss: 0.2433 - val_accuracy: 0.9297\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 7s 23ms/step - loss: 0.2853 - accuracy: 0.9107 - val_loss: 0.2355 - val_accuracy: 0.9297\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 7s 25ms/step - loss: 0.2758 - accuracy: 0.9110 - val_loss: 0.2358 - val_accuracy: 0.9297\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 5s 17ms/step - loss: 0.2737 - accuracy: 0.9109 - val_loss: 0.2388 - val_accuracy: 0.9291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9107Restoring model weights from the end of the best epoch: 2.\n",
      "293/293 [==============================] - 5s 17ms/step - loss: 0.2749 - accuracy: 0.9108 - val_loss: 0.2406 - val_accuracy: 0.9303\n",
      "Epoch 5: early stopping\n",
      "(409, 24, 10)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 5s 75ms/step - loss: 1.1043 - accuracy: 0.2767 - val_loss: 1.0850 - val_accuracy: 0.2258\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9875 - accuracy: 0.2767 - val_loss: 0.9763 - val_accuracy: 0.2258\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 48ms/step - loss: 0.9002 - accuracy: 0.2795 - val_loss: 0.8859 - val_accuracy: 0.2258\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 0.8248 - accuracy: 0.2997 - val_loss: 0.8103 - val_accuracy: 0.2258\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.7561 - accuracy: 0.3689 - val_loss: 0.7469 - val_accuracy: 0.2581\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6942 - accuracy: 0.5072 - val_loss: 0.6977 - val_accuracy: 0.5323\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.6723 - accuracy: 0.5764 - val_loss: 0.6574 - val_accuracy: 0.7258\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.6516 - accuracy: 0.6455 - val_loss: 0.6260 - val_accuracy: 0.7742\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6218 - accuracy: 0.6974 - val_loss: 0.6008 - val_accuracy: 0.7742\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.6068 - accuracy: 0.7176 - val_loss: 0.5838 - val_accuracy: 0.7742\n",
      "(5506, 24, 10)\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 9s 29ms/step - loss: 0.6057 - accuracy: 0.6840 - val_loss: 0.4861 - val_accuracy: 0.8111\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 3s 21ms/step - loss: 0.5113 - accuracy: 0.7750 - val_loss: 0.4543 - val_accuracy: 0.8123\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 3s 23ms/step - loss: 0.4814 - accuracy: 0.7776 - val_loss: 0.4354 - val_accuracy: 0.8136\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.4694 - accuracy: 0.7752 - val_loss: 0.4279 - val_accuracy: 0.8160\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.4613 - accuracy: 0.7765 - val_loss: 0.4246 - val_accuracy: 0.8136\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.4598 - accuracy: 0.7752 - val_loss: 0.4262 - val_accuracy: 0.8148\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 5s 31ms/step - loss: 0.4564 - accuracy: 0.7784 - val_loss: 0.4203 - val_accuracy: 0.8099\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 5s 34ms/step - loss: 0.4555 - accuracy: 0.7786 - val_loss: 0.4208 - val_accuracy: 0.8136\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - 5s 33ms/step - loss: 0.4538 - accuracy: 0.7810 - val_loss: 0.4217 - val_accuracy: 0.8099\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.7821Restoring model weights from the end of the best epoch: 7.\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.4503 - accuracy: 0.7821 - val_loss: 0.4233 - val_accuracy: 0.8111\n",
      "Epoch 10: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "344/344 [==============================] - 3s 6ms/step\n",
      "13/13 [==============================] - 1s 4ms/step\n",
      "173/173 [==============================] - 1s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(2766, 24, 10)\n",
      "Accuracy test cluster  0 : 91.4678\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(113, 24, 10)\n",
      "Accuracy test cluster  1 : 76.9912\n",
      "43/43 [==============================] - 1s 5ms/step\n",
      "----- cluster  2  -----\n",
      "(1349, 24, 10)\n",
      "Accuracy test cluster  2 : 80.9489\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 86.7483\n",
      "Accuracy test: 87.7247\n",
      "AUC-ROC:  0.7757075304161806\n",
      "AUC-PR:  0.34712608191939226\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(437, 24, 10)\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 4s 64ms/step - loss: 0.6149 - accuracy: 0.7143 - val_loss: 0.5783 - val_accuracy: 0.7424\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.5974 - accuracy: 0.7439 - val_loss: 0.5549 - val_accuracy: 0.7424\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.5833 - accuracy: 0.7305 - val_loss: 0.5441 - val_accuracy: 0.7576\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.5682 - accuracy: 0.7251 - val_loss: 0.5362 - val_accuracy: 0.7576\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.5640 - accuracy: 0.7305 - val_loss: 0.5322 - val_accuracy: 0.7576\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.5669 - accuracy: 0.7305 - val_loss: 0.5293 - val_accuracy: 0.7576\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.5595 - accuracy: 0.7278 - val_loss: 0.5279 - val_accuracy: 0.7576\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.5630 - accuracy: 0.7278 - val_loss: 0.5257 - val_accuracy: 0.7576\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 44ms/step - loss: 0.5579 - accuracy: 0.7305 - val_loss: 0.5238 - val_accuracy: 0.7576\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 0.5605 - accuracy: 0.7332 - val_loss: 0.5223 - val_accuracy: 0.7576\n",
      "(11029, 24, 10)\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 9s 21ms/step - loss: 0.4293 - accuracy: 0.8224 - val_loss: 0.2518 - val_accuracy: 0.9293\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 6s 20ms/step - loss: 0.2908 - accuracy: 0.9114 - val_loss: 0.2392 - val_accuracy: 0.9293\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 5s 19ms/step - loss: 0.2815 - accuracy: 0.9116 - val_loss: 0.2345 - val_accuracy: 0.9293\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.2755 - accuracy: 0.9115 - val_loss: 0.2357 - val_accuracy: 0.9293\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - 6s 20ms/step - loss: 0.2717 - accuracy: 0.9116 - val_loss: 0.2360 - val_accuracy: 0.9293\n",
      "Epoch 6/10\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.2701 - accuracy: 0.9119Restoring model weights from the end of the best epoch: 3.\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.2695 - accuracy: 0.9122 - val_loss: 0.2397 - val_accuracy: 0.9287\n",
      "Epoch 6: early stopping\n",
      "(5446, 24, 10)\n",
      "Epoch 1/10\n",
      "145/145 [==============================] - 6s 22ms/step - loss: 0.5701 - accuracy: 0.7447 - val_loss: 0.4889 - val_accuracy: 0.8029\n",
      "Epoch 2/10\n",
      "145/145 [==============================] - 3s 23ms/step - loss: 0.5011 - accuracy: 0.7840 - val_loss: 0.4639 - val_accuracy: 0.8029\n",
      "Epoch 3/10\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4749 - accuracy: 0.7844 - val_loss: 0.4468 - val_accuracy: 0.8005\n",
      "Epoch 4/10\n",
      "145/145 [==============================] - 2s 17ms/step - loss: 0.4603 - accuracy: 0.7859 - val_loss: 0.4410 - val_accuracy: 0.8029\n",
      "Epoch 5/10\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4541 - accuracy: 0.7879 - val_loss: 0.4371 - val_accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "145/145 [==============================] - 3s 22ms/step - loss: 0.4528 - accuracy: 0.7874 - val_loss: 0.4381 - val_accuracy: 0.8042\n",
      "Epoch 7/10\n",
      "145/145 [==============================] - 3s 21ms/step - loss: 0.4430 - accuracy: 0.7876 - val_loss: 0.4400 - val_accuracy: 0.7993\n",
      "Epoch 8/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.4452 - accuracy: 0.7892 - val_loss: 0.4358 - val_accuracy: 0.7968\n",
      "Epoch 9/10\n",
      "145/145 [==============================] - 3s 17ms/step - loss: 0.4422 - accuracy: 0.7872 - val_loss: 0.4358 - val_accuracy: 0.7944\n",
      "Epoch 10/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.4388 - accuracy: 0.7905 - val_loss: 0.4371 - val_accuracy: 0.7993\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "14/14 [==============================] - 1s 5ms/step\n",
      "345/345 [==============================] - 3s 8ms/step\n",
      "171/171 [==============================] - 2s 9ms/step\n",
      "-------- Test metrics ---------\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      "----- cluster  0  -----\n",
      "(129, 24, 10)\n",
      "Accuracy test cluster  0 : 75.1938\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(2774, 24, 10)\n",
      "Accuracy test cluster  1 : 91.3482\n",
      "42/42 [==============================] - 1s 9ms/step\n",
      "----- cluster  2  -----\n",
      "(1324, 24, 10)\n",
      "Accuracy test cluster  2 : 79.0785\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 87.0624\n",
      "Accuracy test: 87.0121\n",
      "AUC-ROC:  0.7726499298052386\n",
      "AUC-PR:  0.351275747028477\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM\n",
    "aucprs_w, aucrocs_w, accuracies_w = kfoldclusteredkmeans(\"LSTM\", X, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigest cluster\n",
    "acc_test = [0.915033, 0.909617, 0.917919, 0.914317, 0.913482]\n",
    "acc_train = [0.9131, 0.9127, 0.9119, 0.9108, 0.9115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aucpr scores: [0.3671300924689447, 0.37740440173099943, 0.37830638105328607, 0.34712608191939226, 0.351275747028477]\n",
      "0.3642 mean aucpr with a standard deviation of 0.0130\n",
      "aucroc scores: [0.7735674720205687, 0.7918580945133872, 0.7939284619602799, 0.7757075304161806, 0.7726499298052386]\n",
      "0.7815 mean aucroc with a standard deviation of 0.0093\n",
      "accuracy scores: [86.3529, 86.6367, 87.3226, 87.7247, 87.0121]\n",
      "87.0098 mean accuracy with a standard deviation of 0.4857\n"
     ]
    }
   ],
   "source": [
    "print ('aucpr scores:', aucprs_w)\n",
    "print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs_w), np.std(aucprs_w)))\n",
    "\n",
    "print ('aucroc scores:', aucrocs_w)\n",
    "print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs_w), np.std(aucrocs_w)))\n",
    "\n",
    "print ('accuracy scores:', accuracies_w)\n",
    "print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies_w), np.std(accuracies_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(481, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 3s 58ms/step - loss: 0.9119 - accuracy: 0.3162 - val_loss: 0.8105 - val_accuracy: 0.2740\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.8806 - accuracy: 0.2794 - val_loss: 0.8469 - val_accuracy: 0.2740\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.8622 - accuracy: 0.2574 - val_loss: 0.8845 - val_accuracy: 0.2740\n",
      "Epoch 4/10\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8629 - accuracy: 0.2386Restoring model weights from the end of the best epoch: 1.\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.8676 - accuracy: 0.2475 - val_loss: 0.9048 - val_accuracy: 0.2740\n",
      "Epoch 4: early stopping\n",
      "(5420, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 6s 22ms/step - loss: 0.8182 - accuracy: 0.2496 - val_loss: 0.9437 - val_accuracy: 0.1845\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 3s 19ms/step - loss: 0.7697 - accuracy: 0.3321 - val_loss: 0.8826 - val_accuracy: 0.2903\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 3s 20ms/step - loss: 0.7368 - accuracy: 0.4376 - val_loss: 0.8631 - val_accuracy: 0.4047\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 3s 19ms/step - loss: 0.7200 - accuracy: 0.4875 - val_loss: 0.8045 - val_accuracy: 0.4748\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 3s 19ms/step - loss: 0.7102 - accuracy: 0.5140 - val_loss: 0.7815 - val_accuracy: 0.5117\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 3s 19ms/step - loss: 0.6982 - accuracy: 0.5489 - val_loss: 0.7945 - val_accuracy: 0.5105\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 3s 19ms/step - loss: 0.6959 - accuracy: 0.5563 - val_loss: 0.7864 - val_accuracy: 0.5191\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.5613Restoring model weights from the end of the best epoch: 5.\n",
      "144/144 [==============================] - 3s 20ms/step - loss: 0.6957 - accuracy: 0.5613 - val_loss: 0.8235 - val_accuracy: 0.5055\n",
      "Epoch 8: early stopping\n",
      "(11010, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 9s 21ms/step - loss: 0.5606 - accuracy: 0.8569 - val_loss: 0.4900 - val_accuracy: 0.9177\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 6s 20ms/step - loss: 0.5246 - accuracy: 0.8369 - val_loss: 0.4457 - val_accuracy: 0.8608\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.5058 - accuracy: 0.8053 - val_loss: 0.4583 - val_accuracy: 0.7984\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.4999 - accuracy: 0.7892 - val_loss: 0.4809 - val_accuracy: 0.7591\n",
      "Epoch 5/10\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4943 - accuracy: 0.7883Restoring model weights from the end of the best epoch: 2.\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.4954 - accuracy: 0.7880 - val_loss: 0.4888 - val_accuracy: 0.7409\n",
      "Epoch 5: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "170/170 [==============================] - 1s 5ms/step\n",
      "345/345 [==============================] - 2s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "----- cluster  0  -----\n",
      "(108, 24, 10)\n",
      "Accuracy test cluster  0 : 31.4815\n",
      "43/43 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(1366, 24, 10)\n",
      "Accuracy test cluster  1 : 54.612\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(2754, 24, 10)\n",
      "Accuracy test cluster  2 : 84.6042\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 1.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 74.2712\n",
      "Accuracy test: 73.5572\n",
      "AUC-ROC:  0.7633871181185723\n",
      "AUC-PR:  0.36013278519677044\n",
      "KFold  2  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(10934, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "291/291 [==============================] - 9s 21ms/step - loss: 0.5834 - accuracy: 0.6944 - val_loss: 0.4961 - val_accuracy: 0.9074\n",
      "Epoch 2/10\n",
      "291/291 [==============================] - 6s 20ms/step - loss: 0.5335 - accuracy: 0.8429 - val_loss: 0.4940 - val_accuracy: 0.8221\n",
      "Epoch 3/10\n",
      "291/291 [==============================] - 6s 20ms/step - loss: 0.5156 - accuracy: 0.8087 - val_loss: 0.5124 - val_accuracy: 0.7514\n",
      "Epoch 4/10\n",
      "291/291 [==============================] - 6s 20ms/step - loss: 0.5073 - accuracy: 0.7943 - val_loss: 0.5503 - val_accuracy: 0.6947\n",
      "Epoch 5/10\n",
      "291/291 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.7907Restoring model weights from the end of the best epoch: 2.\n",
      "291/291 [==============================] - 6s 20ms/step - loss: 0.4982 - accuracy: 0.7907 - val_loss: 0.5267 - val_accuracy: 0.7069\n",
      "Epoch 5: early stopping\n",
      "(5507, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 6s 22ms/step - loss: 0.8298 - accuracy: 0.3831 - val_loss: 0.8523 - val_accuracy: 0.2297\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.7585 - accuracy: 0.3808 - val_loss: 0.8399 - val_accuracy: 0.3664\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.7331 - accuracy: 0.4821 - val_loss: 0.7881 - val_accuracy: 0.4897\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.7123 - accuracy: 0.5288 - val_loss: 0.7669 - val_accuracy: 0.5345\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.7059 - accuracy: 0.5551 - val_loss: 0.7702 - val_accuracy: 0.5453\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.6959 - accuracy: 0.5626 - val_loss: 0.7587 - val_accuracy: 0.5514\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.6881 - accuracy: 0.5682 - val_loss: 0.7516 - val_accuracy: 0.5562\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.6904 - accuracy: 0.5647 - val_loss: 0.7327 - val_accuracy: 0.5659\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.6855 - accuracy: 0.5797 - val_loss: 0.7649 - val_accuracy: 0.5514\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.6769 - accuracy: 0.5776 - val_loss: 0.7598 - val_accuracy: 0.5550\n",
      "(470, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 3s 57ms/step - loss: 0.8786 - accuracy: 0.2657 - val_loss: 0.9787 - val_accuracy: 0.2676\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.8904 - accuracy: 0.2732 - val_loss: 0.9938 - val_accuracy: 0.2676\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.8823 - accuracy: 0.2732 - val_loss: 0.9925 - val_accuracy: 0.2676\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.2682Restoring model weights from the end of the best epoch: 1.\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.8658 - accuracy: 0.2682 - val_loss: 0.9923 - val_accuracy: 0.2676\n",
      "Epoch 4: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "342/342 [==============================] - 2s 6ms/step\n",
      "173/173 [==============================] - 1s 5ms/step\n",
      "15/15 [==============================] - 0s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(2766, 24, 10)\n",
      "Accuracy test cluster  0 : 83.2249\n",
      "43/43 [==============================] - 1s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(1349, 24, 10)\n",
      "Accuracy test cluster  1 : 57.6723\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "----- cluster  2  -----\n",
      "(113, 24, 10)\n",
      "Accuracy test cluster  2 : 23.8938\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 0. 0. ... 1. 1. 1.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy train: 73.2718\n",
      "Accuracy test: 73.4863\n",
      "AUC-ROC:  0.791264501314964\n",
      "AUC-PR:  0.3749157986534367\n",
      "KFold  3  ---\n",
      "---- Clustering Train ---- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- LSTM fitting for each cluster --------\n",
      "(5337, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 6s 22ms/step - loss: 0.8071 - accuracy: 0.2478 - val_loss: 0.8949 - val_accuracy: 0.1985\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7606 - accuracy: 0.3419 - val_loss: 0.8409 - val_accuracy: 0.3508\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7351 - accuracy: 0.4464 - val_loss: 0.8453 - val_accuracy: 0.4257\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7210 - accuracy: 0.4971 - val_loss: 0.8056 - val_accuracy: 0.5006\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7119 - accuracy: 0.5214 - val_loss: 0.7672 - val_accuracy: 0.5381\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7046 - accuracy: 0.5412 - val_loss: 0.7579 - val_accuracy: 0.5518\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7006 - accuracy: 0.5461 - val_loss: 0.7631 - val_accuracy: 0.5468\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6997 - accuracy: 0.5489 - val_loss: 0.7713 - val_accuracy: 0.5443\n",
      "Epoch 9/10\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6969 - accuracy: 0.5521Restoring model weights from the end of the best epoch: 6.\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.6957 - accuracy: 0.5525 - val_loss: 0.7587 - val_accuracy: 0.5506\n",
      "Epoch 9: early stopping\n",
      "(11096, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "  7/295 [..............................] - ETA: 2:36 - loss: 0.6165 - accuracy: 0.1607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 22s 31ms/step - loss: 0.5553 - accuracy: 0.7852 - val_loss: 0.4429 - val_accuracy: 0.9147\n",
      "Epoch 2/10\n",
      "295/295 [==============================] - 6s 19ms/step - loss: 0.5212 - accuracy: 0.8250 - val_loss: 0.4432 - val_accuracy: 0.8589\n",
      "Epoch 3/10\n",
      "295/295 [==============================] - 6s 19ms/step - loss: 0.5107 - accuracy: 0.8008 - val_loss: 0.4755 - val_accuracy: 0.7940\n",
      "Epoch 4/10\n",
      "294/295 [============================>.] - ETA: 0s - loss: 0.4968 - accuracy: 0.7977Restoring model weights from the end of the best epoch: 1.\n",
      "295/295 [==============================] - 6s 19ms/step - loss: 0.4971 - accuracy: 0.7980 - val_loss: 0.5030 - val_accuracy: 0.7483\n",
      "Epoch 4: early stopping\n",
      "(478, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 3s 56ms/step - loss: 1.6725 - accuracy: 0.7389 - val_loss: 0.5267 - val_accuracy: 0.7917\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 1.4789 - accuracy: 0.7389 - val_loss: 0.5405 - val_accuracy: 0.7917\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 1.3025 - accuracy: 0.7389 - val_loss: 0.5645 - val_accuracy: 0.7917\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1942 - accuracy: 0.7365Restoring model weights from the end of the best epoch: 1.\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 1.1942 - accuracy: 0.7365 - val_loss: 0.5974 - val_accuracy: 0.7917\n",
      "Epoch 4: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "167/167 [==============================] - 1s 5ms/step\n",
      "347/347 [==============================] - 2s 5ms/step\n",
      "15/15 [==============================] - 0s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "44/44 [==============================] - 1s 5ms/step\n",
      "----- cluster  0  -----\n",
      "(1404, 24, 10)\n",
      "Accuracy test cluster  0 : 58.6182\n",
      "86/86 [==============================] - 1s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(2729, 24, 10)\n",
      "Accuracy test cluster  1 : 90.5826\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "----- cluster  2  -----\n",
      "(95, 24, 10)\n",
      "Accuracy test cluster  2 : 68.4211\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 1. 1. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 0. 0. ... 0. 0. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Accuracy train: 79.2798\n",
      "Accuracy test: 79.4702\n",
      "AUC-ROC:  0.7539331904263863\n",
      "AUC-PR:  0.3560473663899998\n",
      "KFold  4  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(10996, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 9s 20ms/step - loss: 0.5514 - accuracy: 0.8712 - val_loss: 0.4811 - val_accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.5218 - accuracy: 0.8330 - val_loss: 0.4549 - val_accuracy: 0.8424\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.5138 - accuracy: 0.8093 - val_loss: 0.5214 - val_accuracy: 0.7333\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.5030 - accuracy: 0.7933 - val_loss: 0.4675 - val_accuracy: 0.7782\n",
      "Epoch 5/10\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5026 - accuracy: 0.7956Restoring model weights from the end of the best epoch: 2.\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.5028 - accuracy: 0.7955 - val_loss: 0.4950 - val_accuracy: 0.7394\n",
      "Epoch 5: early stopping\n",
      "(409, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 3s 65ms/step - loss: 0.9449 - accuracy: 0.3775 - val_loss: 0.8253 - val_accuracy: 0.2258\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.8903 - accuracy: 0.3285 - val_loss: 0.8907 - val_accuracy: 0.2258\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9082 - accuracy: 0.2939 - val_loss: 0.9545 - val_accuracy: 0.2258\n",
      "Epoch 4/10\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.8641 - accuracy: 0.2812Restoring model weights from the end of the best epoch: 1.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.8599 - accuracy: 0.2824 - val_loss: 0.9855 - val_accuracy: 0.2258\n",
      "Epoch 4: early stopping\n",
      "(5506, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 6s 24ms/step - loss: 0.8072 - accuracy: 0.2759 - val_loss: 0.8666 - val_accuracy: 0.2203\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.7546 - accuracy: 0.3741 - val_loss: 0.7752 - val_accuracy: 0.4697\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.7195 - accuracy: 0.5135 - val_loss: 0.7612 - val_accuracy: 0.5424\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.7062 - accuracy: 0.5442 - val_loss: 0.7895 - val_accuracy: 0.5375\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 3s 20ms/step - loss: 0.6984 - accuracy: 0.5635 - val_loss: 0.7837 - val_accuracy: 0.5448\n",
      "Epoch 6/10\n",
      "145/147 [============================>.] - ETA: 0s - loss: 0.6892 - accuracy: 0.5720Restoring model weights from the end of the best epoch: 3.\n",
      "147/147 [==============================] - 3s 19ms/step - loss: 0.6903 - accuracy: 0.5722 - val_loss: 0.8182 - val_accuracy: 0.5254\n",
      "Epoch 6: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "344/344 [==============================] - 2s 5ms/step\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "173/173 [==============================] - 1s 6ms/step\n",
      "-------- Test metrics ---------\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(2766, 24, 10)\n",
      "Accuracy test cluster  0 : 85.3941\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "----- cluster  1  -----\n",
      "(113, 24, 10)\n",
      "Accuracy test cluster  1 : 23.8938\n",
      "43/43 [==============================] - 1s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(1349, 24, 10)\n",
      "Accuracy test cluster  2 : 56.4122\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 1. ... 1. 1. 0.]\n",
      "true labels:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [0. 0. 1. ... 1. 1. 0.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy train: 74.1352\n",
      "Accuracy test: 74.5033\n",
      "AUC-ROC:  0.7730158511139015\n",
      "AUC-PR:  0.3371242304088357\n",
      "KFold  5  ---\n",
      "---- Clustering Train ---- \n",
      "-------- LSTM fitting for each cluster --------\n",
      "(437, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 6s 62ms/step - loss: 0.8836 - accuracy: 0.2722 - val_loss: 1.0348 - val_accuracy: 0.2424\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.8766 - accuracy: 0.2722 - val_loss: 1.0145 - val_accuracy: 0.2424\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.8718 - accuracy: 0.2722 - val_loss: 1.0110 - val_accuracy: 0.2424\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.8612 - accuracy: 0.2722 - val_loss: 1.0128 - val_accuracy: 0.2424\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.8746 - accuracy: 0.2722 - val_loss: 1.0092 - val_accuracy: 0.2424\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.8620 - accuracy: 0.2722 - val_loss: 1.0013 - val_accuracy: 0.2424\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.8527 - accuracy: 0.2722 - val_loss: 0.9995 - val_accuracy: 0.2424\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.8696 - accuracy: 0.2722 - val_loss: 1.0002 - val_accuracy: 0.2424\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.8378 - accuracy: 0.2722 - val_loss: 0.9953 - val_accuracy: 0.2424\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.8428 - accuracy: 0.2722 - val_loss: 0.9822 - val_accuracy: 0.2424\n",
      "(11029, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 9s 21ms/step - loss: 0.5589 - accuracy: 0.8525 - val_loss: 0.4741 - val_accuracy: 0.9088\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 6s 20ms/step - loss: 0.5241 - accuracy: 0.8260 - val_loss: 0.4705 - val_accuracy: 0.8260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.5086 - accuracy: 0.7952 - val_loss: 0.5036 - val_accuracy: 0.7498\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 6s 19ms/step - loss: 0.4996 - accuracy: 0.7949 - val_loss: 0.5203 - val_accuracy: 0.7287\n",
      "Epoch 5/10\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4937 - accuracy: 0.7887Restoring model weights from the end of the best epoch: 2.\n",
      "293/293 [==============================] - 5s 19ms/step - loss: 0.4935 - accuracy: 0.7888 - val_loss: 0.5029 - val_accuracy: 0.7299\n",
      "Epoch 5: early stopping\n",
      "(5446, 24, 10)\n",
      "PASSOU\n",
      "Epoch 1/10\n",
      "145/145 [==============================] - 6s 22ms/step - loss: 0.8126 - accuracy: 0.3407 - val_loss: 0.8527 - val_accuracy: 0.2656\n",
      "Epoch 2/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.7405 - accuracy: 0.4290 - val_loss: 0.7900 - val_accuracy: 0.4761\n",
      "Epoch 3/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.7045 - accuracy: 0.5250 - val_loss: 0.8020 - val_accuracy: 0.5202\n",
      "Epoch 4/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6980 - accuracy: 0.5541 - val_loss: 0.8212 - val_accuracy: 0.5141\n",
      "Epoch 5/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6835 - accuracy: 0.5632 - val_loss: 0.7853 - val_accuracy: 0.5373\n",
      "Epoch 6/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6831 - accuracy: 0.5641 - val_loss: 0.7906 - val_accuracy: 0.5410\n",
      "Epoch 7/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6819 - accuracy: 0.5634 - val_loss: 0.7503 - val_accuracy: 0.5581\n",
      "Epoch 8/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6711 - accuracy: 0.5770 - val_loss: 0.7833 - val_accuracy: 0.5483\n",
      "Epoch 9/10\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6763 - accuracy: 0.5716 - val_loss: 0.8009 - val_accuracy: 0.5324\n",
      "Epoch 10/10\n",
      "143/145 [============================>.] - ETA: 0s - loss: 0.6689 - accuracy: 0.5730Restoring model weights from the end of the best epoch: 7.\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.6681 - accuracy: 0.5731 - val_loss: 0.7893 - val_accuracy: 0.5398\n",
      "Epoch 10: early stopping\n",
      "Assigning each test sample to the closest cluster centroid...\n",
      "-------- Train metrics ---------\n",
      "14/14 [==============================] - 0s 6ms/step\n",
      "345/345 [==============================] - 2s 5ms/step\n",
      "171/171 [==============================] - 1s 5ms/step\n",
      "-------- Test metrics ---------\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      "----- cluster  0  -----\n",
      "(129, 24, 10)\n",
      "Accuracy test cluster  0 : 24.8062\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "----- cluster  1  -----\n",
      "(2774, 24, 10)\n",
      "Accuracy test cluster  1 : 84.6071\n",
      "42/42 [==============================] - 1s 6ms/step\n",
      "----- cluster  2  -----\n",
      "(1324, 24, 10)\n",
      "Accuracy test cluster  2 : 57.3263\n",
      "------ Metrics ------\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 1. 0. 1.]\n",
      "true labels:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "labels(0 short stay, 1 long stay) predicted:  [1. 1. 1. ... 0. 0. 1.]\n",
      "true labels:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Accuracy train: 74.1308\n",
      "Accuracy test: 74.237\n",
      "AUC-ROC:  0.7699182483110597\n",
      "AUC-PR:  0.34963638409126413\n"
     ]
    }
   ],
   "source": [
    "# Execute LSTM\n",
    "aucprs_w, aucrocs_w, accuracies_w = kfoldclusteredkmeans(\"LSTM\", X, y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manera 2\n",
    "#https://amirhessam88.github.io/roc-vs-pr/\n",
    "\n",
    "def customCrossValidationMetrics(classifier, X, y, weighted = False  ):\n",
    "    scaler = StandardScaler()\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    #ejemplo classifier=RandomForestClassifier(n_estimators=10, random_state = 42)\n",
    "    aucprs = []\n",
    "    aucrocs = []\n",
    "    accuracies = []\n",
    "    \n",
    "    i=1\n",
    "    for train, test in cv.split(X, y): #train and test are indexes\n",
    "        print('KFold ',i)\n",
    "        #reshape because scaler need <= 2d\n",
    "        X_train_tranformed = scaler.fit_transform(X[train].reshape(train.shape[0]*24, 10))\n",
    "        X_test_tranformed = scaler.transform(X[test].reshape(test.shape[0]*24, 10))\n",
    "        \n",
    "        #re-reshape\n",
    "        \n",
    "        X_train_tranformed = X_train_tranformed.reshape(train.shape[0], 24, 10)\n",
    "        X_test_tranformed = X_test_tranformed.reshape(test.shape[0], 24, 10)\n",
    "        \n",
    "        if(weighted):\n",
    "            classWeight = class_weight.compute_class_weight('balanced',np.unique(np.ravel(y[train])),np.ravel(y[train]))\n",
    "            classWeight = {i : classWeight[i] for i in range(2)}  #convert to dictionary in order to fit to keras model\n",
    "            print(\"training: \")\n",
    "            history = classifier.fit(X_train_tranformed, np.ravel(y[train]), epochs=8, verbose=1,class_weight=classWeight)\n",
    "\n",
    "        else:\n",
    "            print(\"training: \")\n",
    "            history = classifier.fit(X_train_tranformed, np.ravel(y[train]),epochs=8, verbose=1)\n",
    "\n",
    "        print(\"testing: \")\n",
    "        probas_ = np.ravel(classifier.predict(X_test_tranformed, verbose=1))\n",
    "        # Compute PR curve and area the curve\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y[test], probas_)\n",
    "        pr_auc =  np.round(metrics.auc(recall, precision), 6)\n",
    "        aucprs.append(pr_auc)\n",
    "        auroc =  np.round(metrics.roc_auc_score(y[test],probas_),6)\n",
    "        aucrocs.append(auroc)\n",
    "\n",
    "        y_pred_binary = classifier.predict(X_test_tranformed)\n",
    "        y_pred_binary = (y_pred_binary.ravel()>0.5) + 0.0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "        acc = np.round(metrics.accuracy_score(y[test],y_pred_binary)*100,4)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        i = i+1\n",
    "\n",
    "    print ('aucpr scores:', aucprs)\n",
    "    print(\"%0.4f mean aucpr with a standard deviation of %0.4f\" % (np.mean(aucprs), np.std(aucprs)))\n",
    "\n",
    "    print ('aucroc scores:', aucrocs)\n",
    "    print(\"%0.4f mean aucroc with a standard deviation of %0.4f\" % (np.mean(aucrocs), np.std(aucrocs)))\n",
    "\n",
    "    print ('accuracy scores:', accuracies)\n",
    "    print(\"%0.4f mean accuracy with a standard deviation of %0.4f\" % (np.mean(accuracies), np.std(accuracies)))\n",
    "    \n",
    "    return aucprs, aucrocs, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 15s 20ms/step - loss: 0.4437 - accuracy: 0.7985\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 11s 21ms/step - loss: 0.3403 - accuracy: 0.8685\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3337 - accuracy: 0.8709\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3327 - accuracy: 0.8698\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 14s 26ms/step - loss: 0.3304 - accuracy: 0.8710\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 12s 22ms/step - loss: 0.3300 - accuracy: 0.8699\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3279 - accuracy: 0.8698\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 12s 22ms/step - loss: 0.3269 - accuracy: 0.8707\n",
      "testing: \n",
      "133/133 [==============================] - 1s 6ms/step\n",
      "KFold  2\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 11s 20ms/step - loss: 0.3301 - accuracy: 0.8706\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3287 - accuracy: 0.8716\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 13s 24ms/step - loss: 0.3292 - accuracy: 0.8694\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 12s 23ms/step - loss: 0.3276 - accuracy: 0.8712\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 11s 21ms/step - loss: 0.3260 - accuracy: 0.8710\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3272 - accuracy: 0.8710\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 11s 21ms/step - loss: 0.3247 - accuracy: 0.8720\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 9s 18ms/step - loss: 0.3235 - accuracy: 0.8720\n",
      "testing: \n",
      "133/133 [==============================] - 1s 6ms/step\n",
      "KFold  3\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 11s 21ms/step - loss: 0.3282 - accuracy: 0.8703\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 14s 27ms/step - loss: 0.3261 - accuracy: 0.8708\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3263 - accuracy: 0.8706\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 9s 16ms/step - loss: 0.3252 - accuracy: 0.8706\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 11s 21ms/step - loss: 0.3252 - accuracy: 0.8704\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 15s 28ms/step - loss: 0.3243 - accuracy: 0.8711\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3234 - accuracy: 0.8718\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 8s 16ms/step - loss: 0.3238 - accuracy: 0.8719\n",
      "testing: \n",
      "133/133 [==============================] - 1s 5ms/step\n",
      "KFold  4\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3247 - accuracy: 0.8703\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 16s 31ms/step - loss: 0.3223 - accuracy: 0.8726\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 12s 22ms/step - loss: 0.3235 - accuracy: 0.8688\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 12s 22ms/step - loss: 0.3225 - accuracy: 0.8707\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3218 - accuracy: 0.8730\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3217 - accuracy: 0.8714\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 10s 18ms/step - loss: 0.3217 - accuracy: 0.8707\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3217 - accuracy: 0.8719\n",
      "testing: \n",
      "133/133 [==============================] - 1s 5ms/step\n",
      "KFold  5\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3182 - accuracy: 0.8732\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 10s 19ms/step - loss: 0.3165 - accuracy: 0.8739\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 9s 18ms/step - loss: 0.3167 - accuracy: 0.8743\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 9s 17ms/step - loss: 0.3135 - accuracy: 0.8761\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 10s 18ms/step - loss: 0.3148 - accuracy: 0.8758\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 9s 18ms/step - loss: 0.3160 - accuracy: 0.8745\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 10s 18ms/step - loss: 0.3146 - accuracy: 0.8739\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 11s 20ms/step - loss: 0.3132 - accuracy: 0.8762\n",
      "testing: \n",
      "133/133 [==============================] - 1s 6ms/step\n",
      "aucpr scores: [0.376265, 0.403704, 0.441271, 0.391306, 0.388889]\n",
      "0.4003 mean aucpr with a standard deviation of 0.0223\n",
      "aucroc scores: [0.777467, 0.803447, 0.812202, 0.809899, 0.799165]\n",
      "0.8004 mean aucroc with a standard deviation of 0.0124\n",
      "accuracy scores: [86.4711, 86.9205, 87.843, 88.0558, 87.1067]\n",
      "87.2794 mean accuracy with a standard deviation of 0.5886\n"
     ]
    }
   ],
   "source": [
    "aucprs_LSTM, aucrocs_LSTM, accuracies_LSTM = customCrossValidationMetrics(classifier, X, y, weighted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, LSTM\n",
    "\n",
    "classifier_weighted = Sequential()\n",
    "classifier_weighted.add(Bidirectional(LSTM(10, activation='sigmoid'), input_shape=(24, 10)))\n",
    "classifier_weighted.add(Dropout(0.2))\n",
    "classifier_weighted.add(Dense(1, activation='sigmoid'))\n",
    "classifier_weighted.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold  1\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 6s 8ms/step - loss: 0.6720 - accuracy: 0.4515\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5829 - accuracy: 0.6987\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5799 - accuracy: 0.7092\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5783 - accuracy: 0.6979\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5601 - accuracy: 0.7230\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5589 - accuracy: 0.7147\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5629 - accuracy: 0.7116\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5571 - accuracy: 0.7231\n",
      "testing: \n",
      "133/133 [==============================] - 1s 2ms/step\n",
      "KFold  2\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5601 - accuracy: 0.7094\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5602 - accuracy: 0.7107\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5638 - accuracy: 0.7113\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5598 - accuracy: 0.7144\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5588 - accuracy: 0.7182\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5556 - accuracy: 0.7171\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5542 - accuracy: 0.7162\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5522 - accuracy: 0.7179\n",
      "testing: \n",
      "133/133 [==============================] - 0s 3ms/step\n",
      "KFold  3\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 4s 9ms/step - loss: 0.5548 - accuracy: 0.7164\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5508 - accuracy: 0.7165\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5524 - accuracy: 0.7147\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5520 - accuracy: 0.7136\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5501 - accuracy: 0.7165\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5494 - accuracy: 0.7151\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5511 - accuracy: 0.7145\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5465 - accuracy: 0.7217\n",
      "testing: \n",
      "133/133 [==============================] - 0s 3ms/step\n",
      "KFold  4\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5449 - accuracy: 0.7211\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5464 - accuracy: 0.7228\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5440 - accuracy: 0.7237\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5446 - accuracy: 0.7228\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5431 - accuracy: 0.7276\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5405 - accuracy: 0.7239\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5413 - accuracy: 0.7260\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5410 - accuracy: 0.7289\n",
      "testing: \n",
      "133/133 [==============================] - 0s 3ms/step\n",
      "KFold  5\n",
      "training: \n",
      "Epoch 1/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5372 - accuracy: 0.7271\n",
      "Epoch 2/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5388 - accuracy: 0.7265\n",
      "Epoch 3/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5380 - accuracy: 0.7302\n",
      "Epoch 4/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5366 - accuracy: 0.7308\n",
      "Epoch 5/8\n",
      "529/529 [==============================] - 4s 9ms/step - loss: 0.5363 - accuracy: 0.7250\n",
      "Epoch 6/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5346 - accuracy: 0.7272\n",
      "Epoch 7/8\n",
      "529/529 [==============================] - 5s 9ms/step - loss: 0.5332 - accuracy: 0.7274\n",
      "Epoch 8/8\n",
      "529/529 [==============================] - 4s 8ms/step - loss: 0.5344 - accuracy: 0.7262\n",
      "testing: \n",
      "133/133 [==============================] - 0s 3ms/step\n",
      "aucpr scores: [0.373053, 0.397489, 0.429715, 0.382057, 0.39139]\n",
      "0.3947 mean aucpr with a standard deviation of 0.0194\n",
      "aucroc scores: [0.780075, 0.804848, 0.812498, 0.808423, 0.803504]\n",
      "0.8019 mean aucroc with a standard deviation of 0.0113\n",
      "accuracy scores: [68.0464, 72.1145, 71.5941, 74.6452, 73.3144]\n",
      "71.9429 mean accuracy with a standard deviation of 2.2144\n"
     ]
    }
   ],
   "source": [
    "aucprs_LSTM_weighted, aucrocs_LSTM_weighted, accuracies_LSTM_weighted = customCrossValidationMetrics(classifier_weighted, X, y, weighted = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Statistical Significance Tests for Comparing Machine Learning Algorithms\n",
    "\n",
    "https://stackoverflow.com/questions/54498235/compare-whether-the-difference-between-performance-accuracy-of-2-ml-models-is-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM vs LSTM_weighted AUCROC:  Ttest_relResult(statistic=-1.3258967258799943, pvalue=0.2555220127917172)\n"
     ]
    }
   ],
   "source": [
    "print('LSTM vs LSTM_weighted AUCROC: ', stats.ttest_rel(aucrocs_LSTM,aucrocs_LSTM_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM vs LSTM_weighted AUCPR:  Ttest_relResult(statistic=1.715553266087846, pvalue=0.1613879046592789)\n"
     ]
    }
   ],
   "source": [
    "print('LSTM vs LSTM_weighted AUCPR: ', stats.ttest_rel(aucprs_LSTM,aucprs_LSTM_weighted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "las diferencias de aucroc y aucpr entre LSTM y LSTM_weighted no son estadísticamente significativas. (p_value > 0.05, no podemos rechazar la hipotesi nula de que los modelos tienen comportamiento similar)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
